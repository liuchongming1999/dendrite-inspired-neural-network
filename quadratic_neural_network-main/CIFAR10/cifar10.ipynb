{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import model.resnet_cifar10 as ResNet\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "import shutil\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def savefig(name):\n",
    "    plt.savefig(name,dpi=600, bbox_inches='tight')\n",
    "    return\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.benchmark = True #for accelerating the running\n",
    "    return\n",
    "\n",
    "#setup_seed(1)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Cifar10 Training')\n",
    "parser.add_argument('--gpu-id', default=[0], nargs='+', type=int, help='available GPU IDs')\n",
    "parser.add_argument('--epochs', default=250, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int, metavar='N', help='mini-batch size (default: 128),only used for train')\n",
    "parser.add_argument('-w', '--workers', default=5, type=int, metavar='N', help='num_workers, at most 16, must be 0 on windows')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.002, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-3, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=60, type=int, metavar='N', help='print frequency (default: 10)')\n",
    "#parser.add_argument('--resume', default='checkpoint/Alexnet/checkpoint.pth.tar', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-t', '--train', dest='train', action='store_true', help='test model on test set')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "Path_Name = 'Alexnet'\n",
    "checkpoint_path = 'checkpoint/' + Path_Name\n",
    "summary_path = 'summary/' + Path_Name\n",
    "if args.train:\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(summary_path)\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, transform=train_transform, download=True)\n",
    "\n",
    "valid_dataset = torchvision.datasets.CIFAR10(root='data', train=False, transform=test_transform)\n",
    "\n",
    "# valid_dataset, test_dataset = torch.utils.data.random_split(test_dataset, (int(0.5*len(test_dataset)), int(0.5*len(test_dataset))))\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=args.batch_size)\n",
    "batch_size = 200\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "all_train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "# model = LinearNeuralNet(input_size, 5, num_classes).to(device)\n",
    "#model = AlexNet.AlexNet_0()\n",
    "model = ResNet.resnet18()\n",
    "#model = Vgg.VGG('VGG11')\n",
    "#model = LeNet.LeNet()\n",
    "#model = Googlenet.GoogLeNet()\n",
    "model = nn.DataParallel(model, device_ids=args.gpu_id).cuda()\n",
    "image_list = [] \n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "cudnn.benchmark = True\n",
    "KS_dis = []\n",
    "test_accuracy = []\n",
    "best_prec = 0\n",
    "mean_difference_all = []\n",
    "\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print('=> loading checkpoint \"{}\"'.format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        #args.start_epoch = checkpoint['epoch']\n",
    "        #best_prec = checkpoint['best_acc']\n",
    "        checkpoint['state_dict']['module.classifier.bias_r'] = torch.arange(10)\n",
    "        checkpoint['state_dict']['module.classifier.weight_a'] = torch.zeros(10,512,512)\n",
    "        #checkpoint['state_dict']['module.classifier.bias_r'] = torch.arange(10)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {} best_acc {})\".format(args.resume, checkpoint['epoch'], best_prec))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "print(model.module.linear)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if epoch < 150:\n",
    "        lr = args.lr\n",
    "    else:\n",
    "        lr = args.lr * 0.1\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    model.train()\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "    # train for one epoch\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input, target = input.cuda(), target.long().cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        ave_loss = train_loss/(i+1)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "\n",
    "        prec = train_correct / train_total\n",
    "        if (i+1) % args.print_freq == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.5f}, Train_Acc:{:.2f}%'.format(epoch+1, args.epochs, i+1, len(train_loader), loss, prec*100))\n",
    "\n",
    "\n",
    "    # evaluate on test set\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(valid_loader):\n",
    "            input, target = input.cuda(), target.long().cuda()\n",
    "\n",
    "            #input_digit = input[target==digit,:,:,:] \n",
    "            output = model(input)\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            valid_total = output.shape[0]\n",
    "            valid_correct = (predicted == target).sum().item()\n",
    "            prec = valid_correct / valid_total\n",
    "            test_accuracy.append(prec)\n",
    "            print('Accuary on test images:{:.2f}%'.format(prec*100))\n",
    "            best_prec = max(prec, best_prec)\n",
    "\n",
    "\n",
    "        #mean_difference = 0\n",
    "        #for i, (input, target) in enumerate(all_train_loader):\n",
    "         #   input = input.cuda()\n",
    "          #  output = model(input)\n",
    "\n",
    "           # for index in range(output.shape[0]):\n",
    "            #    output[index,:] = output[index,:]/torch.norm(output[index,:],2)\n",
    "             #   for digit in range(10):\n",
    "              #      if digit != target[index]:\n",
    "               #         mean_difference += output[index, target[index]]-output[index, digit]\n",
    "\n",
    "        #mean_difference_all.append(mean_difference/450000)\n",
    "        #output = torch.zeros(50000,10)\n",
    "        #bins=np.arange(min(torch.min(all_cos_cor), torch.min(all_cos_mis)),max(torch.max(all_cos_cor), torch.max(all_cos_mis)), 1/2000) \n",
    "        #frequency_each,_ = np.histogram(all_cos_mis.reshape(9*output.shape[0]).tolist(), bins = bins)\n",
    "        #frequency_each_c,_ = np.histogram(all_cos_cor.tolist(),bins = bins)\n",
    "        #cdf_mistaken = torch.cat((torch.Tensor([0]),torch.cumsum(torch.from_numpy(frequency_each)/(9*output.shape[0]), dim=0)),0)\n",
    "        #cdf_correct = torch.cat((torch.Tensor([0]),torch.cumsum(torch.from_numpy(frequency_each_c)/output.shape[0], dim=0)),0)\n",
    "        #KS_distance = torch.max(abs(cdf_correct-cdf_mistaken))\n",
    "        #KS_dis.append(KS_distance) \n",
    "        print('Best accuracy: {:.2f}%'.format(best_prec*100))\n",
    "\n",
    "    \n",
    "data=open(\"weights0.txt\",'w') \n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param, file=data)\n",
    "data.close()\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Linear(in_features=512, out_features=10, bias=True)\n",
      "Epoch [1/250], Step [60/391], Loss: 1.70681, Train_Acc:24.92%\n",
      "Epoch [1/250], Step [120/391], Loss: 1.67351, Train_Acc:31.73%\n",
      "Epoch [1/250], Step [180/391], Loss: 1.51718, Train_Acc:35.94%\n",
      "Epoch [1/250], Step [240/391], Loss: 1.37635, Train_Acc:38.98%\n",
      "Epoch [1/250], Step [300/391], Loss: 1.26848, Train_Acc:41.40%\n",
      "Epoch [1/250], Step [360/391], Loss: 1.27854, Train_Acc:43.37%\n",
      "Accuary on test images:56.25%\n",
      "Best accuracy: 56.25%\n",
      "Epoch [2/250], Step [60/391], Loss: 1.11836, Train_Acc:57.84%\n",
      "Epoch [2/250], Step [120/391], Loss: 1.04097, Train_Acc:58.66%\n",
      "Epoch [2/250], Step [180/391], Loss: 1.09919, Train_Acc:59.67%\n",
      "Epoch [2/250], Step [240/391], Loss: 1.14349, Train_Acc:60.24%\n",
      "Epoch [2/250], Step [300/391], Loss: 0.99827, Train_Acc:61.09%\n",
      "Epoch [2/250], Step [360/391], Loss: 0.74048, Train_Acc:61.76%\n",
      "Accuary on test images:65.98%\n",
      "Best accuracy: 65.98%\n",
      "Epoch [3/250], Step [60/391], Loss: 0.99671, Train_Acc:67.40%\n",
      "Epoch [3/250], Step [120/391], Loss: 0.90080, Train_Acc:67.92%\n",
      "Epoch [3/250], Step [180/391], Loss: 0.98156, Train_Acc:68.26%\n",
      "Epoch [3/250], Step [240/391], Loss: 0.79385, Train_Acc:68.89%\n",
      "Epoch [3/250], Step [300/391], Loss: 0.91230, Train_Acc:69.17%\n",
      "Epoch [3/250], Step [360/391], Loss: 0.78117, Train_Acc:69.54%\n",
      "Accuary on test images:71.60%\n",
      "Best accuracy: 71.60%\n",
      "Epoch [4/250], Step [60/391], Loss: 0.88421, Train_Acc:73.20%\n",
      "Epoch [4/250], Step [120/391], Loss: 0.57088, Train_Acc:73.59%\n",
      "Epoch [4/250], Step [180/391], Loss: 0.50797, Train_Acc:73.86%\n",
      "Epoch [4/250], Step [240/391], Loss: 0.72562, Train_Acc:74.11%\n",
      "Epoch [4/250], Step [300/391], Loss: 0.64311, Train_Acc:74.30%\n",
      "Epoch [4/250], Step [360/391], Loss: 0.64237, Train_Acc:74.79%\n",
      "Accuary on test images:75.72%\n",
      "Best accuracy: 75.72%\n",
      "Epoch [5/250], Step [60/391], Loss: 0.61760, Train_Acc:78.62%\n",
      "Epoch [5/250], Step [120/391], Loss: 0.73979, Train_Acc:77.96%\n",
      "Epoch [5/250], Step [180/391], Loss: 0.73210, Train_Acc:78.19%\n",
      "Epoch [5/250], Step [240/391], Loss: 0.54869, Train_Acc:78.23%\n",
      "Epoch [5/250], Step [300/391], Loss: 0.69145, Train_Acc:78.27%\n",
      "Epoch [5/250], Step [360/391], Loss: 0.54870, Train_Acc:78.28%\n",
      "Accuary on test images:78.10%\n",
      "Best accuracy: 78.10%\n",
      "Epoch [6/250], Step [60/391], Loss: 0.50902, Train_Acc:80.46%\n",
      "Epoch [6/250], Step [120/391], Loss: 0.56984, Train_Acc:80.48%\n",
      "Epoch [6/250], Step [180/391], Loss: 0.45203, Train_Acc:80.41%\n",
      "Epoch [6/250], Step [240/391], Loss: 0.43202, Train_Acc:80.55%\n",
      "Epoch [6/250], Step [300/391], Loss: 0.70062, Train_Acc:80.60%\n",
      "Epoch [6/250], Step [360/391], Loss: 0.51235, Train_Acc:80.72%\n",
      "Accuary on test images:80.51%\n",
      "Best accuracy: 80.51%\n",
      "Epoch [7/250], Step [60/391], Loss: 0.55975, Train_Acc:81.93%\n",
      "Epoch [7/250], Step [120/391], Loss: 0.57350, Train_Acc:82.07%\n",
      "Epoch [7/250], Step [180/391], Loss: 0.49881, Train_Acc:82.23%\n",
      "Epoch [7/250], Step [240/391], Loss: 0.54033, Train_Acc:82.35%\n",
      "Epoch [7/250], Step [300/391], Loss: 0.37222, Train_Acc:82.48%\n",
      "Epoch [7/250], Step [360/391], Loss: 0.41914, Train_Acc:82.61%\n",
      "Accuary on test images:81.93%\n",
      "Best accuracy: 81.93%\n",
      "Epoch [8/250], Step [60/391], Loss: 0.40128, Train_Acc:84.61%\n",
      "Epoch [8/250], Step [120/391], Loss: 0.55620, Train_Acc:84.06%\n",
      "Epoch [8/250], Step [180/391], Loss: 0.58079, Train_Acc:83.94%\n",
      "Epoch [8/250], Step [240/391], Loss: 0.43121, Train_Acc:83.99%\n",
      "Epoch [8/250], Step [300/391], Loss: 0.43694, Train_Acc:84.05%\n",
      "Epoch [8/250], Step [360/391], Loss: 0.47383, Train_Acc:84.08%\n",
      "Accuary on test images:79.62%\n",
      "Best accuracy: 81.93%\n",
      "Epoch [9/250], Step [60/391], Loss: 0.35983, Train_Acc:85.81%\n",
      "Epoch [9/250], Step [120/391], Loss: 0.39027, Train_Acc:86.13%\n",
      "Epoch [9/250], Step [180/391], Loss: 0.49570, Train_Acc:85.81%\n",
      "Epoch [9/250], Step [240/391], Loss: 0.46330, Train_Acc:85.76%\n",
      "Epoch [9/250], Step [300/391], Loss: 0.30853, Train_Acc:85.62%\n",
      "Epoch [9/250], Step [360/391], Loss: 0.43116, Train_Acc:85.51%\n",
      "Accuary on test images:83.85%\n",
      "Best accuracy: 83.85%\n",
      "Epoch [10/250], Step [60/391], Loss: 0.44753, Train_Acc:86.04%\n",
      "Epoch [10/250], Step [120/391], Loss: 0.37747, Train_Acc:86.68%\n",
      "Epoch [10/250], Step [180/391], Loss: 0.37991, Train_Acc:86.62%\n",
      "Epoch [10/250], Step [240/391], Loss: 0.44041, Train_Acc:86.53%\n",
      "Epoch [10/250], Step [300/391], Loss: 0.32781, Train_Acc:86.43%\n",
      "Epoch [10/250], Step [360/391], Loss: 0.36673, Train_Acc:86.47%\n",
      "Accuary on test images:84.24%\n",
      "Best accuracy: 84.24%\n",
      "Epoch [11/250], Step [60/391], Loss: 0.30643, Train_Acc:87.28%\n",
      "Epoch [11/250], Step [120/391], Loss: 0.30934, Train_Acc:87.58%\n",
      "Epoch [11/250], Step [180/391], Loss: 0.42370, Train_Acc:87.66%\n",
      "Epoch [11/250], Step [240/391], Loss: 0.31777, Train_Acc:87.57%\n",
      "Epoch [11/250], Step [300/391], Loss: 0.40644, Train_Acc:87.46%\n",
      "Epoch [11/250], Step [360/391], Loss: 0.38715, Train_Acc:87.32%\n",
      "Accuary on test images:85.17%\n",
      "Best accuracy: 85.17%\n",
      "Epoch [12/250], Step [60/391], Loss: 0.40407, Train_Acc:88.40%\n",
      "Epoch [12/250], Step [120/391], Loss: 0.34098, Train_Acc:88.21%\n",
      "Epoch [12/250], Step [180/391], Loss: 0.34085, Train_Acc:88.11%\n",
      "Epoch [12/250], Step [240/391], Loss: 0.33955, Train_Acc:88.19%\n",
      "Epoch [12/250], Step [300/391], Loss: 0.44167, Train_Acc:88.34%\n",
      "Epoch [12/250], Step [360/391], Loss: 0.35406, Train_Acc:88.27%\n",
      "Accuary on test images:85.18%\n",
      "Best accuracy: 85.18%\n",
      "Epoch [13/250], Step [60/391], Loss: 0.28315, Train_Acc:88.80%\n",
      "Epoch [13/250], Step [120/391], Loss: 0.19828, Train_Acc:88.70%\n",
      "Epoch [13/250], Step [180/391], Loss: 0.38143, Train_Acc:88.74%\n",
      "Epoch [13/250], Step [240/391], Loss: 0.35527, Train_Acc:88.81%\n",
      "Epoch [13/250], Step [300/391], Loss: 0.28305, Train_Acc:88.74%\n",
      "Epoch [13/250], Step [360/391], Loss: 0.25648, Train_Acc:88.72%\n",
      "Accuary on test images:85.66%\n",
      "Best accuracy: 85.66%\n",
      "Epoch [14/250], Step [60/391], Loss: 0.47034, Train_Acc:89.26%\n",
      "Epoch [14/250], Step [120/391], Loss: 0.26667, Train_Acc:89.57%\n",
      "Epoch [14/250], Step [180/391], Loss: 0.27084, Train_Acc:89.67%\n",
      "Epoch [14/250], Step [240/391], Loss: 0.35818, Train_Acc:89.66%\n",
      "Epoch [14/250], Step [300/391], Loss: 0.31521, Train_Acc:89.49%\n",
      "Epoch [14/250], Step [360/391], Loss: 0.22746, Train_Acc:89.40%\n",
      "Accuary on test images:86.34%\n",
      "Best accuracy: 86.34%\n",
      "Epoch [15/250], Step [60/391], Loss: 0.26521, Train_Acc:90.35%\n",
      "Epoch [15/250], Step [120/391], Loss: 0.25490, Train_Acc:90.17%\n",
      "Epoch [15/250], Step [180/391], Loss: 0.26931, Train_Acc:90.15%\n",
      "Epoch [15/250], Step [240/391], Loss: 0.23498, Train_Acc:90.18%\n",
      "Epoch [15/250], Step [300/391], Loss: 0.26223, Train_Acc:90.01%\n",
      "Epoch [15/250], Step [360/391], Loss: 0.24997, Train_Acc:90.08%\n",
      "Accuary on test images:87.15%\n",
      "Best accuracy: 87.15%\n",
      "Epoch [16/250], Step [60/391], Loss: 0.24525, Train_Acc:91.24%\n",
      "Epoch [16/250], Step [120/391], Loss: 0.27430, Train_Acc:90.80%\n",
      "Epoch [16/250], Step [180/391], Loss: 0.30004, Train_Acc:90.92%\n",
      "Epoch [16/250], Step [240/391], Loss: 0.32984, Train_Acc:90.86%\n",
      "Epoch [16/250], Step [300/391], Loss: 0.28224, Train_Acc:90.80%\n",
      "Epoch [16/250], Step [360/391], Loss: 0.26404, Train_Acc:90.80%\n",
      "Accuary on test images:86.64%\n",
      "Best accuracy: 87.15%\n",
      "Epoch [17/250], Step [60/391], Loss: 0.33282, Train_Acc:91.42%\n",
      "Epoch [17/250], Step [120/391], Loss: 0.28492, Train_Acc:91.39%\n",
      "Epoch [17/250], Step [180/391], Loss: 0.18732, Train_Acc:91.30%\n",
      "Epoch [17/250], Step [240/391], Loss: 0.26758, Train_Acc:91.26%\n",
      "Epoch [17/250], Step [300/391], Loss: 0.18269, Train_Acc:91.15%\n",
      "Epoch [17/250], Step [360/391], Loss: 0.23537, Train_Acc:91.09%\n",
      "Accuary on test images:86.03%\n",
      "Best accuracy: 87.15%\n",
      "Epoch [18/250], Step [60/391], Loss: 0.19950, Train_Acc:92.84%\n",
      "Epoch [18/250], Step [120/391], Loss: 0.18163, Train_Acc:92.28%\n",
      "Epoch [18/250], Step [180/391], Loss: 0.18809, Train_Acc:91.89%\n",
      "Epoch [18/250], Step [240/391], Loss: 0.27297, Train_Acc:91.68%\n",
      "Epoch [18/250], Step [300/391], Loss: 0.38870, Train_Acc:91.65%\n",
      "Epoch [18/250], Step [360/391], Loss: 0.20031, Train_Acc:91.68%\n",
      "Accuary on test images:85.70%\n",
      "Best accuracy: 87.15%\n",
      "Epoch [19/250], Step [60/391], Loss: 0.23502, Train_Acc:92.04%\n",
      "Epoch [19/250], Step [120/391], Loss: 0.20119, Train_Acc:92.64%\n",
      "Epoch [19/250], Step [180/391], Loss: 0.28586, Train_Acc:92.46%\n",
      "Epoch [19/250], Step [240/391], Loss: 0.25274, Train_Acc:92.31%\n",
      "Epoch [19/250], Step [300/391], Loss: 0.18217, Train_Acc:92.21%\n",
      "Epoch [19/250], Step [360/391], Loss: 0.23522, Train_Acc:92.17%\n",
      "Accuary on test images:87.76%\n",
      "Best accuracy: 87.76%\n",
      "Epoch [20/250], Step [60/391], Loss: 0.21310, Train_Acc:92.92%\n",
      "Epoch [20/250], Step [120/391], Loss: 0.15303, Train_Acc:92.59%\n",
      "Epoch [20/250], Step [180/391], Loss: 0.28480, Train_Acc:92.69%\n",
      "Epoch [20/250], Step [240/391], Loss: 0.16488, Train_Acc:92.60%\n",
      "Epoch [20/250], Step [300/391], Loss: 0.15720, Train_Acc:92.55%\n",
      "Epoch [20/250], Step [360/391], Loss: 0.21134, Train_Acc:92.45%\n",
      "Accuary on test images:87.10%\n",
      "Best accuracy: 87.76%\n",
      "Epoch [21/250], Step [60/391], Loss: 0.19479, Train_Acc:92.86%\n",
      "Epoch [21/250], Step [120/391], Loss: 0.15114, Train_Acc:92.94%\n",
      "Epoch [21/250], Step [180/391], Loss: 0.17200, Train_Acc:92.77%\n",
      "Epoch [21/250], Step [240/391], Loss: 0.22191, Train_Acc:92.64%\n",
      "Epoch [21/250], Step [300/391], Loss: 0.26169, Train_Acc:92.81%\n",
      "Epoch [21/250], Step [360/391], Loss: 0.23446, Train_Acc:92.71%\n",
      "Accuary on test images:87.39%\n",
      "Best accuracy: 87.76%\n",
      "Epoch [22/250], Step [60/391], Loss: 0.25052, Train_Acc:93.82%\n",
      "Epoch [22/250], Step [120/391], Loss: 0.13840, Train_Acc:93.80%\n",
      "Epoch [22/250], Step [180/391], Loss: 0.19758, Train_Acc:93.64%\n",
      "Epoch [22/250], Step [240/391], Loss: 0.14498, Train_Acc:93.47%\n",
      "Epoch [22/250], Step [300/391], Loss: 0.15361, Train_Acc:93.39%\n",
      "Epoch [22/250], Step [360/391], Loss: 0.11348, Train_Acc:93.31%\n",
      "Accuary on test images:86.47%\n",
      "Best accuracy: 87.76%\n",
      "Epoch [23/250], Step [60/391], Loss: 0.12672, Train_Acc:93.88%\n",
      "Epoch [23/250], Step [120/391], Loss: 0.23923, Train_Acc:93.91%\n",
      "Epoch [23/250], Step [180/391], Loss: 0.17331, Train_Acc:93.84%\n",
      "Epoch [23/250], Step [240/391], Loss: 0.18570, Train_Acc:93.67%\n",
      "Epoch [23/250], Step [300/391], Loss: 0.21743, Train_Acc:93.47%\n",
      "Epoch [23/250], Step [360/391], Loss: 0.16389, Train_Acc:93.42%\n",
      "Accuary on test images:87.42%\n",
      "Best accuracy: 87.76%\n",
      "Epoch [24/250], Step [60/391], Loss: 0.21121, Train_Acc:93.74%\n",
      "Epoch [24/250], Step [120/391], Loss: 0.09273, Train_Acc:93.80%\n",
      "Epoch [24/250], Step [180/391], Loss: 0.12992, Train_Acc:94.08%\n",
      "Epoch [24/250], Step [240/391], Loss: 0.22835, Train_Acc:93.84%\n",
      "Epoch [24/250], Step [300/391], Loss: 0.23744, Train_Acc:93.80%\n",
      "Epoch [24/250], Step [360/391], Loss: 0.17641, Train_Acc:93.75%\n",
      "Accuary on test images:87.91%\n",
      "Best accuracy: 87.91%\n",
      "Epoch [25/250], Step [60/391], Loss: 0.11095, Train_Acc:94.65%\n",
      "Epoch [25/250], Step [120/391], Loss: 0.19363, Train_Acc:94.51%\n",
      "Epoch [25/250], Step [180/391], Loss: 0.14144, Train_Acc:94.36%\n",
      "Epoch [25/250], Step [240/391], Loss: 0.16417, Train_Acc:94.24%\n",
      "Epoch [25/250], Step [300/391], Loss: 0.17053, Train_Acc:94.15%\n",
      "Epoch [25/250], Step [360/391], Loss: 0.12043, Train_Acc:94.18%\n",
      "Accuary on test images:88.39%\n",
      "Best accuracy: 88.39%\n",
      "Epoch [26/250], Step [60/391], Loss: 0.22152, Train_Acc:94.78%\n",
      "Epoch [26/250], Step [120/391], Loss: 0.12170, Train_Acc:94.45%\n",
      "Epoch [26/250], Step [180/391], Loss: 0.19541, Train_Acc:94.50%\n",
      "Epoch [26/250], Step [240/391], Loss: 0.12564, Train_Acc:94.56%\n",
      "Epoch [26/250], Step [300/391], Loss: 0.19760, Train_Acc:94.53%\n",
      "Epoch [26/250], Step [360/391], Loss: 0.24635, Train_Acc:94.52%\n",
      "Accuary on test images:88.21%\n",
      "Best accuracy: 88.39%\n",
      "Epoch [27/250], Step [60/391], Loss: 0.04726, Train_Acc:95.22%\n",
      "Epoch [27/250], Step [120/391], Loss: 0.13824, Train_Acc:95.17%\n",
      "Epoch [27/250], Step [180/391], Loss: 0.19808, Train_Acc:94.89%\n",
      "Epoch [27/250], Step [240/391], Loss: 0.13838, Train_Acc:94.79%\n",
      "Epoch [27/250], Step [300/391], Loss: 0.11064, Train_Acc:94.84%\n",
      "Epoch [27/250], Step [360/391], Loss: 0.16027, Train_Acc:94.76%\n",
      "Accuary on test images:87.96%\n",
      "Best accuracy: 88.39%\n",
      "Epoch [28/250], Step [60/391], Loss: 0.16369, Train_Acc:95.05%\n",
      "Epoch [28/250], Step [120/391], Loss: 0.11897, Train_Acc:95.40%\n",
      "Epoch [28/250], Step [180/391], Loss: 0.11234, Train_Acc:95.33%\n",
      "Epoch [28/250], Step [240/391], Loss: 0.13790, Train_Acc:95.03%\n",
      "Epoch [28/250], Step [300/391], Loss: 0.12939, Train_Acc:94.92%\n",
      "Epoch [28/250], Step [360/391], Loss: 0.12025, Train_Acc:94.92%\n",
      "Accuary on test images:86.95%\n",
      "Best accuracy: 88.39%\n",
      "Epoch [29/250], Step [60/391], Loss: 0.09128, Train_Acc:95.48%\n",
      "Epoch [29/250], Step [120/391], Loss: 0.10917, Train_Acc:95.48%\n",
      "Epoch [29/250], Step [180/391], Loss: 0.17227, Train_Acc:95.49%\n",
      "Epoch [29/250], Step [240/391], Loss: 0.14399, Train_Acc:95.53%\n",
      "Epoch [29/250], Step [300/391], Loss: 0.12872, Train_Acc:95.40%\n",
      "Epoch [29/250], Step [360/391], Loss: 0.12191, Train_Acc:95.29%\n",
      "Accuary on test images:88.73%\n",
      "Best accuracy: 88.73%\n",
      "Epoch [30/250], Step [60/391], Loss: 0.12965, Train_Acc:96.09%\n",
      "Epoch [30/250], Step [120/391], Loss: 0.28412, Train_Acc:95.70%\n",
      "Epoch [30/250], Step [180/391], Loss: 0.15366, Train_Acc:95.44%\n",
      "Epoch [30/250], Step [240/391], Loss: 0.16534, Train_Acc:95.33%\n",
      "Epoch [30/250], Step [300/391], Loss: 0.12786, Train_Acc:95.32%\n",
      "Epoch [30/250], Step [360/391], Loss: 0.19182, Train_Acc:95.29%\n",
      "Accuary on test images:89.03%\n",
      "Best accuracy: 89.03%\n",
      "Epoch [31/250], Step [60/391], Loss: 0.05687, Train_Acc:96.12%\n",
      "Epoch [31/250], Step [120/391], Loss: 0.13658, Train_Acc:96.06%\n",
      "Epoch [31/250], Step [180/391], Loss: 0.19249, Train_Acc:95.96%\n",
      "Epoch [31/250], Step [240/391], Loss: 0.18153, Train_Acc:95.92%\n",
      "Epoch [31/250], Step [300/391], Loss: 0.06238, Train_Acc:95.85%\n",
      "Epoch [31/250], Step [360/391], Loss: 0.09851, Train_Acc:95.74%\n",
      "Accuary on test images:89.32%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [32/250], Step [60/391], Loss: 0.06070, Train_Acc:96.51%\n",
      "Epoch [32/250], Step [120/391], Loss: 0.12784, Train_Acc:96.31%\n",
      "Epoch [32/250], Step [180/391], Loss: 0.19420, Train_Acc:96.20%\n",
      "Epoch [32/250], Step [240/391], Loss: 0.21719, Train_Acc:96.00%\n",
      "Epoch [32/250], Step [300/391], Loss: 0.09729, Train_Acc:95.93%\n",
      "Epoch [32/250], Step [360/391], Loss: 0.14525, Train_Acc:95.92%\n",
      "Accuary on test images:88.83%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [33/250], Step [60/391], Loss: 0.07751, Train_Acc:96.03%\n",
      "Epoch [33/250], Step [120/391], Loss: 0.13018, Train_Acc:96.21%\n",
      "Epoch [33/250], Step [180/391], Loss: 0.19279, Train_Acc:96.12%\n",
      "Epoch [33/250], Step [240/391], Loss: 0.10907, Train_Acc:96.02%\n",
      "Epoch [33/250], Step [300/391], Loss: 0.14978, Train_Acc:96.00%\n",
      "Epoch [33/250], Step [360/391], Loss: 0.07948, Train_Acc:95.97%\n",
      "Accuary on test images:88.37%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [34/250], Step [60/391], Loss: 0.07742, Train_Acc:96.63%\n",
      "Epoch [34/250], Step [120/391], Loss: 0.07656, Train_Acc:96.45%\n",
      "Epoch [34/250], Step [180/391], Loss: 0.12794, Train_Acc:96.24%\n",
      "Epoch [34/250], Step [240/391], Loss: 0.17397, Train_Acc:96.18%\n",
      "Epoch [34/250], Step [300/391], Loss: 0.09392, Train_Acc:96.16%\n",
      "Epoch [34/250], Step [360/391], Loss: 0.20136, Train_Acc:96.13%\n",
      "Accuary on test images:89.07%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [35/250], Step [60/391], Loss: 0.14585, Train_Acc:96.85%\n",
      "Epoch [35/250], Step [120/391], Loss: 0.11917, Train_Acc:96.54%\n",
      "Epoch [35/250], Step [180/391], Loss: 0.09958, Train_Acc:96.53%\n",
      "Epoch [35/250], Step [240/391], Loss: 0.08304, Train_Acc:96.46%\n",
      "Epoch [35/250], Step [300/391], Loss: 0.13860, Train_Acc:96.44%\n",
      "Epoch [35/250], Step [360/391], Loss: 0.07201, Train_Acc:96.40%\n",
      "Accuary on test images:88.03%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [36/250], Step [60/391], Loss: 0.06654, Train_Acc:96.54%\n",
      "Epoch [36/250], Step [120/391], Loss: 0.18068, Train_Acc:96.50%\n",
      "Epoch [36/250], Step [180/391], Loss: 0.12198, Train_Acc:96.46%\n",
      "Epoch [36/250], Step [240/391], Loss: 0.11798, Train_Acc:96.51%\n",
      "Epoch [36/250], Step [300/391], Loss: 0.12679, Train_Acc:96.50%\n",
      "Epoch [36/250], Step [360/391], Loss: 0.10238, Train_Acc:96.43%\n",
      "Accuary on test images:87.34%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [37/250], Step [60/391], Loss: 0.08268, Train_Acc:97.01%\n",
      "Epoch [37/250], Step [120/391], Loss: 0.08962, Train_Acc:97.02%\n",
      "Epoch [37/250], Step [180/391], Loss: 0.11628, Train_Acc:96.80%\n",
      "Epoch [37/250], Step [240/391], Loss: 0.05691, Train_Acc:96.74%\n",
      "Epoch [37/250], Step [300/391], Loss: 0.06414, Train_Acc:96.69%\n",
      "Epoch [37/250], Step [360/391], Loss: 0.12374, Train_Acc:96.71%\n",
      "Accuary on test images:89.09%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [38/250], Step [60/391], Loss: 0.09252, Train_Acc:96.82%\n",
      "Epoch [38/250], Step [120/391], Loss: 0.06305, Train_Acc:96.76%\n",
      "Epoch [38/250], Step [180/391], Loss: 0.09333, Train_Acc:96.77%\n",
      "Epoch [38/250], Step [240/391], Loss: 0.14392, Train_Acc:96.68%\n",
      "Epoch [38/250], Step [300/391], Loss: 0.08455, Train_Acc:96.70%\n",
      "Epoch [38/250], Step [360/391], Loss: 0.05362, Train_Acc:96.67%\n",
      "Accuary on test images:89.25%\n",
      "Best accuracy: 89.32%\n",
      "Epoch [39/250], Step [60/391], Loss: 0.14132, Train_Acc:97.57%\n",
      "Epoch [39/250], Step [120/391], Loss: 0.11812, Train_Acc:97.49%\n",
      "Epoch [39/250], Step [180/391], Loss: 0.09260, Train_Acc:97.30%\n",
      "Epoch [39/250], Step [240/391], Loss: 0.15777, Train_Acc:97.13%\n",
      "Epoch [39/250], Step [300/391], Loss: 0.10531, Train_Acc:97.05%\n",
      "Epoch [39/250], Step [360/391], Loss: 0.09330, Train_Acc:96.93%\n",
      "Accuary on test images:89.85%\n",
      "Best accuracy: 89.85%\n",
      "Epoch [40/250], Step [60/391], Loss: 0.04099, Train_Acc:97.24%\n",
      "Epoch [40/250], Step [120/391], Loss: 0.09791, Train_Acc:97.30%\n",
      "Epoch [40/250], Step [180/391], Loss: 0.12895, Train_Acc:97.20%\n",
      "Epoch [40/250], Step [240/391], Loss: 0.14502, Train_Acc:97.09%\n",
      "Epoch [40/250], Step [300/391], Loss: 0.11806, Train_Acc:97.02%\n",
      "Epoch [40/250], Step [360/391], Loss: 0.06804, Train_Acc:96.94%\n",
      "Accuary on test images:89.02%\n",
      "Best accuracy: 89.85%\n",
      "Epoch [41/250], Step [60/391], Loss: 0.08058, Train_Acc:97.04%\n",
      "Epoch [41/250], Step [120/391], Loss: 0.06334, Train_Acc:97.27%\n",
      "Epoch [41/250], Step [180/391], Loss: 0.03552, Train_Acc:97.35%\n",
      "Epoch [41/250], Step [240/391], Loss: 0.12480, Train_Acc:97.34%\n",
      "Epoch [41/250], Step [300/391], Loss: 0.10466, Train_Acc:97.28%\n",
      "Epoch [41/250], Step [360/391], Loss: 0.05477, Train_Acc:97.21%\n",
      "Accuary on test images:88.71%\n",
      "Best accuracy: 89.85%\n",
      "Epoch [42/250], Step [60/391], Loss: 0.07000, Train_Acc:97.19%\n",
      "Epoch [42/250], Step [120/391], Loss: 0.06654, Train_Acc:97.50%\n",
      "Epoch [42/250], Step [180/391], Loss: 0.04971, Train_Acc:97.55%\n",
      "Epoch [42/250], Step [240/391], Loss: 0.11011, Train_Acc:97.51%\n",
      "Epoch [42/250], Step [300/391], Loss: 0.08782, Train_Acc:97.46%\n",
      "Epoch [42/250], Step [360/391], Loss: 0.06864, Train_Acc:97.34%\n",
      "Accuary on test images:90.43%\n",
      "Best accuracy: 90.43%\n",
      "Epoch [43/250], Step [60/391], Loss: 0.06280, Train_Acc:97.63%\n",
      "Epoch [43/250], Step [120/391], Loss: 0.04243, Train_Acc:97.62%\n",
      "Epoch [43/250], Step [180/391], Loss: 0.10421, Train_Acc:97.47%\n",
      "Epoch [43/250], Step [240/391], Loss: 0.09674, Train_Acc:97.37%\n",
      "Epoch [43/250], Step [300/391], Loss: 0.07829, Train_Acc:97.33%\n",
      "Epoch [43/250], Step [360/391], Loss: 0.09139, Train_Acc:97.30%\n",
      "Accuary on test images:89.89%\n",
      "Best accuracy: 90.43%\n",
      "Epoch [44/250], Step [60/391], Loss: 0.09791, Train_Acc:97.62%\n",
      "Epoch [44/250], Step [120/391], Loss: 0.04320, Train_Acc:97.74%\n",
      "Epoch [44/250], Step [180/391], Loss: 0.03148, Train_Acc:97.73%\n",
      "Epoch [44/250], Step [240/391], Loss: 0.06100, Train_Acc:97.67%\n",
      "Epoch [44/250], Step [300/391], Loss: 0.10716, Train_Acc:97.50%\n",
      "Epoch [44/250], Step [360/391], Loss: 0.07913, Train_Acc:97.43%\n",
      "Accuary on test images:87.92%\n",
      "Best accuracy: 90.43%\n",
      "Epoch [45/250], Step [60/391], Loss: 0.07340, Train_Acc:97.17%\n",
      "Epoch [45/250], Step [120/391], Loss: 0.15614, Train_Acc:97.42%\n",
      "Epoch [45/250], Step [180/391], Loss: 0.06058, Train_Acc:97.50%\n",
      "Epoch [45/250], Step [240/391], Loss: 0.15740, Train_Acc:97.48%\n",
      "Epoch [45/250], Step [300/391], Loss: 0.04220, Train_Acc:97.54%\n",
      "Epoch [45/250], Step [360/391], Loss: 0.06149, Train_Acc:97.53%\n",
      "Accuary on test images:88.73%\n",
      "Best accuracy: 90.43%\n",
      "Epoch [46/250], Step [60/391], Loss: 0.05598, Train_Acc:97.83%\n",
      "Epoch [46/250], Step [120/391], Loss: 0.05112, Train_Acc:97.85%\n",
      "Epoch [46/250], Step [180/391], Loss: 0.11617, Train_Acc:97.85%\n",
      "Epoch [46/250], Step [240/391], Loss: 0.05305, Train_Acc:97.77%\n",
      "Epoch [46/250], Step [300/391], Loss: 0.02962, Train_Acc:97.67%\n",
      "Epoch [46/250], Step [360/391], Loss: 0.07298, Train_Acc:97.65%\n",
      "Accuary on test images:89.99%\n",
      "Best accuracy: 90.43%\n",
      "Epoch [47/250], Step [60/391], Loss: 0.04450, Train_Acc:97.81%\n",
      "Epoch [47/250], Step [120/391], Loss: 0.05301, Train_Acc:97.69%\n",
      "Epoch [47/250], Step [180/391], Loss: 0.03238, Train_Acc:97.71%\n",
      "Epoch [47/250], Step [240/391], Loss: 0.05104, Train_Acc:97.67%\n",
      "Epoch [47/250], Step [300/391], Loss: 0.07840, Train_Acc:97.69%\n",
      "Epoch [47/250], Step [360/391], Loss: 0.05422, Train_Acc:97.66%\n",
      "Accuary on test images:89.58%\n",
      "Best accuracy: 90.43%\n",
      "Epoch [48/250], Step [60/391], Loss: 0.08928, Train_Acc:97.84%\n",
      "Epoch [48/250], Step [120/391], Loss: 0.09267, Train_Acc:97.92%\n",
      "Epoch [48/250], Step [180/391], Loss: 0.04888, Train_Acc:97.83%\n",
      "Epoch [48/250], Step [240/391], Loss: 0.08437, Train_Acc:97.83%\n",
      "Epoch [48/250], Step [300/391], Loss: 0.06142, Train_Acc:97.78%\n",
      "Epoch [48/250], Step [360/391], Loss: 0.05319, Train_Acc:97.70%\n",
      "Accuary on test images:90.52%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [49/250], Step [60/391], Loss: 0.04484, Train_Acc:97.97%\n",
      "Epoch [49/250], Step [120/391], Loss: 0.04953, Train_Acc:98.15%\n",
      "Epoch [49/250], Step [180/391], Loss: 0.09920, Train_Acc:98.04%\n",
      "Epoch [49/250], Step [240/391], Loss: 0.05919, Train_Acc:98.01%\n",
      "Epoch [49/250], Step [300/391], Loss: 0.07550, Train_Acc:97.98%\n",
      "Epoch [49/250], Step [360/391], Loss: 0.04687, Train_Acc:98.02%\n",
      "Accuary on test images:90.06%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [50/250], Step [60/391], Loss: 0.07491, Train_Acc:97.94%\n",
      "Epoch [50/250], Step [120/391], Loss: 0.02607, Train_Acc:97.98%\n",
      "Epoch [50/250], Step [180/391], Loss: 0.12949, Train_Acc:97.90%\n",
      "Epoch [50/250], Step [240/391], Loss: 0.02774, Train_Acc:97.97%\n",
      "Epoch [50/250], Step [300/391], Loss: 0.04232, Train_Acc:97.91%\n",
      "Epoch [50/250], Step [360/391], Loss: 0.02657, Train_Acc:97.93%\n",
      "Accuary on test images:90.36%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [51/250], Step [60/391], Loss: 0.02996, Train_Acc:98.16%\n",
      "Epoch [51/250], Step [120/391], Loss: 0.02313, Train_Acc:98.04%\n",
      "Epoch [51/250], Step [180/391], Loss: 0.08163, Train_Acc:97.92%\n",
      "Epoch [51/250], Step [240/391], Loss: 0.08351, Train_Acc:97.85%\n",
      "Epoch [51/250], Step [300/391], Loss: 0.05591, Train_Acc:97.78%\n",
      "Epoch [51/250], Step [360/391], Loss: 0.05782, Train_Acc:97.71%\n",
      "Accuary on test images:89.76%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [52/250], Step [60/391], Loss: 0.05589, Train_Acc:98.11%\n",
      "Epoch [52/250], Step [120/391], Loss: 0.03041, Train_Acc:98.11%\n",
      "Epoch [52/250], Step [180/391], Loss: 0.06074, Train_Acc:98.12%\n",
      "Epoch [52/250], Step [240/391], Loss: 0.02889, Train_Acc:98.16%\n",
      "Epoch [52/250], Step [300/391], Loss: 0.05529, Train_Acc:98.10%\n",
      "Epoch [52/250], Step [360/391], Loss: 0.06882, Train_Acc:98.01%\n",
      "Accuary on test images:90.03%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [53/250], Step [60/391], Loss: 0.03569, Train_Acc:98.06%\n",
      "Epoch [53/250], Step [120/391], Loss: 0.04388, Train_Acc:98.21%\n",
      "Epoch [53/250], Step [180/391], Loss: 0.03165, Train_Acc:98.18%\n",
      "Epoch [53/250], Step [240/391], Loss: 0.02270, Train_Acc:98.20%\n",
      "Epoch [53/250], Step [300/391], Loss: 0.06850, Train_Acc:98.11%\n",
      "Epoch [53/250], Step [360/391], Loss: 0.02623, Train_Acc:98.06%\n",
      "Accuary on test images:90.39%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [54/250], Step [60/391], Loss: 0.07093, Train_Acc:98.33%\n",
      "Epoch [54/250], Step [120/391], Loss: 0.07978, Train_Acc:98.23%\n",
      "Epoch [54/250], Step [180/391], Loss: 0.07830, Train_Acc:98.23%\n",
      "Epoch [54/250], Step [240/391], Loss: 0.06380, Train_Acc:98.13%\n",
      "Epoch [54/250], Step [300/391], Loss: 0.04889, Train_Acc:98.10%\n",
      "Epoch [54/250], Step [360/391], Loss: 0.06074, Train_Acc:98.14%\n",
      "Accuary on test images:90.02%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [55/250], Step [60/391], Loss: 0.06385, Train_Acc:98.40%\n",
      "Epoch [55/250], Step [120/391], Loss: 0.03665, Train_Acc:98.38%\n",
      "Epoch [55/250], Step [180/391], Loss: 0.04038, Train_Acc:98.44%\n",
      "Epoch [55/250], Step [240/391], Loss: 0.07086, Train_Acc:98.40%\n",
      "Epoch [55/250], Step [300/391], Loss: 0.14324, Train_Acc:98.27%\n",
      "Epoch [55/250], Step [360/391], Loss: 0.07864, Train_Acc:98.18%\n",
      "Accuary on test images:90.16%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [56/250], Step [60/391], Loss: 0.05748, Train_Acc:98.35%\n",
      "Epoch [56/250], Step [120/391], Loss: 0.06748, Train_Acc:98.43%\n",
      "Epoch [56/250], Step [180/391], Loss: 0.04062, Train_Acc:98.51%\n",
      "Epoch [56/250], Step [240/391], Loss: 0.05258, Train_Acc:98.47%\n",
      "Epoch [56/250], Step [300/391], Loss: 0.03192, Train_Acc:98.43%\n",
      "Epoch [56/250], Step [360/391], Loss: 0.03650, Train_Acc:98.41%\n",
      "Accuary on test images:90.39%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [57/250], Step [60/391], Loss: 0.02093, Train_Acc:98.57%\n",
      "Epoch [57/250], Step [120/391], Loss: 0.04157, Train_Acc:98.64%\n",
      "Epoch [57/250], Step [180/391], Loss: 0.01873, Train_Acc:98.72%\n",
      "Epoch [57/250], Step [240/391], Loss: 0.02316, Train_Acc:98.75%\n",
      "Epoch [57/250], Step [300/391], Loss: 0.01072, Train_Acc:98.71%\n",
      "Epoch [57/250], Step [360/391], Loss: 0.07445, Train_Acc:98.63%\n",
      "Accuary on test images:90.25%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [58/250], Step [60/391], Loss: 0.06034, Train_Acc:98.71%\n",
      "Epoch [58/250], Step [120/391], Loss: 0.02356, Train_Acc:98.60%\n",
      "Epoch [58/250], Step [180/391], Loss: 0.04967, Train_Acc:98.52%\n",
      "Epoch [58/250], Step [240/391], Loss: 0.05529, Train_Acc:98.50%\n",
      "Epoch [58/250], Step [300/391], Loss: 0.07319, Train_Acc:98.48%\n",
      "Epoch [58/250], Step [360/391], Loss: 0.02988, Train_Acc:98.42%\n",
      "Accuary on test images:90.35%\n",
      "Best accuracy: 90.52%\n",
      "Epoch [59/250], Step [60/391], Loss: 0.01783, Train_Acc:98.92%\n",
      "Epoch [59/250], Step [120/391], Loss: 0.06215, Train_Acc:98.78%\n",
      "Epoch [59/250], Step [180/391], Loss: 0.01932, Train_Acc:98.75%\n",
      "Epoch [59/250], Step [240/391], Loss: 0.04132, Train_Acc:98.65%\n",
      "Epoch [59/250], Step [300/391], Loss: 0.06030, Train_Acc:98.55%\n",
      "Epoch [59/250], Step [360/391], Loss: 0.02556, Train_Acc:98.56%\n",
      "Accuary on test images:91.01%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [60/250], Step [60/391], Loss: 0.04934, Train_Acc:98.67%\n",
      "Epoch [60/250], Step [120/391], Loss: 0.06323, Train_Acc:98.68%\n",
      "Epoch [60/250], Step [180/391], Loss: 0.06238, Train_Acc:98.55%\n",
      "Epoch [60/250], Step [240/391], Loss: 0.04136, Train_Acc:98.54%\n",
      "Epoch [60/250], Step [300/391], Loss: 0.02629, Train_Acc:98.59%\n",
      "Epoch [60/250], Step [360/391], Loss: 0.02732, Train_Acc:98.58%\n",
      "Accuary on test images:90.53%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [61/250], Step [60/391], Loss: 0.00731, Train_Acc:98.70%\n",
      "Epoch [61/250], Step [120/391], Loss: 0.02117, Train_Acc:98.61%\n",
      "Epoch [61/250], Step [180/391], Loss: 0.02528, Train_Acc:98.72%\n",
      "Epoch [61/250], Step [240/391], Loss: 0.10931, Train_Acc:98.70%\n",
      "Epoch [61/250], Step [300/391], Loss: 0.10228, Train_Acc:98.66%\n",
      "Epoch [61/250], Step [360/391], Loss: 0.03006, Train_Acc:98.56%\n",
      "Accuary on test images:89.78%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [62/250], Step [60/391], Loss: 0.03299, Train_Acc:98.59%\n",
      "Epoch [62/250], Step [120/391], Loss: 0.08312, Train_Acc:98.56%\n",
      "Epoch [62/250], Step [180/391], Loss: 0.03580, Train_Acc:98.61%\n",
      "Epoch [62/250], Step [240/391], Loss: 0.06656, Train_Acc:98.61%\n",
      "Epoch [62/250], Step [300/391], Loss: 0.04250, Train_Acc:98.54%\n",
      "Epoch [62/250], Step [360/391], Loss: 0.11187, Train_Acc:98.52%\n",
      "Accuary on test images:89.78%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [63/250], Step [60/391], Loss: 0.04012, Train_Acc:98.87%\n",
      "Epoch [63/250], Step [120/391], Loss: 0.01498, Train_Acc:98.67%\n",
      "Epoch [63/250], Step [180/391], Loss: 0.05139, Train_Acc:98.64%\n",
      "Epoch [63/250], Step [240/391], Loss: 0.04296, Train_Acc:98.60%\n",
      "Epoch [63/250], Step [300/391], Loss: 0.05269, Train_Acc:98.60%\n",
      "Epoch [63/250], Step [360/391], Loss: 0.02472, Train_Acc:98.57%\n",
      "Accuary on test images:90.34%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [64/250], Step [60/391], Loss: 0.03472, Train_Acc:98.92%\n",
      "Epoch [64/250], Step [120/391], Loss: 0.10150, Train_Acc:98.87%\n",
      "Epoch [64/250], Step [180/391], Loss: 0.02764, Train_Acc:98.84%\n",
      "Epoch [64/250], Step [240/391], Loss: 0.02326, Train_Acc:98.82%\n",
      "Epoch [64/250], Step [300/391], Loss: 0.04690, Train_Acc:98.79%\n",
      "Epoch [64/250], Step [360/391], Loss: 0.04634, Train_Acc:98.78%\n",
      "Accuary on test images:89.87%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [65/250], Step [60/391], Loss: 0.02660, Train_Acc:98.78%\n",
      "Epoch [65/250], Step [120/391], Loss: 0.00461, Train_Acc:98.83%\n",
      "Epoch [65/250], Step [180/391], Loss: 0.01364, Train_Acc:98.82%\n",
      "Epoch [65/250], Step [240/391], Loss: 0.03100, Train_Acc:98.87%\n",
      "Epoch [65/250], Step [300/391], Loss: 0.04575, Train_Acc:98.87%\n",
      "Epoch [65/250], Step [360/391], Loss: 0.04434, Train_Acc:98.82%\n",
      "Accuary on test images:90.61%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [66/250], Step [60/391], Loss: 0.03671, Train_Acc:98.82%\n",
      "Epoch [66/250], Step [120/391], Loss: 0.01227, Train_Acc:98.52%\n",
      "Epoch [66/250], Step [180/391], Loss: 0.06216, Train_Acc:98.58%\n",
      "Epoch [66/250], Step [240/391], Loss: 0.01814, Train_Acc:98.66%\n",
      "Epoch [66/250], Step [300/391], Loss: 0.05902, Train_Acc:98.68%\n",
      "Epoch [66/250], Step [360/391], Loss: 0.03490, Train_Acc:98.67%\n",
      "Accuary on test images:90.45%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [67/250], Step [60/391], Loss: 0.02004, Train_Acc:98.67%\n",
      "Epoch [67/250], Step [120/391], Loss: 0.05315, Train_Acc:98.51%\n",
      "Epoch [67/250], Step [180/391], Loss: 0.02439, Train_Acc:98.52%\n",
      "Epoch [67/250], Step [240/391], Loss: 0.03964, Train_Acc:98.53%\n",
      "Epoch [67/250], Step [300/391], Loss: 0.03786, Train_Acc:98.58%\n",
      "Epoch [67/250], Step [360/391], Loss: 0.03173, Train_Acc:98.59%\n",
      "Accuary on test images:90.90%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [68/250], Step [60/391], Loss: 0.06400, Train_Acc:98.97%\n",
      "Epoch [68/250], Step [120/391], Loss: 0.05537, Train_Acc:98.82%\n",
      "Epoch [68/250], Step [180/391], Loss: 0.03708, Train_Acc:98.82%\n",
      "Epoch [68/250], Step [240/391], Loss: 0.04884, Train_Acc:98.77%\n",
      "Epoch [68/250], Step [300/391], Loss: 0.02049, Train_Acc:98.69%\n",
      "Epoch [68/250], Step [360/391], Loss: 0.06505, Train_Acc:98.67%\n",
      "Accuary on test images:90.74%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [69/250], Step [60/391], Loss: 0.02473, Train_Acc:98.63%\n",
      "Epoch [69/250], Step [120/391], Loss: 0.04621, Train_Acc:98.79%\n",
      "Epoch [69/250], Step [180/391], Loss: 0.04088, Train_Acc:98.70%\n",
      "Epoch [69/250], Step [240/391], Loss: 0.04789, Train_Acc:98.60%\n",
      "Epoch [69/250], Step [300/391], Loss: 0.01591, Train_Acc:98.61%\n",
      "Epoch [69/250], Step [360/391], Loss: 0.04073, Train_Acc:98.60%\n",
      "Accuary on test images:90.42%\n",
      "Best accuracy: 91.01%\n",
      "Epoch [70/250], Step [60/391], Loss: 0.02445, Train_Acc:99.13%\n",
      "Epoch [70/250], Step [120/391], Loss: 0.03287, Train_Acc:99.14%\n",
      "Epoch [70/250], Step [180/391], Loss: 0.02052, Train_Acc:99.14%\n",
      "Epoch [70/250], Step [240/391], Loss: 0.00836, Train_Acc:99.10%\n",
      "Epoch [70/250], Step [300/391], Loss: 0.04734, Train_Acc:99.05%\n",
      "Epoch [70/250], Step [360/391], Loss: 0.04054, Train_Acc:99.05%\n",
      "Accuary on test images:91.44%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [71/250], Step [60/391], Loss: 0.03335, Train_Acc:99.14%\n",
      "Epoch [71/250], Step [120/391], Loss: 0.01876, Train_Acc:99.12%\n",
      "Epoch [71/250], Step [180/391], Loss: 0.00654, Train_Acc:99.09%\n",
      "Epoch [71/250], Step [240/391], Loss: 0.03859, Train_Acc:99.13%\n",
      "Epoch [71/250], Step [300/391], Loss: 0.04143, Train_Acc:99.10%\n",
      "Epoch [71/250], Step [360/391], Loss: 0.03852, Train_Acc:99.06%\n",
      "Accuary on test images:91.34%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [72/250], Step [60/391], Loss: 0.04882, Train_Acc:98.92%\n",
      "Epoch [72/250], Step [120/391], Loss: 0.03365, Train_Acc:99.06%\n",
      "Epoch [72/250], Step [180/391], Loss: 0.01933, Train_Acc:99.10%\n",
      "Epoch [72/250], Step [240/391], Loss: 0.03835, Train_Acc:99.06%\n",
      "Epoch [72/250], Step [300/391], Loss: 0.01164, Train_Acc:99.05%\n",
      "Epoch [72/250], Step [360/391], Loss: 0.05645, Train_Acc:99.01%\n",
      "Accuary on test images:90.15%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [73/250], Step [60/391], Loss: 0.04590, Train_Acc:98.92%\n",
      "Epoch [73/250], Step [120/391], Loss: 0.02790, Train_Acc:98.92%\n",
      "Epoch [73/250], Step [180/391], Loss: 0.08349, Train_Acc:98.88%\n",
      "Epoch [73/250], Step [240/391], Loss: 0.07761, Train_Acc:98.77%\n",
      "Epoch [73/250], Step [300/391], Loss: 0.04630, Train_Acc:98.76%\n",
      "Epoch [73/250], Step [360/391], Loss: 0.03936, Train_Acc:98.76%\n",
      "Accuary on test images:89.68%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [74/250], Step [60/391], Loss: 0.01867, Train_Acc:98.63%\n",
      "Epoch [74/250], Step [120/391], Loss: 0.08978, Train_Acc:98.50%\n",
      "Epoch [74/250], Step [180/391], Loss: 0.02446, Train_Acc:98.36%\n",
      "Epoch [74/250], Step [240/391], Loss: 0.03842, Train_Acc:98.42%\n",
      "Epoch [74/250], Step [300/391], Loss: 0.01403, Train_Acc:98.48%\n",
      "Epoch [74/250], Step [360/391], Loss: 0.06790, Train_Acc:98.47%\n",
      "Accuary on test images:89.85%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [75/250], Step [60/391], Loss: 0.04147, Train_Acc:98.76%\n",
      "Epoch [75/250], Step [120/391], Loss: 0.06040, Train_Acc:98.79%\n",
      "Epoch [75/250], Step [180/391], Loss: 0.02903, Train_Acc:98.82%\n",
      "Epoch [75/250], Step [240/391], Loss: 0.03746, Train_Acc:98.82%\n",
      "Epoch [75/250], Step [300/391], Loss: 0.01536, Train_Acc:98.83%\n",
      "Epoch [75/250], Step [360/391], Loss: 0.06210, Train_Acc:98.78%\n",
      "Accuary on test images:90.78%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [76/250], Step [60/391], Loss: 0.07000, Train_Acc:98.87%\n",
      "Epoch [76/250], Step [120/391], Loss: 0.04162, Train_Acc:98.91%\n",
      "Epoch [76/250], Step [180/391], Loss: 0.02607, Train_Acc:98.89%\n",
      "Epoch [76/250], Step [240/391], Loss: 0.03196, Train_Acc:98.86%\n",
      "Epoch [76/250], Step [300/391], Loss: 0.02245, Train_Acc:98.88%\n",
      "Epoch [76/250], Step [360/391], Loss: 0.05471, Train_Acc:98.85%\n",
      "Accuary on test images:90.46%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [77/250], Step [60/391], Loss: 0.05698, Train_Acc:98.82%\n",
      "Epoch [77/250], Step [120/391], Loss: 0.04464, Train_Acc:98.84%\n",
      "Epoch [77/250], Step [180/391], Loss: 0.05478, Train_Acc:98.96%\n",
      "Epoch [77/250], Step [240/391], Loss: 0.06878, Train_Acc:98.96%\n",
      "Epoch [77/250], Step [300/391], Loss: 0.07845, Train_Acc:98.93%\n",
      "Epoch [77/250], Step [360/391], Loss: 0.02712, Train_Acc:98.86%\n",
      "Accuary on test images:89.89%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [78/250], Step [60/391], Loss: 0.01239, Train_Acc:98.97%\n",
      "Epoch [78/250], Step [120/391], Loss: 0.05990, Train_Acc:99.10%\n",
      "Epoch [78/250], Step [180/391], Loss: 0.07060, Train_Acc:99.11%\n",
      "Epoch [78/250], Step [240/391], Loss: 0.02290, Train_Acc:99.03%\n",
      "Epoch [78/250], Step [300/391], Loss: 0.04200, Train_Acc:98.95%\n",
      "Epoch [78/250], Step [360/391], Loss: 0.05051, Train_Acc:98.94%\n",
      "Accuary on test images:90.85%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [79/250], Step [60/391], Loss: 0.04524, Train_Acc:99.13%\n",
      "Epoch [79/250], Step [120/391], Loss: 0.03513, Train_Acc:99.07%\n",
      "Epoch [79/250], Step [180/391], Loss: 0.01441, Train_Acc:99.09%\n",
      "Epoch [79/250], Step [240/391], Loss: 0.02355, Train_Acc:99.11%\n",
      "Epoch [79/250], Step [300/391], Loss: 0.03794, Train_Acc:99.15%\n",
      "Epoch [79/250], Step [360/391], Loss: 0.00612, Train_Acc:99.14%\n",
      "Accuary on test images:91.11%\n",
      "Best accuracy: 91.44%\n",
      "Epoch [80/250], Step [60/391], Loss: 0.04859, Train_Acc:99.17%\n",
      "Epoch [80/250], Step [120/391], Loss: 0.04437, Train_Acc:98.96%\n",
      "Epoch [80/250], Step [180/391], Loss: 0.01537, Train_Acc:99.00%\n",
      "Epoch [80/250], Step [240/391], Loss: 0.01382, Train_Acc:99.02%\n",
      "Epoch [80/250], Step [300/391], Loss: 0.01295, Train_Acc:99.04%\n",
      "Epoch [80/250], Step [360/391], Loss: 0.05015, Train_Acc:99.03%\n",
      "Accuary on test images:91.52%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [81/250], Step [60/391], Loss: 0.03593, Train_Acc:99.08%\n",
      "Epoch [81/250], Step [120/391], Loss: 0.07324, Train_Acc:99.09%\n",
      "Epoch [81/250], Step [180/391], Loss: 0.01179, Train_Acc:99.01%\n",
      "Epoch [81/250], Step [240/391], Loss: 0.01344, Train_Acc:99.04%\n",
      "Epoch [81/250], Step [300/391], Loss: 0.00990, Train_Acc:99.10%\n",
      "Epoch [81/250], Step [360/391], Loss: 0.01821, Train_Acc:99.06%\n",
      "Accuary on test images:90.00%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [82/250], Step [60/391], Loss: 0.01569, Train_Acc:99.02%\n",
      "Epoch [82/250], Step [120/391], Loss: 0.01321, Train_Acc:99.02%\n",
      "Epoch [82/250], Step [180/391], Loss: 0.01410, Train_Acc:99.05%\n",
      "Epoch [82/250], Step [240/391], Loss: 0.02795, Train_Acc:98.99%\n",
      "Epoch [82/250], Step [300/391], Loss: 0.00677, Train_Acc:99.00%\n",
      "Epoch [82/250], Step [360/391], Loss: 0.04670, Train_Acc:99.00%\n",
      "Accuary on test images:91.03%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [83/250], Step [60/391], Loss: 0.02710, Train_Acc:99.09%\n",
      "Epoch [83/250], Step [120/391], Loss: 0.02774, Train_Acc:99.24%\n",
      "Epoch [83/250], Step [180/391], Loss: 0.04471, Train_Acc:99.23%\n",
      "Epoch [83/250], Step [240/391], Loss: 0.00890, Train_Acc:99.21%\n",
      "Epoch [83/250], Step [300/391], Loss: 0.13077, Train_Acc:99.17%\n",
      "Epoch [83/250], Step [360/391], Loss: 0.05133, Train_Acc:99.13%\n",
      "Accuary on test images:90.54%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [84/250], Step [60/391], Loss: 0.02529, Train_Acc:98.98%\n",
      "Epoch [84/250], Step [120/391], Loss: 0.01685, Train_Acc:98.98%\n",
      "Epoch [84/250], Step [180/391], Loss: 0.01101, Train_Acc:99.08%\n",
      "Epoch [84/250], Step [240/391], Loss: 0.01997, Train_Acc:99.11%\n",
      "Epoch [84/250], Step [300/391], Loss: 0.02539, Train_Acc:99.13%\n",
      "Epoch [84/250], Step [360/391], Loss: 0.03245, Train_Acc:99.10%\n",
      "Accuary on test images:90.98%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [85/250], Step [60/391], Loss: 0.02262, Train_Acc:99.15%\n",
      "Epoch [85/250], Step [120/391], Loss: 0.03582, Train_Acc:99.08%\n",
      "Epoch [85/250], Step [180/391], Loss: 0.02207, Train_Acc:99.08%\n",
      "Epoch [85/250], Step [240/391], Loss: 0.04007, Train_Acc:98.98%\n",
      "Epoch [85/250], Step [300/391], Loss: 0.04116, Train_Acc:99.01%\n",
      "Epoch [85/250], Step [360/391], Loss: 0.01887, Train_Acc:99.02%\n",
      "Accuary on test images:90.64%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [86/250], Step [60/391], Loss: 0.02601, Train_Acc:98.80%\n",
      "Epoch [86/250], Step [120/391], Loss: 0.06646, Train_Acc:98.76%\n",
      "Epoch [86/250], Step [180/391], Loss: 0.03772, Train_Acc:98.79%\n",
      "Epoch [86/250], Step [240/391], Loss: 0.07989, Train_Acc:98.81%\n",
      "Epoch [86/250], Step [300/391], Loss: 0.02549, Train_Acc:98.87%\n",
      "Epoch [86/250], Step [360/391], Loss: 0.01483, Train_Acc:98.91%\n",
      "Accuary on test images:91.18%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [87/250], Step [60/391], Loss: 0.01895, Train_Acc:98.98%\n",
      "Epoch [87/250], Step [120/391], Loss: 0.01328, Train_Acc:99.04%\n",
      "Epoch [87/250], Step [180/391], Loss: 0.06846, Train_Acc:99.07%\n",
      "Epoch [87/250], Step [240/391], Loss: 0.02695, Train_Acc:99.05%\n",
      "Epoch [87/250], Step [300/391], Loss: 0.04753, Train_Acc:99.07%\n",
      "Epoch [87/250], Step [360/391], Loss: 0.02015, Train_Acc:99.09%\n",
      "Accuary on test images:90.88%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [88/250], Step [60/391], Loss: 0.02498, Train_Acc:99.40%\n",
      "Epoch [88/250], Step [120/391], Loss: 0.00971, Train_Acc:99.28%\n",
      "Epoch [88/250], Step [180/391], Loss: 0.00623, Train_Acc:99.22%\n",
      "Epoch [88/250], Step [240/391], Loss: 0.02252, Train_Acc:99.16%\n",
      "Epoch [88/250], Step [300/391], Loss: 0.07041, Train_Acc:99.13%\n",
      "Epoch [88/250], Step [360/391], Loss: 0.01487, Train_Acc:99.12%\n",
      "Accuary on test images:91.35%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [89/250], Step [60/391], Loss: 0.00744, Train_Acc:99.36%\n",
      "Epoch [89/250], Step [120/391], Loss: 0.01741, Train_Acc:99.42%\n",
      "Epoch [89/250], Step [180/391], Loss: 0.05375, Train_Acc:99.41%\n",
      "Epoch [89/250], Step [240/391], Loss: 0.02702, Train_Acc:99.38%\n",
      "Epoch [89/250], Step [300/391], Loss: 0.02311, Train_Acc:99.34%\n",
      "Epoch [89/250], Step [360/391], Loss: 0.02840, Train_Acc:99.27%\n",
      "Accuary on test images:90.72%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [90/250], Step [60/391], Loss: 0.03335, Train_Acc:99.02%\n",
      "Epoch [90/250], Step [120/391], Loss: 0.01557, Train_Acc:98.98%\n",
      "Epoch [90/250], Step [180/391], Loss: 0.01857, Train_Acc:99.01%\n",
      "Epoch [90/250], Step [240/391], Loss: 0.02065, Train_Acc:98.98%\n",
      "Epoch [90/250], Step [300/391], Loss: 0.03470, Train_Acc:98.96%\n",
      "Epoch [90/250], Step [360/391], Loss: 0.01630, Train_Acc:98.96%\n",
      "Accuary on test images:89.80%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [91/250], Step [60/391], Loss: 0.01246, Train_Acc:99.01%\n",
      "Epoch [91/250], Step [120/391], Loss: 0.03610, Train_Acc:98.92%\n",
      "Epoch [91/250], Step [180/391], Loss: 0.04435, Train_Acc:98.94%\n",
      "Epoch [91/250], Step [240/391], Loss: 0.07709, Train_Acc:98.94%\n",
      "Epoch [91/250], Step [300/391], Loss: 0.03747, Train_Acc:98.92%\n",
      "Epoch [91/250], Step [360/391], Loss: 0.03262, Train_Acc:98.92%\n",
      "Accuary on test images:90.17%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [92/250], Step [60/391], Loss: 0.02723, Train_Acc:99.19%\n",
      "Epoch [92/250], Step [120/391], Loss: 0.01274, Train_Acc:99.24%\n",
      "Epoch [92/250], Step [180/391], Loss: 0.02014, Train_Acc:99.19%\n",
      "Epoch [92/250], Step [240/391], Loss: 0.03468, Train_Acc:99.16%\n",
      "Epoch [92/250], Step [300/391], Loss: 0.01652, Train_Acc:99.20%\n",
      "Epoch [92/250], Step [360/391], Loss: 0.05560, Train_Acc:99.18%\n",
      "Accuary on test images:91.08%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [93/250], Step [60/391], Loss: 0.02171, Train_Acc:98.95%\n",
      "Epoch [93/250], Step [120/391], Loss: 0.00956, Train_Acc:99.15%\n",
      "Epoch [93/250], Step [180/391], Loss: 0.01784, Train_Acc:99.14%\n",
      "Epoch [93/250], Step [240/391], Loss: 0.02102, Train_Acc:99.20%\n",
      "Epoch [93/250], Step [300/391], Loss: 0.01246, Train_Acc:99.23%\n",
      "Epoch [93/250], Step [360/391], Loss: 0.00822, Train_Acc:99.22%\n",
      "Accuary on test images:90.58%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [94/250], Step [60/391], Loss: 0.01843, Train_Acc:99.19%\n",
      "Epoch [94/250], Step [120/391], Loss: 0.03262, Train_Acc:99.16%\n",
      "Epoch [94/250], Step [180/391], Loss: 0.01799, Train_Acc:99.23%\n",
      "Epoch [94/250], Step [240/391], Loss: 0.00781, Train_Acc:99.26%\n",
      "Epoch [94/250], Step [300/391], Loss: 0.03371, Train_Acc:99.25%\n",
      "Epoch [94/250], Step [360/391], Loss: 0.02192, Train_Acc:99.23%\n",
      "Accuary on test images:90.86%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [95/250], Step [60/391], Loss: 0.07063, Train_Acc:99.35%\n",
      "Epoch [95/250], Step [120/391], Loss: 0.03927, Train_Acc:99.24%\n",
      "Epoch [95/250], Step [180/391], Loss: 0.03056, Train_Acc:99.11%\n",
      "Epoch [95/250], Step [240/391], Loss: 0.03729, Train_Acc:99.03%\n",
      "Epoch [95/250], Step [300/391], Loss: 0.02678, Train_Acc:99.05%\n",
      "Epoch [95/250], Step [360/391], Loss: 0.04993, Train_Acc:99.04%\n",
      "Accuary on test images:91.23%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [96/250], Step [60/391], Loss: 0.02174, Train_Acc:98.87%\n",
      "Epoch [96/250], Step [120/391], Loss: 0.01240, Train_Acc:99.09%\n",
      "Epoch [96/250], Step [180/391], Loss: 0.02468, Train_Acc:99.04%\n",
      "Epoch [96/250], Step [240/391], Loss: 0.02715, Train_Acc:99.01%\n",
      "Epoch [96/250], Step [300/391], Loss: 0.04275, Train_Acc:99.03%\n",
      "Epoch [96/250], Step [360/391], Loss: 0.02590, Train_Acc:99.02%\n",
      "Accuary on test images:88.89%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [97/250], Step [60/391], Loss: 0.04571, Train_Acc:98.74%\n",
      "Epoch [97/250], Step [120/391], Loss: 0.05258, Train_Acc:98.84%\n",
      "Epoch [97/250], Step [180/391], Loss: 0.06022, Train_Acc:98.93%\n",
      "Epoch [97/250], Step [240/391], Loss: 0.03117, Train_Acc:98.92%\n",
      "Epoch [97/250], Step [300/391], Loss: 0.03271, Train_Acc:98.98%\n",
      "Epoch [97/250], Step [360/391], Loss: 0.03935, Train_Acc:98.99%\n",
      "Accuary on test images:91.22%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [98/250], Step [60/391], Loss: 0.00700, Train_Acc:99.35%\n",
      "Epoch [98/250], Step [120/391], Loss: 0.00879, Train_Acc:99.38%\n",
      "Epoch [98/250], Step [180/391], Loss: 0.03991, Train_Acc:99.35%\n",
      "Epoch [98/250], Step [240/391], Loss: 0.01362, Train_Acc:99.28%\n",
      "Epoch [98/250], Step [300/391], Loss: 0.01203, Train_Acc:99.27%\n",
      "Epoch [98/250], Step [360/391], Loss: 0.02096, Train_Acc:99.26%\n",
      "Accuary on test images:90.54%\n",
      "Best accuracy: 91.52%\n",
      "Epoch [99/250], Step [60/391], Loss: 0.00544, Train_Acc:99.13%\n",
      "Epoch [99/250], Step [120/391], Loss: 0.03085, Train_Acc:99.14%\n",
      "Epoch [99/250], Step [180/391], Loss: 0.02246, Train_Acc:99.19%\n",
      "Epoch [99/250], Step [240/391], Loss: 0.01152, Train_Acc:99.11%\n",
      "Epoch [99/250], Step [300/391], Loss: 0.05586, Train_Acc:99.07%\n",
      "Epoch [99/250], Step [360/391], Loss: 0.02616, Train_Acc:99.07%\n",
      "Accuary on test images:91.61%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [100/250], Step [60/391], Loss: 0.03216, Train_Acc:99.21%\n",
      "Epoch [100/250], Step [120/391], Loss: 0.02449, Train_Acc:99.17%\n",
      "Epoch [100/250], Step [180/391], Loss: 0.01651, Train_Acc:99.13%\n",
      "Epoch [100/250], Step [240/391], Loss: 0.02948, Train_Acc:99.12%\n",
      "Epoch [100/250], Step [300/391], Loss: 0.01868, Train_Acc:99.14%\n",
      "Epoch [100/250], Step [360/391], Loss: 0.00697, Train_Acc:99.14%\n",
      "Accuary on test images:90.92%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [101/250], Step [60/391], Loss: 0.01873, Train_Acc:99.27%\n",
      "Epoch [101/250], Step [120/391], Loss: 0.00602, Train_Acc:99.36%\n",
      "Epoch [101/250], Step [180/391], Loss: 0.01522, Train_Acc:99.40%\n",
      "Epoch [101/250], Step [240/391], Loss: 0.02356, Train_Acc:99.39%\n",
      "Epoch [101/250], Step [300/391], Loss: 0.02002, Train_Acc:99.40%\n",
      "Epoch [101/250], Step [360/391], Loss: 0.01594, Train_Acc:99.40%\n",
      "Accuary on test images:91.58%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [102/250], Step [60/391], Loss: 0.06742, Train_Acc:99.04%\n",
      "Epoch [102/250], Step [120/391], Loss: 0.03578, Train_Acc:99.06%\n",
      "Epoch [102/250], Step [180/391], Loss: 0.02924, Train_Acc:99.07%\n",
      "Epoch [102/250], Step [240/391], Loss: 0.04833, Train_Acc:99.07%\n",
      "Epoch [102/250], Step [300/391], Loss: 0.02288, Train_Acc:99.06%\n",
      "Epoch [102/250], Step [360/391], Loss: 0.02461, Train_Acc:99.03%\n",
      "Accuary on test images:90.54%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [103/250], Step [60/391], Loss: 0.04239, Train_Acc:99.22%\n",
      "Epoch [103/250], Step [120/391], Loss: 0.01743, Train_Acc:99.29%\n",
      "Epoch [103/250], Step [180/391], Loss: 0.04890, Train_Acc:99.25%\n",
      "Epoch [103/250], Step [240/391], Loss: 0.01363, Train_Acc:99.21%\n",
      "Epoch [103/250], Step [300/391], Loss: 0.03570, Train_Acc:99.21%\n",
      "Epoch [103/250], Step [360/391], Loss: 0.05931, Train_Acc:99.19%\n",
      "Accuary on test images:91.36%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [104/250], Step [60/391], Loss: 0.04807, Train_Acc:99.23%\n",
      "Epoch [104/250], Step [120/391], Loss: 0.03332, Train_Acc:99.13%\n",
      "Epoch [104/250], Step [180/391], Loss: 0.04337, Train_Acc:99.11%\n",
      "Epoch [104/250], Step [240/391], Loss: 0.01689, Train_Acc:99.15%\n",
      "Epoch [104/250], Step [300/391], Loss: 0.02741, Train_Acc:99.17%\n",
      "Epoch [104/250], Step [360/391], Loss: 0.01688, Train_Acc:99.15%\n",
      "Accuary on test images:91.45%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [105/250], Step [60/391], Loss: 0.02970, Train_Acc:99.14%\n",
      "Epoch [105/250], Step [120/391], Loss: 0.02126, Train_Acc:99.19%\n",
      "Epoch [105/250], Step [180/391], Loss: 0.04503, Train_Acc:99.14%\n",
      "Epoch [105/250], Step [240/391], Loss: 0.04966, Train_Acc:99.12%\n",
      "Epoch [105/250], Step [300/391], Loss: 0.02253, Train_Acc:99.10%\n",
      "Epoch [105/250], Step [360/391], Loss: 0.00881, Train_Acc:99.06%\n",
      "Accuary on test images:90.64%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [106/250], Step [60/391], Loss: 0.05548, Train_Acc:99.10%\n",
      "Epoch [106/250], Step [120/391], Loss: 0.01568, Train_Acc:99.08%\n",
      "Epoch [106/250], Step [180/391], Loss: 0.03204, Train_Acc:99.10%\n",
      "Epoch [106/250], Step [240/391], Loss: 0.04065, Train_Acc:99.05%\n",
      "Epoch [106/250], Step [300/391], Loss: 0.01460, Train_Acc:99.06%\n",
      "Epoch [106/250], Step [360/391], Loss: 0.05186, Train_Acc:99.03%\n",
      "Accuary on test images:91.44%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [107/250], Step [60/391], Loss: 0.04664, Train_Acc:98.91%\n",
      "Epoch [107/250], Step [120/391], Loss: 0.02340, Train_Acc:98.96%\n",
      "Epoch [107/250], Step [180/391], Loss: 0.04576, Train_Acc:99.03%\n",
      "Epoch [107/250], Step [240/391], Loss: 0.01831, Train_Acc:99.08%\n",
      "Epoch [107/250], Step [300/391], Loss: 0.01581, Train_Acc:99.10%\n",
      "Epoch [107/250], Step [360/391], Loss: 0.03121, Train_Acc:99.11%\n",
      "Accuary on test images:91.20%\n",
      "Best accuracy: 91.61%\n",
      "Epoch [108/250], Step [60/391], Loss: 0.02221, Train_Acc:99.23%\n",
      "Epoch [108/250], Step [120/391], Loss: 0.01865, Train_Acc:99.21%\n",
      "Epoch [108/250], Step [180/391], Loss: 0.04660, Train_Acc:99.19%\n",
      "Epoch [108/250], Step [240/391], Loss: 0.02218, Train_Acc:99.20%\n",
      "Epoch [108/250], Step [300/391], Loss: 0.03905, Train_Acc:99.19%\n",
      "Epoch [108/250], Step [360/391], Loss: 0.01121, Train_Acc:99.14%\n",
      "Accuary on test images:91.76%\n",
      "Best accuracy: 91.76%\n",
      "Epoch [109/250], Step [60/391], Loss: 0.03228, Train_Acc:99.27%\n",
      "Epoch [109/250], Step [120/391], Loss: 0.01332, Train_Acc:99.32%\n",
      "Epoch [109/250], Step [180/391], Loss: 0.03090, Train_Acc:99.12%\n",
      "Epoch [109/250], Step [240/391], Loss: 0.03044, Train_Acc:99.10%\n",
      "Epoch [109/250], Step [300/391], Loss: 0.06139, Train_Acc:99.16%\n",
      "Epoch [109/250], Step [360/391], Loss: 0.00751, Train_Acc:99.19%\n",
      "Accuary on test images:90.74%\n",
      "Best accuracy: 91.76%\n",
      "Epoch [110/250], Step [60/391], Loss: 0.01238, Train_Acc:98.91%\n",
      "Epoch [110/250], Step [120/391], Loss: 0.01853, Train_Acc:98.90%\n",
      "Epoch [110/250], Step [180/391], Loss: 0.01164, Train_Acc:98.95%\n",
      "Epoch [110/250], Step [240/391], Loss: 0.03187, Train_Acc:99.00%\n",
      "Epoch [110/250], Step [300/391], Loss: 0.06232, Train_Acc:98.98%\n",
      "Epoch [110/250], Step [360/391], Loss: 0.04410, Train_Acc:98.97%\n",
      "Accuary on test images:90.30%\n",
      "Best accuracy: 91.76%\n",
      "Epoch [111/250], Step [60/391], Loss: 0.01535, Train_Acc:99.27%\n",
      "Epoch [111/250], Step [120/391], Loss: 0.00833, Train_Acc:99.33%\n",
      "Epoch [111/250], Step [180/391], Loss: 0.01881, Train_Acc:99.35%\n",
      "Epoch [111/250], Step [240/391], Loss: 0.02862, Train_Acc:99.38%\n",
      "Epoch [111/250], Step [300/391], Loss: 0.02944, Train_Acc:99.35%\n",
      "Epoch [111/250], Step [360/391], Loss: 0.03637, Train_Acc:99.36%\n",
      "Accuary on test images:91.95%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [112/250], Step [60/391], Loss: 0.00507, Train_Acc:99.49%\n",
      "Epoch [112/250], Step [120/391], Loss: 0.01158, Train_Acc:99.46%\n",
      "Epoch [112/250], Step [180/391], Loss: 0.02079, Train_Acc:99.42%\n",
      "Epoch [112/250], Step [240/391], Loss: 0.01465, Train_Acc:99.39%\n",
      "Epoch [112/250], Step [300/391], Loss: 0.06879, Train_Acc:99.36%\n",
      "Epoch [112/250], Step [360/391], Loss: 0.05434, Train_Acc:99.30%\n",
      "Accuary on test images:91.17%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [113/250], Step [60/391], Loss: 0.05459, Train_Acc:99.17%\n",
      "Epoch [113/250], Step [120/391], Loss: 0.01168, Train_Acc:99.25%\n",
      "Epoch [113/250], Step [180/391], Loss: 0.05997, Train_Acc:99.19%\n",
      "Epoch [113/250], Step [240/391], Loss: 0.03952, Train_Acc:99.18%\n",
      "Epoch [113/250], Step [300/391], Loss: 0.00854, Train_Acc:99.12%\n",
      "Epoch [113/250], Step [360/391], Loss: 0.03932, Train_Acc:99.12%\n",
      "Accuary on test images:91.07%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [114/250], Step [60/391], Loss: 0.01937, Train_Acc:99.13%\n",
      "Epoch [114/250], Step [120/391], Loss: 0.03169, Train_Acc:99.12%\n",
      "Epoch [114/250], Step [180/391], Loss: 0.02158, Train_Acc:99.08%\n",
      "Epoch [114/250], Step [240/391], Loss: 0.03648, Train_Acc:99.15%\n",
      "Epoch [114/250], Step [300/391], Loss: 0.02095, Train_Acc:99.17%\n",
      "Epoch [114/250], Step [360/391], Loss: 0.03075, Train_Acc:99.19%\n",
      "Accuary on test images:91.78%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [115/250], Step [60/391], Loss: 0.00856, Train_Acc:99.49%\n",
      "Epoch [115/250], Step [120/391], Loss: 0.01303, Train_Acc:99.47%\n",
      "Epoch [115/250], Step [180/391], Loss: 0.00334, Train_Acc:99.51%\n",
      "Epoch [115/250], Step [240/391], Loss: 0.00804, Train_Acc:99.49%\n",
      "Epoch [115/250], Step [300/391], Loss: 0.02817, Train_Acc:99.43%\n",
      "Epoch [115/250], Step [360/391], Loss: 0.02540, Train_Acc:99.41%\n",
      "Accuary on test images:91.53%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [116/250], Step [60/391], Loss: 0.01579, Train_Acc:99.48%\n",
      "Epoch [116/250], Step [120/391], Loss: 0.01836, Train_Acc:99.32%\n",
      "Epoch [116/250], Step [180/391], Loss: 0.02087, Train_Acc:99.33%\n",
      "Epoch [116/250], Step [240/391], Loss: 0.03891, Train_Acc:99.27%\n",
      "Epoch [116/250], Step [300/391], Loss: 0.01222, Train_Acc:99.23%\n",
      "Epoch [116/250], Step [360/391], Loss: 0.02062, Train_Acc:99.23%\n",
      "Accuary on test images:91.68%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [117/250], Step [60/391], Loss: 0.01098, Train_Acc:99.36%\n",
      "Epoch [117/250], Step [120/391], Loss: 0.00818, Train_Acc:99.32%\n",
      "Epoch [117/250], Step [180/391], Loss: 0.01329, Train_Acc:99.26%\n",
      "Epoch [117/250], Step [240/391], Loss: 0.02657, Train_Acc:99.19%\n",
      "Epoch [117/250], Step [300/391], Loss: 0.00986, Train_Acc:99.16%\n",
      "Epoch [117/250], Step [360/391], Loss: 0.02766, Train_Acc:99.18%\n",
      "Accuary on test images:89.89%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [118/250], Step [60/391], Loss: 0.01020, Train_Acc:99.43%\n",
      "Epoch [118/250], Step [120/391], Loss: 0.02612, Train_Acc:99.29%\n",
      "Epoch [118/250], Step [180/391], Loss: 0.01307, Train_Acc:99.35%\n",
      "Epoch [118/250], Step [240/391], Loss: 0.00636, Train_Acc:99.34%\n",
      "Epoch [118/250], Step [300/391], Loss: 0.04825, Train_Acc:99.28%\n",
      "Epoch [118/250], Step [360/391], Loss: 0.00955, Train_Acc:99.29%\n",
      "Accuary on test images:90.28%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [119/250], Step [60/391], Loss: 0.00723, Train_Acc:99.28%\n",
      "Epoch [119/250], Step [120/391], Loss: 0.00412, Train_Acc:99.34%\n",
      "Epoch [119/250], Step [180/391], Loss: 0.00765, Train_Acc:99.22%\n",
      "Epoch [119/250], Step [240/391], Loss: 0.03000, Train_Acc:99.25%\n",
      "Epoch [119/250], Step [300/391], Loss: 0.03551, Train_Acc:99.22%\n",
      "Epoch [119/250], Step [360/391], Loss: 0.02735, Train_Acc:99.21%\n",
      "Accuary on test images:91.11%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [120/250], Step [60/391], Loss: 0.09580, Train_Acc:98.95%\n",
      "Epoch [120/250], Step [120/391], Loss: 0.00964, Train_Acc:99.14%\n",
      "Epoch [120/250], Step [180/391], Loss: 0.00608, Train_Acc:99.22%\n",
      "Epoch [120/250], Step [240/391], Loss: 0.00988, Train_Acc:99.24%\n",
      "Epoch [120/250], Step [300/391], Loss: 0.01988, Train_Acc:99.30%\n",
      "Epoch [120/250], Step [360/391], Loss: 0.04145, Train_Acc:99.32%\n",
      "Accuary on test images:91.23%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [121/250], Step [60/391], Loss: 0.00624, Train_Acc:99.24%\n",
      "Epoch [121/250], Step [120/391], Loss: 0.01477, Train_Acc:99.20%\n",
      "Epoch [121/250], Step [180/391], Loss: 0.01096, Train_Acc:99.28%\n",
      "Epoch [121/250], Step [240/391], Loss: 0.00650, Train_Acc:99.35%\n",
      "Epoch [121/250], Step [300/391], Loss: 0.04923, Train_Acc:99.30%\n",
      "Epoch [121/250], Step [360/391], Loss: 0.01067, Train_Acc:99.31%\n",
      "Accuary on test images:91.13%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [122/250], Step [60/391], Loss: 0.03393, Train_Acc:99.05%\n",
      "Epoch [122/250], Step [120/391], Loss: 0.00927, Train_Acc:99.13%\n",
      "Epoch [122/250], Step [180/391], Loss: 0.01294, Train_Acc:99.20%\n",
      "Epoch [122/250], Step [240/391], Loss: 0.05007, Train_Acc:99.22%\n",
      "Epoch [122/250], Step [300/391], Loss: 0.01122, Train_Acc:99.23%\n",
      "Epoch [122/250], Step [360/391], Loss: 0.05138, Train_Acc:99.18%\n",
      "Accuary on test images:91.01%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [123/250], Step [60/391], Loss: 0.01608, Train_Acc:98.91%\n",
      "Epoch [123/250], Step [120/391], Loss: 0.08094, Train_Acc:98.82%\n",
      "Epoch [123/250], Step [180/391], Loss: 0.05487, Train_Acc:98.81%\n",
      "Epoch [123/250], Step [240/391], Loss: 0.03470, Train_Acc:98.88%\n",
      "Epoch [123/250], Step [300/391], Loss: 0.02891, Train_Acc:98.86%\n",
      "Epoch [123/250], Step [360/391], Loss: 0.01413, Train_Acc:98.85%\n",
      "Accuary on test images:91.23%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [124/250], Step [60/391], Loss: 0.04355, Train_Acc:98.97%\n",
      "Epoch [124/250], Step [120/391], Loss: 0.00693, Train_Acc:99.09%\n",
      "Epoch [124/250], Step [180/391], Loss: 0.02568, Train_Acc:99.08%\n",
      "Epoch [124/250], Step [240/391], Loss: 0.01863, Train_Acc:99.02%\n",
      "Epoch [124/250], Step [300/391], Loss: 0.01251, Train_Acc:99.03%\n",
      "Epoch [124/250], Step [360/391], Loss: 0.03337, Train_Acc:99.04%\n",
      "Accuary on test images:90.70%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [125/250], Step [60/391], Loss: 0.01373, Train_Acc:99.06%\n",
      "Epoch [125/250], Step [120/391], Loss: 0.00633, Train_Acc:99.11%\n",
      "Epoch [125/250], Step [180/391], Loss: 0.02170, Train_Acc:99.12%\n",
      "Epoch [125/250], Step [240/391], Loss: 0.04276, Train_Acc:99.12%\n",
      "Epoch [125/250], Step [300/391], Loss: 0.01334, Train_Acc:99.09%\n",
      "Epoch [125/250], Step [360/391], Loss: 0.02124, Train_Acc:99.10%\n",
      "Accuary on test images:91.02%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [126/250], Step [60/391], Loss: 0.05079, Train_Acc:99.31%\n",
      "Epoch [126/250], Step [120/391], Loss: 0.01352, Train_Acc:99.38%\n",
      "Epoch [126/250], Step [180/391], Loss: 0.00997, Train_Acc:99.32%\n",
      "Epoch [126/250], Step [240/391], Loss: 0.00788, Train_Acc:99.33%\n",
      "Epoch [126/250], Step [300/391], Loss: 0.02502, Train_Acc:99.36%\n",
      "Epoch [126/250], Step [360/391], Loss: 0.02971, Train_Acc:99.38%\n",
      "Accuary on test images:91.15%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [127/250], Step [60/391], Loss: 0.00568, Train_Acc:99.34%\n",
      "Epoch [127/250], Step [120/391], Loss: 0.02535, Train_Acc:99.36%\n",
      "Epoch [127/250], Step [180/391], Loss: 0.03012, Train_Acc:99.31%\n",
      "Epoch [127/250], Step [240/391], Loss: 0.01314, Train_Acc:99.27%\n",
      "Epoch [127/250], Step [300/391], Loss: 0.01919, Train_Acc:99.31%\n",
      "Epoch [127/250], Step [360/391], Loss: 0.00994, Train_Acc:99.29%\n",
      "Accuary on test images:91.04%\n",
      "Best accuracy: 91.95%\n",
      "Epoch [128/250], Step [60/391], Loss: 0.00965, Train_Acc:99.00%\n",
      "Epoch [128/250], Step [120/391], Loss: 0.02249, Train_Acc:99.13%\n",
      "Epoch [128/250], Step [180/391], Loss: 0.01987, Train_Acc:99.24%\n",
      "Epoch [128/250], Step [240/391], Loss: 0.01608, Train_Acc:99.27%\n",
      "Epoch [128/250], Step [300/391], Loss: 0.01971, Train_Acc:99.24%\n",
      "Epoch [128/250], Step [360/391], Loss: 0.01436, Train_Acc:99.24%\n",
      "Accuary on test images:92.19%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [129/250], Step [60/391], Loss: 0.00646, Train_Acc:99.28%\n",
      "Epoch [129/250], Step [120/391], Loss: 0.01043, Train_Acc:99.31%\n",
      "Epoch [129/250], Step [180/391], Loss: 0.01239, Train_Acc:99.34%\n",
      "Epoch [129/250], Step [240/391], Loss: 0.00522, Train_Acc:99.39%\n",
      "Epoch [129/250], Step [300/391], Loss: 0.01071, Train_Acc:99.34%\n",
      "Epoch [129/250], Step [360/391], Loss: 0.01246, Train_Acc:99.32%\n",
      "Accuary on test images:91.79%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [130/250], Step [60/391], Loss: 0.03074, Train_Acc:99.41%\n",
      "Epoch [130/250], Step [120/391], Loss: 0.02634, Train_Acc:99.39%\n",
      "Epoch [130/250], Step [180/391], Loss: 0.05710, Train_Acc:99.38%\n",
      "Epoch [130/250], Step [240/391], Loss: 0.00785, Train_Acc:99.36%\n",
      "Epoch [130/250], Step [300/391], Loss: 0.01519, Train_Acc:99.34%\n",
      "Epoch [130/250], Step [360/391], Loss: 0.04990, Train_Acc:99.29%\n",
      "Accuary on test images:90.56%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [131/250], Step [60/391], Loss: 0.00851, Train_Acc:99.05%\n",
      "Epoch [131/250], Step [120/391], Loss: 0.01149, Train_Acc:99.13%\n",
      "Epoch [131/250], Step [180/391], Loss: 0.02825, Train_Acc:99.09%\n",
      "Epoch [131/250], Step [240/391], Loss: 0.01471, Train_Acc:99.11%\n",
      "Epoch [131/250], Step [300/391], Loss: 0.01611, Train_Acc:99.07%\n",
      "Epoch [131/250], Step [360/391], Loss: 0.03027, Train_Acc:99.07%\n",
      "Accuary on test images:91.39%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [132/250], Step [60/391], Loss: 0.00657, Train_Acc:99.09%\n",
      "Epoch [132/250], Step [120/391], Loss: 0.00542, Train_Acc:99.21%\n",
      "Epoch [132/250], Step [180/391], Loss: 0.01211, Train_Acc:99.23%\n",
      "Epoch [132/250], Step [240/391], Loss: 0.01277, Train_Acc:99.25%\n",
      "Epoch [132/250], Step [300/391], Loss: 0.04404, Train_Acc:99.30%\n",
      "Epoch [132/250], Step [360/391], Loss: 0.01457, Train_Acc:99.28%\n",
      "Accuary on test images:91.21%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [133/250], Step [60/391], Loss: 0.01244, Train_Acc:99.47%\n",
      "Epoch [133/250], Step [120/391], Loss: 0.02437, Train_Acc:99.34%\n",
      "Epoch [133/250], Step [180/391], Loss: 0.02533, Train_Acc:99.25%\n",
      "Epoch [133/250], Step [240/391], Loss: 0.03004, Train_Acc:99.24%\n",
      "Epoch [133/250], Step [300/391], Loss: 0.04657, Train_Acc:99.25%\n",
      "Epoch [133/250], Step [360/391], Loss: 0.05375, Train_Acc:99.20%\n",
      "Accuary on test images:91.01%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [134/250], Step [60/391], Loss: 0.04108, Train_Acc:99.06%\n",
      "Epoch [134/250], Step [120/391], Loss: 0.01744, Train_Acc:99.08%\n",
      "Epoch [134/250], Step [180/391], Loss: 0.01814, Train_Acc:99.18%\n",
      "Epoch [134/250], Step [240/391], Loss: 0.01487, Train_Acc:99.22%\n",
      "Epoch [134/250], Step [300/391], Loss: 0.01041, Train_Acc:99.23%\n",
      "Epoch [134/250], Step [360/391], Loss: 0.01730, Train_Acc:99.25%\n",
      "Accuary on test images:91.32%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [135/250], Step [60/391], Loss: 0.00687, Train_Acc:99.02%\n",
      "Epoch [135/250], Step [120/391], Loss: 0.02532, Train_Acc:98.98%\n",
      "Epoch [135/250], Step [180/391], Loss: 0.02150, Train_Acc:99.00%\n",
      "Epoch [135/250], Step [240/391], Loss: 0.06945, Train_Acc:98.95%\n",
      "Epoch [135/250], Step [300/391], Loss: 0.04475, Train_Acc:98.97%\n",
      "Epoch [135/250], Step [360/391], Loss: 0.02927, Train_Acc:98.96%\n",
      "Accuary on test images:90.94%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [136/250], Step [60/391], Loss: 0.03165, Train_Acc:98.66%\n",
      "Epoch [136/250], Step [120/391], Loss: 0.01629, Train_Acc:98.89%\n",
      "Epoch [136/250], Step [180/391], Loss: 0.02132, Train_Acc:98.95%\n",
      "Epoch [136/250], Step [240/391], Loss: 0.03531, Train_Acc:99.01%\n",
      "Epoch [136/250], Step [300/391], Loss: 0.02219, Train_Acc:99.08%\n",
      "Epoch [136/250], Step [360/391], Loss: 0.02678, Train_Acc:99.08%\n",
      "Accuary on test images:91.81%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [137/250], Step [60/391], Loss: 0.02390, Train_Acc:99.47%\n",
      "Epoch [137/250], Step [120/391], Loss: 0.01754, Train_Acc:99.38%\n",
      "Epoch [137/250], Step [180/391], Loss: 0.03826, Train_Acc:99.38%\n",
      "Epoch [137/250], Step [240/391], Loss: 0.04402, Train_Acc:99.36%\n",
      "Epoch [137/250], Step [300/391], Loss: 0.02349, Train_Acc:99.34%\n",
      "Epoch [137/250], Step [360/391], Loss: 0.01766, Train_Acc:99.30%\n",
      "Accuary on test images:91.61%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [138/250], Step [60/391], Loss: 0.01113, Train_Acc:99.21%\n",
      "Epoch [138/250], Step [120/391], Loss: 0.02365, Train_Acc:99.27%\n",
      "Epoch [138/250], Step [180/391], Loss: 0.05461, Train_Acc:99.16%\n",
      "Epoch [138/250], Step [240/391], Loss: 0.00766, Train_Acc:99.12%\n",
      "Epoch [138/250], Step [300/391], Loss: 0.01182, Train_Acc:99.15%\n",
      "Epoch [138/250], Step [360/391], Loss: 0.03427, Train_Acc:99.14%\n",
      "Accuary on test images:91.14%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [139/250], Step [60/391], Loss: 0.03371, Train_Acc:98.82%\n",
      "Epoch [139/250], Step [120/391], Loss: 0.01641, Train_Acc:98.83%\n",
      "Epoch [139/250], Step [180/391], Loss: 0.02093, Train_Acc:98.94%\n",
      "Epoch [139/250], Step [240/391], Loss: 0.03342, Train_Acc:98.98%\n",
      "Epoch [139/250], Step [300/391], Loss: 0.03485, Train_Acc:98.97%\n",
      "Epoch [139/250], Step [360/391], Loss: 0.02408, Train_Acc:98.99%\n",
      "Accuary on test images:91.13%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [140/250], Step [60/391], Loss: 0.04624, Train_Acc:99.31%\n",
      "Epoch [140/250], Step [120/391], Loss: 0.05959, Train_Acc:99.30%\n",
      "Epoch [140/250], Step [180/391], Loss: 0.02980, Train_Acc:99.34%\n",
      "Epoch [140/250], Step [240/391], Loss: 0.01168, Train_Acc:99.34%\n",
      "Epoch [140/250], Step [300/391], Loss: 0.01164, Train_Acc:99.30%\n",
      "Epoch [140/250], Step [360/391], Loss: 0.02745, Train_Acc:99.30%\n",
      "Accuary on test images:91.44%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [141/250], Step [60/391], Loss: 0.01042, Train_Acc:99.24%\n",
      "Epoch [141/250], Step [120/391], Loss: 0.06472, Train_Acc:99.29%\n",
      "Epoch [141/250], Step [180/391], Loss: 0.03943, Train_Acc:99.32%\n",
      "Epoch [141/250], Step [240/391], Loss: 0.02797, Train_Acc:99.25%\n",
      "Epoch [141/250], Step [300/391], Loss: 0.01657, Train_Acc:99.25%\n",
      "Epoch [141/250], Step [360/391], Loss: 0.01345, Train_Acc:99.24%\n",
      "Accuary on test images:91.15%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [142/250], Step [60/391], Loss: 0.00378, Train_Acc:99.18%\n",
      "Epoch [142/250], Step [120/391], Loss: 0.01512, Train_Acc:99.21%\n",
      "Epoch [142/250], Step [180/391], Loss: 0.05798, Train_Acc:99.28%\n",
      "Epoch [142/250], Step [240/391], Loss: 0.01986, Train_Acc:99.31%\n",
      "Epoch [142/250], Step [300/391], Loss: 0.03610, Train_Acc:99.32%\n",
      "Epoch [142/250], Step [360/391], Loss: 0.01802, Train_Acc:99.26%\n",
      "Accuary on test images:90.95%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [143/250], Step [60/391], Loss: 0.00637, Train_Acc:99.43%\n",
      "Epoch [143/250], Step [120/391], Loss: 0.03081, Train_Acc:99.34%\n",
      "Epoch [143/250], Step [180/391], Loss: 0.03827, Train_Acc:99.28%\n",
      "Epoch [143/250], Step [240/391], Loss: 0.02086, Train_Acc:99.21%\n",
      "Epoch [143/250], Step [300/391], Loss: 0.02483, Train_Acc:99.19%\n",
      "Epoch [143/250], Step [360/391], Loss: 0.03013, Train_Acc:99.17%\n",
      "Accuary on test images:90.95%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [144/250], Step [60/391], Loss: 0.04798, Train_Acc:99.49%\n",
      "Epoch [144/250], Step [120/391], Loss: 0.03382, Train_Acc:99.31%\n",
      "Epoch [144/250], Step [180/391], Loss: 0.00894, Train_Acc:99.31%\n",
      "Epoch [144/250], Step [240/391], Loss: 0.01317, Train_Acc:99.27%\n",
      "Epoch [144/250], Step [300/391], Loss: 0.07017, Train_Acc:99.23%\n",
      "Epoch [144/250], Step [360/391], Loss: 0.03303, Train_Acc:99.21%\n",
      "Accuary on test images:90.55%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [145/250], Step [60/391], Loss: 0.01898, Train_Acc:98.96%\n",
      "Epoch [145/250], Step [120/391], Loss: 0.00484, Train_Acc:99.19%\n",
      "Epoch [145/250], Step [180/391], Loss: 0.02852, Train_Acc:99.23%\n",
      "Epoch [145/250], Step [240/391], Loss: 0.01840, Train_Acc:99.12%\n",
      "Epoch [145/250], Step [300/391], Loss: 0.03222, Train_Acc:99.15%\n",
      "Epoch [145/250], Step [360/391], Loss: 0.01437, Train_Acc:99.14%\n",
      "Accuary on test images:91.74%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [146/250], Step [60/391], Loss: 0.01187, Train_Acc:99.48%\n",
      "Epoch [146/250], Step [120/391], Loss: 0.01446, Train_Acc:99.50%\n",
      "Epoch [146/250], Step [180/391], Loss: 0.02687, Train_Acc:99.40%\n",
      "Epoch [146/250], Step [240/391], Loss: 0.01602, Train_Acc:99.38%\n",
      "Epoch [146/250], Step [300/391], Loss: 0.01541, Train_Acc:99.36%\n",
      "Epoch [146/250], Step [360/391], Loss: 0.00856, Train_Acc:99.33%\n",
      "Accuary on test images:91.87%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [147/250], Step [60/391], Loss: 0.04111, Train_Acc:99.28%\n",
      "Epoch [147/250], Step [120/391], Loss: 0.01831, Train_Acc:99.25%\n",
      "Epoch [147/250], Step [180/391], Loss: 0.00913, Train_Acc:99.32%\n",
      "Epoch [147/250], Step [240/391], Loss: 0.01788, Train_Acc:99.39%\n",
      "Epoch [147/250], Step [300/391], Loss: 0.03355, Train_Acc:99.37%\n",
      "Epoch [147/250], Step [360/391], Loss: 0.04092, Train_Acc:99.31%\n",
      "Accuary on test images:90.73%\n",
      "Best accuracy: 92.19%\n",
      "Epoch [148/250], Step [60/391], Loss: 0.06501, Train_Acc:99.11%\n",
      "Epoch [148/250], Step [120/391], Loss: 0.00605, Train_Acc:99.23%\n",
      "Epoch [148/250], Step [180/391], Loss: 0.00932, Train_Acc:99.23%\n",
      "Epoch [148/250], Step [240/391], Loss: 0.01814, Train_Acc:99.23%\n",
      "Epoch [148/250], Step [300/391], Loss: 0.01543, Train_Acc:99.23%\n",
      "Epoch [148/250], Step [360/391], Loss: 0.04694, Train_Acc:99.21%\n",
      "Accuary on test images:92.32%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [149/250], Step [60/391], Loss: 0.01534, Train_Acc:99.13%\n",
      "Epoch [149/250], Step [120/391], Loss: 0.05048, Train_Acc:99.26%\n",
      "Epoch [149/250], Step [180/391], Loss: 0.01569, Train_Acc:99.19%\n",
      "Epoch [149/250], Step [240/391], Loss: 0.02710, Train_Acc:99.13%\n",
      "Epoch [149/250], Step [300/391], Loss: 0.02038, Train_Acc:99.14%\n",
      "Epoch [149/250], Step [360/391], Loss: 0.01900, Train_Acc:99.15%\n",
      "Accuary on test images:90.40%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [150/250], Step [60/391], Loss: 0.02960, Train_Acc:99.15%\n",
      "Epoch [150/250], Step [120/391], Loss: 0.01481, Train_Acc:99.34%\n",
      "Epoch [150/250], Step [180/391], Loss: 0.00722, Train_Acc:99.35%\n",
      "Epoch [150/250], Step [240/391], Loss: 0.03134, Train_Acc:99.30%\n",
      "Epoch [150/250], Step [300/391], Loss: 0.05107, Train_Acc:99.33%\n",
      "Epoch [150/250], Step [360/391], Loss: 0.04273, Train_Acc:99.30%\n",
      "Accuary on test images:91.40%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [151/250], Step [60/391], Loss: 0.03730, Train_Acc:99.21%\n",
      "Epoch [151/250], Step [120/391], Loss: 0.02455, Train_Acc:99.28%\n",
      "Epoch [151/250], Step [180/391], Loss: 0.01365, Train_Acc:99.29%\n",
      "Epoch [151/250], Step [240/391], Loss: 0.02710, Train_Acc:99.28%\n",
      "Epoch [151/250], Step [300/391], Loss: 0.01913, Train_Acc:99.33%\n",
      "Epoch [151/250], Step [360/391], Loss: 0.01113, Train_Acc:99.31%\n",
      "Accuary on test images:90.59%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [152/250], Step [60/391], Loss: 0.01860, Train_Acc:98.98%\n",
      "Epoch [152/250], Step [120/391], Loss: 0.03739, Train_Acc:98.85%\n",
      "Epoch [152/250], Step [180/391], Loss: 0.05508, Train_Acc:98.79%\n",
      "Epoch [152/250], Step [240/391], Loss: 0.02125, Train_Acc:98.90%\n",
      "Epoch [152/250], Step [300/391], Loss: 0.00719, Train_Acc:98.93%\n",
      "Epoch [152/250], Step [360/391], Loss: 0.04679, Train_Acc:98.96%\n",
      "Accuary on test images:91.72%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [153/250], Step [60/391], Loss: 0.02023, Train_Acc:99.43%\n",
      "Epoch [153/250], Step [120/391], Loss: 0.02214, Train_Acc:99.51%\n",
      "Epoch [153/250], Step [180/391], Loss: 0.01571, Train_Acc:99.54%\n",
      "Epoch [153/250], Step [240/391], Loss: 0.02885, Train_Acc:99.49%\n",
      "Epoch [153/250], Step [300/391], Loss: 0.02139, Train_Acc:99.43%\n",
      "Epoch [153/250], Step [360/391], Loss: 0.02719, Train_Acc:99.40%\n",
      "Accuary on test images:91.77%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [154/250], Step [60/391], Loss: 0.06418, Train_Acc:99.27%\n",
      "Epoch [154/250], Step [120/391], Loss: 0.02143, Train_Acc:99.38%\n",
      "Epoch [154/250], Step [180/391], Loss: 0.00938, Train_Acc:99.44%\n",
      "Epoch [154/250], Step [240/391], Loss: 0.03498, Train_Acc:99.39%\n",
      "Epoch [154/250], Step [300/391], Loss: 0.02044, Train_Acc:99.37%\n",
      "Epoch [154/250], Step [360/391], Loss: 0.00971, Train_Acc:99.31%\n",
      "Accuary on test images:91.72%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [155/250], Step [60/391], Loss: 0.02596, Train_Acc:99.23%\n",
      "Epoch [155/250], Step [120/391], Loss: 0.00901, Train_Acc:99.27%\n",
      "Epoch [155/250], Step [180/391], Loss: 0.02590, Train_Acc:99.21%\n",
      "Epoch [155/250], Step [240/391], Loss: 0.01164, Train_Acc:99.17%\n",
      "Epoch [155/250], Step [300/391], Loss: 0.03319, Train_Acc:99.15%\n",
      "Epoch [155/250], Step [360/391], Loss: 0.01073, Train_Acc:99.19%\n",
      "Accuary on test images:91.45%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [156/250], Step [60/391], Loss: 0.01723, Train_Acc:98.83%\n",
      "Epoch [156/250], Step [120/391], Loss: 0.01135, Train_Acc:98.83%\n",
      "Epoch [156/250], Step [180/391], Loss: 0.03635, Train_Acc:98.97%\n",
      "Epoch [156/250], Step [240/391], Loss: 0.03777, Train_Acc:98.98%\n",
      "Epoch [156/250], Step [300/391], Loss: 0.02440, Train_Acc:98.98%\n",
      "Epoch [156/250], Step [360/391], Loss: 0.01305, Train_Acc:99.02%\n",
      "Accuary on test images:91.94%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [157/250], Step [60/391], Loss: 0.01839, Train_Acc:99.32%\n",
      "Epoch [157/250], Step [120/391], Loss: 0.02975, Train_Acc:99.15%\n",
      "Epoch [157/250], Step [180/391], Loss: 0.01440, Train_Acc:99.11%\n",
      "Epoch [157/250], Step [240/391], Loss: 0.01079, Train_Acc:99.15%\n",
      "Epoch [157/250], Step [300/391], Loss: 0.03745, Train_Acc:99.14%\n",
      "Epoch [157/250], Step [360/391], Loss: 0.04576, Train_Acc:99.12%\n",
      "Accuary on test images:91.37%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [158/250], Step [60/391], Loss: 0.01361, Train_Acc:99.31%\n",
      "Epoch [158/250], Step [120/391], Loss: 0.00577, Train_Acc:99.40%\n",
      "Epoch [158/250], Step [180/391], Loss: 0.00451, Train_Acc:99.36%\n",
      "Epoch [158/250], Step [240/391], Loss: 0.01408, Train_Acc:99.36%\n",
      "Epoch [158/250], Step [300/391], Loss: 0.04416, Train_Acc:99.34%\n",
      "Epoch [158/250], Step [360/391], Loss: 0.01060, Train_Acc:99.30%\n",
      "Accuary on test images:91.31%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [159/250], Step [60/391], Loss: 0.04360, Train_Acc:99.05%\n",
      "Epoch [159/250], Step [120/391], Loss: 0.03959, Train_Acc:99.23%\n",
      "Epoch [159/250], Step [180/391], Loss: 0.00892, Train_Acc:99.27%\n",
      "Epoch [159/250], Step [240/391], Loss: 0.01109, Train_Acc:99.31%\n",
      "Epoch [159/250], Step [300/391], Loss: 0.01233, Train_Acc:99.32%\n",
      "Epoch [159/250], Step [360/391], Loss: 0.03797, Train_Acc:99.30%\n",
      "Accuary on test images:90.93%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [160/250], Step [60/391], Loss: 0.09944, Train_Acc:99.36%\n",
      "Epoch [160/250], Step [120/391], Loss: 0.01295, Train_Acc:99.24%\n",
      "Epoch [160/250], Step [180/391], Loss: 0.02507, Train_Acc:99.22%\n",
      "Epoch [160/250], Step [240/391], Loss: 0.03138, Train_Acc:99.11%\n",
      "Epoch [160/250], Step [300/391], Loss: 0.05001, Train_Acc:99.07%\n",
      "Epoch [160/250], Step [360/391], Loss: 0.01074, Train_Acc:99.08%\n",
      "Accuary on test images:90.81%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [161/250], Step [60/391], Loss: 0.01667, Train_Acc:99.36%\n",
      "Epoch [161/250], Step [120/391], Loss: 0.01017, Train_Acc:99.23%\n",
      "Epoch [161/250], Step [180/391], Loss: 0.02137, Train_Acc:99.24%\n",
      "Epoch [161/250], Step [240/391], Loss: 0.02450, Train_Acc:99.23%\n",
      "Epoch [161/250], Step [300/391], Loss: 0.00273, Train_Acc:99.23%\n",
      "Epoch [161/250], Step [360/391], Loss: 0.03263, Train_Acc:99.21%\n",
      "Accuary on test images:91.13%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [162/250], Step [60/391], Loss: 0.04852, Train_Acc:99.14%\n",
      "Epoch [162/250], Step [120/391], Loss: 0.02769, Train_Acc:99.15%\n",
      "Epoch [162/250], Step [180/391], Loss: 0.02447, Train_Acc:99.19%\n",
      "Epoch [162/250], Step [240/391], Loss: 0.02316, Train_Acc:99.20%\n",
      "Epoch [162/250], Step [300/391], Loss: 0.01203, Train_Acc:99.18%\n",
      "Epoch [162/250], Step [360/391], Loss: 0.02378, Train_Acc:99.14%\n",
      "Accuary on test images:91.38%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [163/250], Step [60/391], Loss: 0.02019, Train_Acc:99.14%\n",
      "Epoch [163/250], Step [120/391], Loss: 0.02688, Train_Acc:99.25%\n",
      "Epoch [163/250], Step [180/391], Loss: 0.01695, Train_Acc:99.21%\n",
      "Epoch [163/250], Step [240/391], Loss: 0.05238, Train_Acc:99.14%\n",
      "Epoch [163/250], Step [300/391], Loss: 0.07464, Train_Acc:99.08%\n",
      "Epoch [163/250], Step [360/391], Loss: 0.02277, Train_Acc:99.06%\n",
      "Accuary on test images:91.43%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [164/250], Step [60/391], Loss: 0.01700, Train_Acc:99.13%\n",
      "Epoch [164/250], Step [120/391], Loss: 0.01334, Train_Acc:99.05%\n",
      "Epoch [164/250], Step [180/391], Loss: 0.01945, Train_Acc:99.14%\n",
      "Epoch [164/250], Step [240/391], Loss: 0.04232, Train_Acc:99.18%\n",
      "Epoch [164/250], Step [300/391], Loss: 0.02545, Train_Acc:99.19%\n",
      "Epoch [164/250], Step [360/391], Loss: 0.01736, Train_Acc:99.19%\n",
      "Accuary on test images:91.79%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [165/250], Step [60/391], Loss: 0.00523, Train_Acc:99.38%\n",
      "Epoch [165/250], Step [120/391], Loss: 0.03750, Train_Acc:99.38%\n",
      "Epoch [165/250], Step [180/391], Loss: 0.02783, Train_Acc:99.31%\n",
      "Epoch [165/250], Step [240/391], Loss: 0.00654, Train_Acc:99.33%\n",
      "Epoch [165/250], Step [300/391], Loss: 0.03400, Train_Acc:99.34%\n",
      "Epoch [165/250], Step [360/391], Loss: 0.00387, Train_Acc:99.32%\n",
      "Accuary on test images:92.10%\n",
      "Best accuracy: 92.32%\n",
      "Epoch [166/250], Step [60/391], Loss: 0.03201, Train_Acc:99.44%\n",
      "Epoch [166/250], Step [120/391], Loss: 0.00601, Train_Acc:99.28%\n",
      "Epoch [166/250], Step [180/391], Loss: 0.01973, Train_Acc:99.33%\n",
      "Epoch [166/250], Step [240/391], Loss: 0.00845, Train_Acc:99.39%\n",
      "Epoch [166/250], Step [300/391], Loss: 0.01212, Train_Acc:99.36%\n",
      "Epoch [166/250], Step [360/391], Loss: 0.01586, Train_Acc:99.37%\n",
      "Accuary on test images:92.39%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [167/250], Step [60/391], Loss: 0.02723, Train_Acc:99.40%\n",
      "Epoch [167/250], Step [120/391], Loss: 0.01659, Train_Acc:99.29%\n",
      "Epoch [167/250], Step [180/391], Loss: 0.01162, Train_Acc:99.26%\n",
      "Epoch [167/250], Step [240/391], Loss: 0.00782, Train_Acc:99.29%\n",
      "Epoch [167/250], Step [300/391], Loss: 0.01762, Train_Acc:99.31%\n",
      "Epoch [167/250], Step [360/391], Loss: 0.01609, Train_Acc:99.32%\n",
      "Accuary on test images:91.78%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [168/250], Step [60/391], Loss: 0.01390, Train_Acc:99.54%\n",
      "Epoch [168/250], Step [120/391], Loss: 0.01325, Train_Acc:99.47%\n",
      "Epoch [168/250], Step [180/391], Loss: 0.00927, Train_Acc:99.44%\n",
      "Epoch [168/250], Step [240/391], Loss: 0.00804, Train_Acc:99.43%\n",
      "Epoch [168/250], Step [300/391], Loss: 0.02439, Train_Acc:99.36%\n",
      "Epoch [168/250], Step [360/391], Loss: 0.06835, Train_Acc:99.28%\n",
      "Accuary on test images:91.59%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [169/250], Step [60/391], Loss: 0.03647, Train_Acc:99.26%\n",
      "Epoch [169/250], Step [120/391], Loss: 0.07467, Train_Acc:99.17%\n",
      "Epoch [169/250], Step [180/391], Loss: 0.01231, Train_Acc:99.14%\n",
      "Epoch [169/250], Step [240/391], Loss: 0.07421, Train_Acc:99.10%\n",
      "Epoch [169/250], Step [300/391], Loss: 0.01429, Train_Acc:99.11%\n",
      "Epoch [169/250], Step [360/391], Loss: 0.02525, Train_Acc:99.14%\n",
      "Accuary on test images:91.63%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [170/250], Step [60/391], Loss: 0.01138, Train_Acc:99.34%\n",
      "Epoch [170/250], Step [120/391], Loss: 0.00955, Train_Acc:99.36%\n",
      "Epoch [170/250], Step [180/391], Loss: 0.00548, Train_Acc:99.35%\n",
      "Epoch [170/250], Step [240/391], Loss: 0.01874, Train_Acc:99.25%\n",
      "Epoch [170/250], Step [300/391], Loss: 0.00797, Train_Acc:99.27%\n",
      "Epoch [170/250], Step [360/391], Loss: 0.01283, Train_Acc:99.27%\n",
      "Accuary on test images:91.49%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [171/250], Step [60/391], Loss: 0.04823, Train_Acc:98.91%\n",
      "Epoch [171/250], Step [120/391], Loss: 0.01016, Train_Acc:98.93%\n",
      "Epoch [171/250], Step [180/391], Loss: 0.02512, Train_Acc:98.97%\n",
      "Epoch [171/250], Step [240/391], Loss: 0.00672, Train_Acc:98.95%\n",
      "Epoch [171/250], Step [300/391], Loss: 0.02656, Train_Acc:98.97%\n",
      "Epoch [171/250], Step [360/391], Loss: 0.03474, Train_Acc:99.01%\n",
      "Accuary on test images:91.65%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [172/250], Step [60/391], Loss: 0.03963, Train_Acc:98.96%\n",
      "Epoch [172/250], Step [120/391], Loss: 0.01756, Train_Acc:99.05%\n",
      "Epoch [172/250], Step [180/391], Loss: 0.01303, Train_Acc:99.11%\n",
      "Epoch [172/250], Step [240/391], Loss: 0.02830, Train_Acc:99.14%\n",
      "Epoch [172/250], Step [300/391], Loss: 0.03798, Train_Acc:99.13%\n",
      "Epoch [172/250], Step [360/391], Loss: 0.04751, Train_Acc:99.13%\n",
      "Accuary on test images:91.75%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [173/250], Step [60/391], Loss: 0.03174, Train_Acc:99.01%\n",
      "Epoch [173/250], Step [120/391], Loss: 0.01122, Train_Acc:99.11%\n",
      "Epoch [173/250], Step [180/391], Loss: 0.01079, Train_Acc:99.22%\n",
      "Epoch [173/250], Step [240/391], Loss: 0.02232, Train_Acc:99.24%\n",
      "Epoch [173/250], Step [300/391], Loss: 0.00614, Train_Acc:99.24%\n",
      "Epoch [173/250], Step [360/391], Loss: 0.04107, Train_Acc:99.23%\n",
      "Accuary on test images:91.17%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [174/250], Step [60/391], Loss: 0.03024, Train_Acc:99.30%\n",
      "Epoch [174/250], Step [120/391], Loss: 0.00909, Train_Acc:99.29%\n",
      "Epoch [174/250], Step [180/391], Loss: 0.02980, Train_Acc:99.23%\n",
      "Epoch [174/250], Step [240/391], Loss: 0.03902, Train_Acc:99.26%\n",
      "Epoch [174/250], Step [300/391], Loss: 0.00984, Train_Acc:99.26%\n",
      "Epoch [174/250], Step [360/391], Loss: 0.06439, Train_Acc:99.24%\n",
      "Accuary on test images:91.51%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [175/250], Step [60/391], Loss: 0.00941, Train_Acc:99.17%\n",
      "Epoch [175/250], Step [120/391], Loss: 0.00698, Train_Acc:99.19%\n",
      "Epoch [175/250], Step [180/391], Loss: 0.02522, Train_Acc:99.17%\n",
      "Epoch [175/250], Step [240/391], Loss: 0.05069, Train_Acc:99.18%\n",
      "Epoch [175/250], Step [300/391], Loss: 0.02218, Train_Acc:99.16%\n",
      "Epoch [175/250], Step [360/391], Loss: 0.03605, Train_Acc:99.06%\n",
      "Accuary on test images:91.20%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [176/250], Step [60/391], Loss: 0.02830, Train_Acc:99.24%\n",
      "Epoch [176/250], Step [120/391], Loss: 0.00766, Train_Acc:99.35%\n",
      "Epoch [176/250], Step [180/391], Loss: 0.05584, Train_Acc:99.25%\n",
      "Epoch [176/250], Step [240/391], Loss: 0.02869, Train_Acc:99.21%\n",
      "Epoch [176/250], Step [300/391], Loss: 0.03622, Train_Acc:99.16%\n",
      "Epoch [176/250], Step [360/391], Loss: 0.00813, Train_Acc:99.13%\n",
      "Accuary on test images:90.54%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [177/250], Step [60/391], Loss: 0.03005, Train_Acc:99.04%\n",
      "Epoch [177/250], Step [120/391], Loss: 0.00914, Train_Acc:99.06%\n",
      "Epoch [177/250], Step [180/391], Loss: 0.02324, Train_Acc:99.18%\n",
      "Epoch [177/250], Step [240/391], Loss: 0.01304, Train_Acc:99.25%\n",
      "Epoch [177/250], Step [300/391], Loss: 0.00723, Train_Acc:99.27%\n",
      "Epoch [177/250], Step [360/391], Loss: 0.01630, Train_Acc:99.27%\n",
      "Accuary on test images:92.08%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [178/250], Step [60/391], Loss: 0.00952, Train_Acc:99.32%\n",
      "Epoch [178/250], Step [120/391], Loss: 0.00694, Train_Acc:99.44%\n",
      "Epoch [178/250], Step [180/391], Loss: 0.00657, Train_Acc:99.47%\n",
      "Epoch [178/250], Step [240/391], Loss: 0.00395, Train_Acc:99.46%\n",
      "Epoch [178/250], Step [300/391], Loss: 0.01917, Train_Acc:99.42%\n",
      "Epoch [178/250], Step [360/391], Loss: 0.02513, Train_Acc:99.38%\n",
      "Accuary on test images:91.42%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [179/250], Step [60/391], Loss: 0.00355, Train_Acc:99.40%\n",
      "Epoch [179/250], Step [120/391], Loss: 0.04829, Train_Acc:99.34%\n",
      "Epoch [179/250], Step [180/391], Loss: 0.02410, Train_Acc:99.33%\n",
      "Epoch [179/250], Step [240/391], Loss: 0.01719, Train_Acc:99.31%\n",
      "Epoch [179/250], Step [300/391], Loss: 0.02338, Train_Acc:99.30%\n",
      "Epoch [179/250], Step [360/391], Loss: 0.05293, Train_Acc:99.26%\n",
      "Accuary on test images:91.73%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [180/250], Step [60/391], Loss: 0.03312, Train_Acc:99.23%\n",
      "Epoch [180/250], Step [120/391], Loss: 0.04409, Train_Acc:99.30%\n",
      "Epoch [180/250], Step [180/391], Loss: 0.01404, Train_Acc:99.33%\n",
      "Epoch [180/250], Step [240/391], Loss: 0.07686, Train_Acc:99.37%\n",
      "Epoch [180/250], Step [300/391], Loss: 0.01864, Train_Acc:99.34%\n",
      "Epoch [180/250], Step [360/391], Loss: 0.00641, Train_Acc:99.32%\n",
      "Accuary on test images:90.91%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [181/250], Step [60/391], Loss: 0.02227, Train_Acc:99.15%\n",
      "Epoch [181/250], Step [120/391], Loss: 0.00960, Train_Acc:99.21%\n",
      "Epoch [181/250], Step [180/391], Loss: 0.01344, Train_Acc:99.10%\n",
      "Epoch [181/250], Step [240/391], Loss: 0.04495, Train_Acc:99.07%\n",
      "Epoch [181/250], Step [300/391], Loss: 0.01574, Train_Acc:99.08%\n",
      "Epoch [181/250], Step [360/391], Loss: 0.02135, Train_Acc:99.10%\n",
      "Accuary on test images:91.42%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [182/250], Step [60/391], Loss: 0.01780, Train_Acc:99.27%\n",
      "Epoch [182/250], Step [120/391], Loss: 0.03453, Train_Acc:99.25%\n",
      "Epoch [182/250], Step [180/391], Loss: 0.01166, Train_Acc:99.23%\n",
      "Epoch [182/250], Step [240/391], Loss: 0.02854, Train_Acc:99.24%\n",
      "Epoch [182/250], Step [300/391], Loss: 0.03636, Train_Acc:99.28%\n",
      "Epoch [182/250], Step [360/391], Loss: 0.02249, Train_Acc:99.26%\n",
      "Accuary on test images:91.62%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [183/250], Step [60/391], Loss: 0.05162, Train_Acc:99.39%\n",
      "Epoch [183/250], Step [120/391], Loss: 0.04501, Train_Acc:99.33%\n",
      "Epoch [183/250], Step [180/391], Loss: 0.03183, Train_Acc:99.24%\n",
      "Epoch [183/250], Step [240/391], Loss: 0.04309, Train_Acc:99.18%\n",
      "Epoch [183/250], Step [300/391], Loss: 0.00490, Train_Acc:99.16%\n",
      "Epoch [183/250], Step [360/391], Loss: 0.05372, Train_Acc:99.15%\n",
      "Accuary on test images:91.73%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [184/250], Step [60/391], Loss: 0.01232, Train_Acc:99.27%\n",
      "Epoch [184/250], Step [120/391], Loss: 0.02514, Train_Acc:99.39%\n",
      "Epoch [184/250], Step [180/391], Loss: 0.00706, Train_Acc:99.32%\n",
      "Epoch [184/250], Step [240/391], Loss: 0.02487, Train_Acc:99.32%\n",
      "Epoch [184/250], Step [300/391], Loss: 0.01952, Train_Acc:99.27%\n",
      "Epoch [184/250], Step [360/391], Loss: 0.01472, Train_Acc:99.24%\n",
      "Accuary on test images:91.49%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [185/250], Step [60/391], Loss: 0.00562, Train_Acc:99.23%\n",
      "Epoch [185/250], Step [120/391], Loss: 0.02280, Train_Acc:99.31%\n",
      "Epoch [185/250], Step [180/391], Loss: 0.01692, Train_Acc:99.42%\n",
      "Epoch [185/250], Step [240/391], Loss: 0.04215, Train_Acc:99.46%\n",
      "Epoch [185/250], Step [300/391], Loss: 0.01646, Train_Acc:99.42%\n",
      "Epoch [185/250], Step [360/391], Loss: 0.02126, Train_Acc:99.37%\n",
      "Accuary on test images:90.79%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [186/250], Step [60/391], Loss: 0.01757, Train_Acc:99.24%\n",
      "Epoch [186/250], Step [120/391], Loss: 0.01544, Train_Acc:99.31%\n",
      "Epoch [186/250], Step [180/391], Loss: 0.04019, Train_Acc:99.30%\n",
      "Epoch [186/250], Step [240/391], Loss: 0.02375, Train_Acc:99.28%\n",
      "Epoch [186/250], Step [300/391], Loss: 0.00516, Train_Acc:99.28%\n",
      "Epoch [186/250], Step [360/391], Loss: 0.01368, Train_Acc:99.27%\n",
      "Accuary on test images:91.41%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [187/250], Step [60/391], Loss: 0.01380, Train_Acc:99.27%\n",
      "Epoch [187/250], Step [120/391], Loss: 0.00444, Train_Acc:99.44%\n",
      "Epoch [187/250], Step [180/391], Loss: 0.02176, Train_Acc:99.44%\n",
      "Epoch [187/250], Step [240/391], Loss: 0.02841, Train_Acc:99.38%\n",
      "Epoch [187/250], Step [300/391], Loss: 0.02046, Train_Acc:99.35%\n",
      "Epoch [187/250], Step [360/391], Loss: 0.00936, Train_Acc:99.32%\n",
      "Accuary on test images:91.29%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [188/250], Step [60/391], Loss: 0.01300, Train_Acc:99.26%\n",
      "Epoch [188/250], Step [120/391], Loss: 0.02160, Train_Acc:99.13%\n",
      "Epoch [188/250], Step [180/391], Loss: 0.01083, Train_Acc:99.15%\n",
      "Epoch [188/250], Step [240/391], Loss: 0.02296, Train_Acc:99.16%\n",
      "Epoch [188/250], Step [300/391], Loss: 0.06737, Train_Acc:99.16%\n",
      "Epoch [188/250], Step [360/391], Loss: 0.02957, Train_Acc:99.13%\n",
      "Accuary on test images:92.14%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [189/250], Step [60/391], Loss: 0.00321, Train_Acc:99.48%\n",
      "Epoch [189/250], Step [120/391], Loss: 0.02151, Train_Acc:99.34%\n",
      "Epoch [189/250], Step [180/391], Loss: 0.01433, Train_Acc:99.11%\n",
      "Epoch [189/250], Step [240/391], Loss: 0.02626, Train_Acc:99.02%\n",
      "Epoch [189/250], Step [300/391], Loss: 0.01057, Train_Acc:99.05%\n",
      "Epoch [189/250], Step [360/391], Loss: 0.02466, Train_Acc:99.10%\n",
      "Accuary on test images:91.24%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [190/250], Step [60/391], Loss: 0.01662, Train_Acc:99.24%\n",
      "Epoch [190/250], Step [120/391], Loss: 0.03446, Train_Acc:99.28%\n",
      "Epoch [190/250], Step [180/391], Loss: 0.01118, Train_Acc:99.32%\n",
      "Epoch [190/250], Step [240/391], Loss: 0.02909, Train_Acc:99.37%\n",
      "Epoch [190/250], Step [300/391], Loss: 0.01105, Train_Acc:99.41%\n",
      "Epoch [190/250], Step [360/391], Loss: 0.01372, Train_Acc:99.37%\n",
      "Accuary on test images:91.63%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [191/250], Step [60/391], Loss: 0.02506, Train_Acc:99.34%\n",
      "Epoch [191/250], Step [120/391], Loss: 0.00611, Train_Acc:99.38%\n",
      "Epoch [191/250], Step [180/391], Loss: 0.02065, Train_Acc:99.34%\n",
      "Epoch [191/250], Step [240/391], Loss: 0.01842, Train_Acc:99.34%\n",
      "Epoch [191/250], Step [300/391], Loss: 0.02705, Train_Acc:99.29%\n",
      "Epoch [191/250], Step [360/391], Loss: 0.02755, Train_Acc:99.24%\n",
      "Accuary on test images:91.67%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [192/250], Step [60/391], Loss: 0.01877, Train_Acc:99.22%\n",
      "Epoch [192/250], Step [120/391], Loss: 0.03326, Train_Acc:99.30%\n",
      "Epoch [192/250], Step [180/391], Loss: 0.03097, Train_Acc:99.29%\n",
      "Epoch [192/250], Step [240/391], Loss: 0.00486, Train_Acc:99.31%\n",
      "Epoch [192/250], Step [300/391], Loss: 0.02847, Train_Acc:99.29%\n",
      "Epoch [192/250], Step [360/391], Loss: 0.02093, Train_Acc:99.26%\n",
      "Accuary on test images:91.36%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [193/250], Step [60/391], Loss: 0.01802, Train_Acc:99.18%\n",
      "Epoch [193/250], Step [120/391], Loss: 0.02523, Train_Acc:99.15%\n",
      "Epoch [193/250], Step [180/391], Loss: 0.02638, Train_Acc:99.17%\n",
      "Epoch [193/250], Step [240/391], Loss: 0.05751, Train_Acc:99.11%\n",
      "Epoch [193/250], Step [300/391], Loss: 0.04817, Train_Acc:99.06%\n",
      "Epoch [193/250], Step [360/391], Loss: 0.07070, Train_Acc:99.03%\n",
      "Accuary on test images:91.47%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [194/250], Step [60/391], Loss: 0.02094, Train_Acc:98.82%\n",
      "Epoch [194/250], Step [120/391], Loss: 0.00784, Train_Acc:99.04%\n",
      "Epoch [194/250], Step [180/391], Loss: 0.02493, Train_Acc:99.11%\n",
      "Epoch [194/250], Step [240/391], Loss: 0.00514, Train_Acc:99.17%\n",
      "Epoch [194/250], Step [300/391], Loss: 0.03804, Train_Acc:99.22%\n",
      "Epoch [194/250], Step [360/391], Loss: 0.00568, Train_Acc:99.25%\n",
      "Accuary on test images:90.92%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [195/250], Step [60/391], Loss: 0.06037, Train_Acc:99.19%\n",
      "Epoch [195/250], Step [120/391], Loss: 0.01817, Train_Acc:99.28%\n",
      "Epoch [195/250], Step [180/391], Loss: 0.02293, Train_Acc:99.29%\n",
      "Epoch [195/250], Step [240/391], Loss: 0.04772, Train_Acc:99.32%\n",
      "Epoch [195/250], Step [300/391], Loss: 0.03607, Train_Acc:99.33%\n",
      "Epoch [195/250], Step [360/391], Loss: 0.05153, Train_Acc:99.29%\n",
      "Accuary on test images:91.66%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [196/250], Step [60/391], Loss: 0.01367, Train_Acc:99.28%\n",
      "Epoch [196/250], Step [120/391], Loss: 0.03642, Train_Acc:99.20%\n",
      "Epoch [196/250], Step [180/391], Loss: 0.02375, Train_Acc:99.24%\n",
      "Epoch [196/250], Step [240/391], Loss: 0.02196, Train_Acc:99.28%\n",
      "Epoch [196/250], Step [300/391], Loss: 0.02342, Train_Acc:99.18%\n",
      "Epoch [196/250], Step [360/391], Loss: 0.02109, Train_Acc:99.20%\n",
      "Accuary on test images:90.82%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [197/250], Step [60/391], Loss: 0.04203, Train_Acc:99.36%\n",
      "Epoch [197/250], Step [120/391], Loss: 0.00772, Train_Acc:99.54%\n",
      "Epoch [197/250], Step [180/391], Loss: 0.00893, Train_Acc:99.51%\n",
      "Epoch [197/250], Step [240/391], Loss: 0.03659, Train_Acc:99.45%\n",
      "Epoch [197/250], Step [300/391], Loss: 0.04869, Train_Acc:99.39%\n",
      "Epoch [197/250], Step [360/391], Loss: 0.02926, Train_Acc:99.34%\n",
      "Accuary on test images:91.56%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [198/250], Step [60/391], Loss: 0.02511, Train_Acc:99.18%\n",
      "Epoch [198/250], Step [120/391], Loss: 0.04679, Train_Acc:99.27%\n",
      "Epoch [198/250], Step [180/391], Loss: 0.02569, Train_Acc:99.31%\n",
      "Epoch [198/250], Step [240/391], Loss: 0.00780, Train_Acc:99.33%\n",
      "Epoch [198/250], Step [300/391], Loss: 0.03517, Train_Acc:99.34%\n",
      "Epoch [198/250], Step [360/391], Loss: 0.02055, Train_Acc:99.31%\n",
      "Accuary on test images:91.80%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [199/250], Step [60/391], Loss: 0.03342, Train_Acc:99.13%\n",
      "Epoch [199/250], Step [120/391], Loss: 0.01055, Train_Acc:99.16%\n",
      "Epoch [199/250], Step [180/391], Loss: 0.04706, Train_Acc:99.20%\n",
      "Epoch [199/250], Step [240/391], Loss: 0.01336, Train_Acc:99.19%\n",
      "Epoch [199/250], Step [300/391], Loss: 0.03797, Train_Acc:99.18%\n",
      "Epoch [199/250], Step [360/391], Loss: 0.02036, Train_Acc:99.17%\n",
      "Accuary on test images:92.07%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [200/250], Step [60/391], Loss: 0.01249, Train_Acc:99.36%\n",
      "Epoch [200/250], Step [120/391], Loss: 0.02384, Train_Acc:99.32%\n",
      "Epoch [200/250], Step [180/391], Loss: 0.03983, Train_Acc:99.25%\n",
      "Epoch [200/250], Step [240/391], Loss: 0.05518, Train_Acc:99.21%\n",
      "Epoch [200/250], Step [300/391], Loss: 0.02417, Train_Acc:99.26%\n",
      "Epoch [200/250], Step [360/391], Loss: 0.03745, Train_Acc:99.24%\n",
      "Accuary on test images:90.69%\n",
      "Best accuracy: 92.39%\n",
      "Epoch [201/250], Step [60/391], Loss: 0.03689, Train_Acc:99.36%\n",
      "Epoch [201/250], Step [120/391], Loss: 0.00641, Train_Acc:99.52%\n",
      "Epoch [201/250], Step [180/391], Loss: 0.03348, Train_Acc:99.59%\n",
      "Epoch [201/250], Step [240/391], Loss: 0.00876, Train_Acc:99.65%\n",
      "Epoch [201/250], Step [300/391], Loss: 0.00467, Train_Acc:99.68%\n",
      "Epoch [201/250], Step [360/391], Loss: 0.00407, Train_Acc:99.72%\n",
      "Accuary on test images:93.27%\n",
      "Best accuracy: 93.27%\n",
      "Epoch [202/250], Step [60/391], Loss: 0.01136, Train_Acc:99.83%\n",
      "Epoch [202/250], Step [120/391], Loss: 0.00264, Train_Acc:99.87%\n",
      "Epoch [202/250], Step [180/391], Loss: 0.00270, Train_Acc:99.88%\n",
      "Epoch [202/250], Step [240/391], Loss: 0.01509, Train_Acc:99.88%\n",
      "Epoch [202/250], Step [300/391], Loss: 0.00584, Train_Acc:99.87%\n",
      "Epoch [202/250], Step [360/391], Loss: 0.00370, Train_Acc:99.88%\n",
      "Accuary on test images:93.40%\n",
      "Best accuracy: 93.40%\n",
      "Epoch [203/250], Step [60/391], Loss: 0.00468, Train_Acc:99.88%\n",
      "Epoch [203/250], Step [120/391], Loss: 0.00224, Train_Acc:99.88%\n",
      "Epoch [203/250], Step [180/391], Loss: 0.00596, Train_Acc:99.90%\n",
      "Epoch [203/250], Step [240/391], Loss: 0.00276, Train_Acc:99.91%\n",
      "Epoch [203/250], Step [300/391], Loss: 0.00368, Train_Acc:99.92%\n",
      "Epoch [203/250], Step [360/391], Loss: 0.03011, Train_Acc:99.92%\n",
      "Accuary on test images:93.38%\n",
      "Best accuracy: 93.40%\n",
      "Epoch [204/250], Step [60/391], Loss: 0.00238, Train_Acc:99.96%\n",
      "Epoch [204/250], Step [120/391], Loss: 0.00248, Train_Acc:99.97%\n",
      "Epoch [204/250], Step [180/391], Loss: 0.00241, Train_Acc:99.97%\n",
      "Epoch [204/250], Step [240/391], Loss: 0.00864, Train_Acc:99.96%\n",
      "Epoch [204/250], Step [300/391], Loss: 0.00600, Train_Acc:99.96%\n",
      "Epoch [204/250], Step [360/391], Loss: 0.00423, Train_Acc:99.96%\n",
      "Accuary on test images:93.47%\n",
      "Best accuracy: 93.47%\n",
      "Epoch [205/250], Step [60/391], Loss: 0.01417, Train_Acc:99.95%\n",
      "Epoch [205/250], Step [120/391], Loss: 0.00336, Train_Acc:99.97%\n",
      "Epoch [205/250], Step [180/391], Loss: 0.00263, Train_Acc:99.97%\n",
      "Epoch [205/250], Step [240/391], Loss: 0.00267, Train_Acc:99.97%\n",
      "Epoch [205/250], Step [300/391], Loss: 0.00186, Train_Acc:99.97%\n",
      "Epoch [205/250], Step [360/391], Loss: 0.00272, Train_Acc:99.97%\n",
      "Accuary on test images:93.46%\n",
      "Best accuracy: 93.47%\n",
      "Epoch [206/250], Step [60/391], Loss: 0.01296, Train_Acc:99.96%\n",
      "Epoch [206/250], Step [120/391], Loss: 0.00135, Train_Acc:99.94%\n",
      "Epoch [206/250], Step [180/391], Loss: 0.00299, Train_Acc:99.96%\n",
      "Epoch [206/250], Step [240/391], Loss: 0.00596, Train_Acc:99.95%\n",
      "Epoch [206/250], Step [300/391], Loss: 0.00155, Train_Acc:99.96%\n",
      "Epoch [206/250], Step [360/391], Loss: 0.00297, Train_Acc:99.97%\n",
      "Accuary on test images:93.51%\n",
      "Best accuracy: 93.51%\n",
      "Epoch [207/250], Step [60/391], Loss: 0.00219, Train_Acc:100.00%\n",
      "Epoch [207/250], Step [120/391], Loss: 0.00440, Train_Acc:99.98%\n",
      "Epoch [207/250], Step [180/391], Loss: 0.00169, Train_Acc:99.98%\n",
      "Epoch [207/250], Step [240/391], Loss: 0.00618, Train_Acc:99.97%\n",
      "Epoch [207/250], Step [300/391], Loss: 0.00431, Train_Acc:99.97%\n",
      "Epoch [207/250], Step [360/391], Loss: 0.00243, Train_Acc:99.98%\n",
      "Accuary on test images:93.48%\n",
      "Best accuracy: 93.51%\n",
      "Epoch [208/250], Step [60/391], Loss: 0.00325, Train_Acc:99.97%\n",
      "Epoch [208/250], Step [120/391], Loss: 0.00433, Train_Acc:99.97%\n",
      "Epoch [208/250], Step [180/391], Loss: 0.00381, Train_Acc:99.97%\n",
      "Epoch [208/250], Step [240/391], Loss: 0.00135, Train_Acc:99.97%\n",
      "Epoch [208/250], Step [300/391], Loss: 0.00219, Train_Acc:99.97%\n",
      "Epoch [208/250], Step [360/391], Loss: 0.00191, Train_Acc:99.97%\n",
      "Accuary on test images:93.62%\n",
      "Best accuracy: 93.62%\n",
      "Epoch [209/250], Step [60/391], Loss: 0.00156, Train_Acc:99.97%\n",
      "Epoch [209/250], Step [120/391], Loss: 0.00125, Train_Acc:99.99%\n",
      "Epoch [209/250], Step [180/391], Loss: 0.00401, Train_Acc:99.99%\n",
      "Epoch [209/250], Step [240/391], Loss: 0.00198, Train_Acc:99.99%\n",
      "Epoch [209/250], Step [300/391], Loss: 0.00176, Train_Acc:99.99%\n",
      "Epoch [209/250], Step [360/391], Loss: 0.00119, Train_Acc:99.98%\n",
      "Accuary on test images:93.70%\n",
      "Best accuracy: 93.70%\n",
      "Epoch [210/250], Step [60/391], Loss: 0.00466, Train_Acc:100.00%\n",
      "Epoch [210/250], Step [120/391], Loss: 0.00233, Train_Acc:99.99%\n",
      "Epoch [210/250], Step [180/391], Loss: 0.00213, Train_Acc:99.97%\n",
      "Epoch [210/250], Step [240/391], Loss: 0.00348, Train_Acc:99.97%\n",
      "Epoch [210/250], Step [300/391], Loss: 0.00178, Train_Acc:99.97%\n",
      "Epoch [210/250], Step [360/391], Loss: 0.00209, Train_Acc:99.97%\n",
      "Accuary on test images:93.80%\n",
      "Best accuracy: 93.80%\n",
      "Epoch [211/250], Step [60/391], Loss: 0.00262, Train_Acc:99.97%\n",
      "Epoch [211/250], Step [120/391], Loss: 0.00665, Train_Acc:99.97%\n",
      "Epoch [211/250], Step [180/391], Loss: 0.00241, Train_Acc:99.98%\n",
      "Epoch [211/250], Step [240/391], Loss: 0.00133, Train_Acc:99.98%\n",
      "Epoch [211/250], Step [300/391], Loss: 0.00184, Train_Acc:99.99%\n",
      "Epoch [211/250], Step [360/391], Loss: 0.00276, Train_Acc:99.98%\n",
      "Accuary on test images:93.72%\n",
      "Best accuracy: 93.80%\n",
      "Epoch [212/250], Step [60/391], Loss: 0.00138, Train_Acc:99.96%\n",
      "Epoch [212/250], Step [120/391], Loss: 0.00249, Train_Acc:99.97%\n",
      "Epoch [212/250], Step [180/391], Loss: 0.00126, Train_Acc:99.98%\n",
      "Epoch [212/250], Step [240/391], Loss: 0.00278, Train_Acc:99.98%\n",
      "Epoch [212/250], Step [300/391], Loss: 0.00244, Train_Acc:99.98%\n",
      "Epoch [212/250], Step [360/391], Loss: 0.00197, Train_Acc:99.98%\n",
      "Accuary on test images:93.69%\n",
      "Best accuracy: 93.80%\n",
      "Epoch [213/250], Step [60/391], Loss: 0.00147, Train_Acc:100.00%\n",
      "Epoch [213/250], Step [120/391], Loss: 0.00175, Train_Acc:99.99%\n",
      "Epoch [213/250], Step [180/391], Loss: 0.00415, Train_Acc:99.99%\n",
      "Epoch [213/250], Step [240/391], Loss: 0.00109, Train_Acc:99.99%\n",
      "Epoch [213/250], Step [300/391], Loss: 0.00188, Train_Acc:99.99%\n",
      "Epoch [213/250], Step [360/391], Loss: 0.00146, Train_Acc:99.98%\n",
      "Accuary on test images:93.80%\n",
      "Best accuracy: 93.80%\n",
      "Epoch [214/250], Step [60/391], Loss: 0.00146, Train_Acc:100.00%\n",
      "Epoch [214/250], Step [120/391], Loss: 0.00156, Train_Acc:99.99%\n",
      "Epoch [214/250], Step [180/391], Loss: 0.00163, Train_Acc:99.99%\n",
      "Epoch [214/250], Step [240/391], Loss: 0.00281, Train_Acc:99.98%\n",
      "Epoch [214/250], Step [300/391], Loss: 0.00179, Train_Acc:99.98%\n",
      "Epoch [214/250], Step [360/391], Loss: 0.00479, Train_Acc:99.99%\n",
      "Accuary on test images:93.86%\n",
      "Best accuracy: 93.86%\n",
      "Epoch [215/250], Step [60/391], Loss: 0.00400, Train_Acc:99.97%\n",
      "Epoch [215/250], Step [120/391], Loss: 0.00147, Train_Acc:99.99%\n",
      "Epoch [215/250], Step [180/391], Loss: 0.00355, Train_Acc:99.99%\n",
      "Epoch [215/250], Step [240/391], Loss: 0.00236, Train_Acc:99.99%\n",
      "Epoch [215/250], Step [300/391], Loss: 0.00130, Train_Acc:99.99%\n",
      "Epoch [215/250], Step [360/391], Loss: 0.00184, Train_Acc:99.99%\n",
      "Accuary on test images:93.80%\n",
      "Best accuracy: 93.86%\n",
      "Epoch [216/250], Step [60/391], Loss: 0.00219, Train_Acc:99.96%\n",
      "Epoch [216/250], Step [120/391], Loss: 0.00217, Train_Acc:99.98%\n",
      "Epoch [216/250], Step [180/391], Loss: 0.00267, Train_Acc:99.99%\n",
      "Epoch [216/250], Step [240/391], Loss: 0.00177, Train_Acc:99.99%\n",
      "Epoch [216/250], Step [300/391], Loss: 0.00121, Train_Acc:99.99%\n",
      "Epoch [216/250], Step [360/391], Loss: 0.00193, Train_Acc:99.98%\n",
      "Accuary on test images:93.90%\n",
      "Best accuracy: 93.90%\n",
      "Epoch [217/250], Step [60/391], Loss: 0.00140, Train_Acc:99.99%\n",
      "Epoch [217/250], Step [120/391], Loss: 0.00128, Train_Acc:99.99%\n",
      "Epoch [217/250], Step [180/391], Loss: 0.00135, Train_Acc:99.99%\n",
      "Epoch [217/250], Step [240/391], Loss: 0.00458, Train_Acc:99.99%\n",
      "Epoch [217/250], Step [300/391], Loss: 0.00113, Train_Acc:99.99%\n",
      "Epoch [217/250], Step [360/391], Loss: 0.00166, Train_Acc:99.99%\n",
      "Accuary on test images:93.75%\n",
      "Best accuracy: 93.90%\n",
      "Epoch [218/250], Step [60/391], Loss: 0.00106, Train_Acc:100.00%\n",
      "Epoch [218/250], Step [120/391], Loss: 0.00331, Train_Acc:100.00%\n",
      "Epoch [218/250], Step [180/391], Loss: 0.00238, Train_Acc:100.00%\n",
      "Epoch [218/250], Step [240/391], Loss: 0.00297, Train_Acc:99.99%\n",
      "Epoch [218/250], Step [300/391], Loss: 0.00221, Train_Acc:99.99%\n",
      "Epoch [218/250], Step [360/391], Loss: 0.00092, Train_Acc:99.99%\n",
      "Accuary on test images:93.96%\n",
      "Best accuracy: 93.96%\n",
      "Epoch [219/250], Step [60/391], Loss: 0.00150, Train_Acc:100.00%\n",
      "Epoch [219/250], Step [120/391], Loss: 0.00191, Train_Acc:99.99%\n",
      "Epoch [219/250], Step [180/391], Loss: 0.00237, Train_Acc:99.98%\n",
      "Epoch [219/250], Step [240/391], Loss: 0.00111, Train_Acc:99.99%\n",
      "Epoch [219/250], Step [300/391], Loss: 0.00148, Train_Acc:99.99%\n",
      "Epoch [219/250], Step [360/391], Loss: 0.00205, Train_Acc:99.99%\n",
      "Accuary on test images:93.88%\n",
      "Best accuracy: 93.96%\n",
      "Epoch [220/250], Step [60/391], Loss: 0.00573, Train_Acc:99.97%\n",
      "Epoch [220/250], Step [120/391], Loss: 0.00445, Train_Acc:99.99%\n",
      "Epoch [220/250], Step [180/391], Loss: 0.00220, Train_Acc:99.99%\n",
      "Epoch [220/250], Step [240/391], Loss: 0.00108, Train_Acc:99.99%\n",
      "Epoch [220/250], Step [300/391], Loss: 0.00178, Train_Acc:99.99%\n",
      "Epoch [220/250], Step [360/391], Loss: 0.00215, Train_Acc:99.99%\n",
      "Accuary on test images:93.93%\n",
      "Best accuracy: 93.96%\n",
      "Epoch [221/250], Step [60/391], Loss: 0.00124, Train_Acc:99.97%\n",
      "Epoch [221/250], Step [120/391], Loss: 0.00098, Train_Acc:99.97%\n",
      "Epoch [221/250], Step [180/391], Loss: 0.00140, Train_Acc:99.98%\n",
      "Epoch [221/250], Step [240/391], Loss: 0.00287, Train_Acc:99.99%\n",
      "Epoch [221/250], Step [300/391], Loss: 0.00135, Train_Acc:99.99%\n",
      "Epoch [221/250], Step [360/391], Loss: 0.00132, Train_Acc:99.99%\n",
      "Accuary on test images:93.99%\n",
      "Best accuracy: 93.99%\n",
      "Epoch [222/250], Step [60/391], Loss: 0.00146, Train_Acc:100.00%\n",
      "Epoch [222/250], Step [120/391], Loss: 0.00119, Train_Acc:99.99%\n",
      "Epoch [222/250], Step [180/391], Loss: 0.00116, Train_Acc:99.99%\n",
      "Epoch [222/250], Step [240/391], Loss: 0.00193, Train_Acc:99.99%\n",
      "Epoch [222/250], Step [300/391], Loss: 0.00182, Train_Acc:99.99%\n",
      "Epoch [222/250], Step [360/391], Loss: 0.00172, Train_Acc:99.99%\n",
      "Accuary on test images:93.85%\n",
      "Best accuracy: 93.99%\n",
      "Epoch [223/250], Step [60/391], Loss: 0.00161, Train_Acc:99.99%\n",
      "Epoch [223/250], Step [120/391], Loss: 0.00116, Train_Acc:99.99%\n",
      "Epoch [223/250], Step [180/391], Loss: 0.00153, Train_Acc:99.99%\n",
      "Epoch [223/250], Step [240/391], Loss: 0.00204, Train_Acc:99.99%\n",
      "Epoch [223/250], Step [300/391], Loss: 0.00156, Train_Acc:99.99%\n",
      "Epoch [223/250], Step [360/391], Loss: 0.00459, Train_Acc:100.00%\n",
      "Accuary on test images:93.96%\n",
      "Best accuracy: 93.99%\n",
      "Epoch [224/250], Step [60/391], Loss: 0.00136, Train_Acc:99.99%\n",
      "Epoch [224/250], Step [120/391], Loss: 0.00413, Train_Acc:99.99%\n",
      "Epoch [224/250], Step [180/391], Loss: 0.00169, Train_Acc:99.99%\n",
      "Epoch [224/250], Step [240/391], Loss: 0.00201, Train_Acc:99.99%\n",
      "Epoch [224/250], Step [300/391], Loss: 0.00146, Train_Acc:99.98%\n",
      "Epoch [224/250], Step [360/391], Loss: 0.00138, Train_Acc:99.98%\n",
      "Accuary on test images:93.83%\n",
      "Best accuracy: 93.99%\n",
      "Epoch [225/250], Step [60/391], Loss: 0.00166, Train_Acc:99.99%\n",
      "Epoch [225/250], Step [120/391], Loss: 0.00168, Train_Acc:99.97%\n",
      "Epoch [225/250], Step [180/391], Loss: 0.00671, Train_Acc:99.98%\n",
      "Epoch [225/250], Step [240/391], Loss: 0.00140, Train_Acc:99.98%\n",
      "Epoch [225/250], Step [300/391], Loss: 0.00408, Train_Acc:99.99%\n",
      "Epoch [225/250], Step [360/391], Loss: 0.00313, Train_Acc:99.98%\n",
      "Accuary on test images:93.97%\n",
      "Best accuracy: 93.99%\n",
      "Epoch [226/250], Step [60/391], Loss: 0.00391, Train_Acc:100.00%\n",
      "Epoch [226/250], Step [120/391], Loss: 0.00122, Train_Acc:99.99%\n",
      "Epoch [226/250], Step [180/391], Loss: 0.00189, Train_Acc:100.00%\n",
      "Epoch [226/250], Step [240/391], Loss: 0.00141, Train_Acc:100.00%\n",
      "Epoch [226/250], Step [300/391], Loss: 0.00087, Train_Acc:100.00%\n",
      "Epoch [226/250], Step [360/391], Loss: 0.00261, Train_Acc:100.00%\n",
      "Accuary on test images:94.05%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [227/250], Step [60/391], Loss: 0.00096, Train_Acc:100.00%\n",
      "Epoch [227/250], Step [120/391], Loss: 0.00192, Train_Acc:100.00%\n",
      "Epoch [227/250], Step [180/391], Loss: 0.00134, Train_Acc:99.99%\n",
      "Epoch [227/250], Step [240/391], Loss: 0.00141, Train_Acc:99.99%\n",
      "Epoch [227/250], Step [300/391], Loss: 0.00158, Train_Acc:99.99%\n",
      "Epoch [227/250], Step [360/391], Loss: 0.00199, Train_Acc:100.00%\n",
      "Accuary on test images:93.92%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [228/250], Step [60/391], Loss: 0.00099, Train_Acc:100.00%\n",
      "Epoch [228/250], Step [120/391], Loss: 0.00224, Train_Acc:100.00%\n",
      "Epoch [228/250], Step [180/391], Loss: 0.00122, Train_Acc:100.00%\n",
      "Epoch [228/250], Step [240/391], Loss: 0.00166, Train_Acc:100.00%\n",
      "Epoch [228/250], Step [300/391], Loss: 0.00098, Train_Acc:100.00%\n",
      "Epoch [228/250], Step [360/391], Loss: 0.00162, Train_Acc:99.99%\n",
      "Accuary on test images:94.00%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [229/250], Step [60/391], Loss: 0.00304, Train_Acc:99.99%\n",
      "Epoch [229/250], Step [120/391], Loss: 0.00135, Train_Acc:99.99%\n",
      "Epoch [229/250], Step [180/391], Loss: 0.00156, Train_Acc:99.99%\n",
      "Epoch [229/250], Step [240/391], Loss: 0.00155, Train_Acc:99.99%\n",
      "Epoch [229/250], Step [300/391], Loss: 0.00228, Train_Acc:99.99%\n",
      "Epoch [229/250], Step [360/391], Loss: 0.00175, Train_Acc:99.99%\n",
      "Accuary on test images:93.95%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [230/250], Step [60/391], Loss: 0.00118, Train_Acc:100.00%\n",
      "Epoch [230/250], Step [120/391], Loss: 0.00213, Train_Acc:100.00%\n",
      "Epoch [230/250], Step [180/391], Loss: 0.00131, Train_Acc:100.00%\n",
      "Epoch [230/250], Step [240/391], Loss: 0.00204, Train_Acc:100.00%\n",
      "Epoch [230/250], Step [300/391], Loss: 0.00129, Train_Acc:99.99%\n",
      "Epoch [230/250], Step [360/391], Loss: 0.00118, Train_Acc:99.99%\n",
      "Accuary on test images:93.91%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [231/250], Step [60/391], Loss: 0.00103, Train_Acc:100.00%\n",
      "Epoch [231/250], Step [120/391], Loss: 0.00118, Train_Acc:99.99%\n",
      "Epoch [231/250], Step [180/391], Loss: 0.00412, Train_Acc:100.00%\n",
      "Epoch [231/250], Step [240/391], Loss: 0.00175, Train_Acc:100.00%\n",
      "Epoch [231/250], Step [300/391], Loss: 0.00123, Train_Acc:99.99%\n",
      "Epoch [231/250], Step [360/391], Loss: 0.00086, Train_Acc:100.00%\n",
      "Accuary on test images:93.92%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [232/250], Step [60/391], Loss: 0.00197, Train_Acc:99.99%\n",
      "Epoch [232/250], Step [120/391], Loss: 0.00129, Train_Acc:99.99%\n",
      "Epoch [232/250], Step [180/391], Loss: 0.00134, Train_Acc:100.00%\n",
      "Epoch [232/250], Step [240/391], Loss: 0.00245, Train_Acc:100.00%\n",
      "Epoch [232/250], Step [300/391], Loss: 0.00254, Train_Acc:99.99%\n",
      "Epoch [232/250], Step [360/391], Loss: 0.00120, Train_Acc:100.00%\n",
      "Accuary on test images:94.01%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [233/250], Step [60/391], Loss: 0.00147, Train_Acc:99.97%\n",
      "Epoch [233/250], Step [120/391], Loss: 0.00104, Train_Acc:99.99%\n",
      "Epoch [233/250], Step [180/391], Loss: 0.00118, Train_Acc:99.99%\n",
      "Epoch [233/250], Step [240/391], Loss: 0.00105, Train_Acc:99.99%\n",
      "Epoch [233/250], Step [300/391], Loss: 0.00112, Train_Acc:99.99%\n",
      "Epoch [233/250], Step [360/391], Loss: 0.00328, Train_Acc:99.99%\n",
      "Accuary on test images:94.00%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [234/250], Step [60/391], Loss: 0.00110, Train_Acc:99.99%\n",
      "Epoch [234/250], Step [120/391], Loss: 0.00265, Train_Acc:99.99%\n",
      "Epoch [234/250], Step [180/391], Loss: 0.00101, Train_Acc:99.99%\n",
      "Epoch [234/250], Step [240/391], Loss: 0.00115, Train_Acc:99.99%\n",
      "Epoch [234/250], Step [300/391], Loss: 0.00103, Train_Acc:99.99%\n",
      "Epoch [234/250], Step [360/391], Loss: 0.00131, Train_Acc:99.99%\n",
      "Accuary on test images:93.98%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [235/250], Step [60/391], Loss: 0.00102, Train_Acc:100.00%\n",
      "Epoch [235/250], Step [120/391], Loss: 0.00233, Train_Acc:99.99%\n",
      "Epoch [235/250], Step [180/391], Loss: 0.00090, Train_Acc:100.00%\n",
      "Epoch [235/250], Step [240/391], Loss: 0.00196, Train_Acc:100.00%\n",
      "Epoch [235/250], Step [300/391], Loss: 0.00148, Train_Acc:100.00%\n",
      "Epoch [235/250], Step [360/391], Loss: 0.00154, Train_Acc:100.00%\n",
      "Accuary on test images:93.94%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [236/250], Step [60/391], Loss: 0.00159, Train_Acc:100.00%\n",
      "Epoch [236/250], Step [120/391], Loss: 0.00112, Train_Acc:100.00%\n",
      "Epoch [236/250], Step [180/391], Loss: 0.00195, Train_Acc:100.00%\n",
      "Epoch [236/250], Step [240/391], Loss: 0.00111, Train_Acc:100.00%\n",
      "Epoch [236/250], Step [300/391], Loss: 0.00120, Train_Acc:100.00%\n",
      "Epoch [236/250], Step [360/391], Loss: 0.00164, Train_Acc:100.00%\n",
      "Accuary on test images:93.92%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [237/250], Step [60/391], Loss: 0.00160, Train_Acc:100.00%\n",
      "Epoch [237/250], Step [120/391], Loss: 0.00159, Train_Acc:99.99%\n",
      "Epoch [237/250], Step [180/391], Loss: 0.00122, Train_Acc:99.99%\n",
      "Epoch [237/250], Step [240/391], Loss: 0.00116, Train_Acc:99.99%\n",
      "Epoch [237/250], Step [300/391], Loss: 0.00093, Train_Acc:99.99%\n",
      "Epoch [237/250], Step [360/391], Loss: 0.00108, Train_Acc:100.00%\n",
      "Accuary on test images:93.92%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [238/250], Step [60/391], Loss: 0.00142, Train_Acc:99.99%\n",
      "Epoch [238/250], Step [120/391], Loss: 0.00276, Train_Acc:99.99%\n",
      "Epoch [238/250], Step [180/391], Loss: 0.00095, Train_Acc:100.00%\n",
      "Epoch [238/250], Step [240/391], Loss: 0.00217, Train_Acc:100.00%\n",
      "Epoch [238/250], Step [300/391], Loss: 0.00218, Train_Acc:100.00%\n",
      "Epoch [238/250], Step [360/391], Loss: 0.00096, Train_Acc:100.00%\n",
      "Accuary on test images:94.04%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [239/250], Step [60/391], Loss: 0.00104, Train_Acc:100.00%\n",
      "Epoch [239/250], Step [120/391], Loss: 0.00090, Train_Acc:100.00%\n",
      "Epoch [239/250], Step [180/391], Loss: 0.00152, Train_Acc:99.99%\n",
      "Epoch [239/250], Step [240/391], Loss: 0.00146, Train_Acc:99.99%\n",
      "Epoch [239/250], Step [300/391], Loss: 0.00149, Train_Acc:99.99%\n",
      "Epoch [239/250], Step [360/391], Loss: 0.00092, Train_Acc:99.99%\n",
      "Accuary on test images:93.97%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [240/250], Step [60/391], Loss: 0.00100, Train_Acc:100.00%\n",
      "Epoch [240/250], Step [120/391], Loss: 0.00120, Train_Acc:99.98%\n",
      "Epoch [240/250], Step [180/391], Loss: 0.00129, Train_Acc:99.99%\n",
      "Epoch [240/250], Step [240/391], Loss: 0.00134, Train_Acc:99.99%\n",
      "Epoch [240/250], Step [300/391], Loss: 0.00515, Train_Acc:99.99%\n",
      "Epoch [240/250], Step [360/391], Loss: 0.00149, Train_Acc:99.99%\n",
      "Accuary on test images:94.03%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [241/250], Step [60/391], Loss: 0.00137, Train_Acc:100.00%\n",
      "Epoch [241/250], Step [120/391], Loss: 0.00397, Train_Acc:100.00%\n",
      "Epoch [241/250], Step [180/391], Loss: 0.00138, Train_Acc:100.00%\n",
      "Epoch [241/250], Step [240/391], Loss: 0.00168, Train_Acc:100.00%\n",
      "Epoch [241/250], Step [300/391], Loss: 0.00117, Train_Acc:100.00%\n",
      "Epoch [241/250], Step [360/391], Loss: 0.00128, Train_Acc:100.00%\n",
      "Accuary on test images:93.95%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [242/250], Step [60/391], Loss: 0.00336, Train_Acc:100.00%\n",
      "Epoch [242/250], Step [120/391], Loss: 0.00113, Train_Acc:99.99%\n",
      "Epoch [242/250], Step [180/391], Loss: 0.00323, Train_Acc:99.99%\n",
      "Epoch [242/250], Step [240/391], Loss: 0.00121, Train_Acc:99.99%\n",
      "Epoch [242/250], Step [300/391], Loss: 0.00160, Train_Acc:99.99%\n",
      "Epoch [242/250], Step [360/391], Loss: 0.00132, Train_Acc:99.99%\n",
      "Accuary on test images:94.02%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [243/250], Step [60/391], Loss: 0.00114, Train_Acc:100.00%\n",
      "Epoch [243/250], Step [120/391], Loss: 0.00156, Train_Acc:100.00%\n",
      "Epoch [243/250], Step [180/391], Loss: 0.00114, Train_Acc:100.00%\n",
      "Epoch [243/250], Step [240/391], Loss: 0.00125, Train_Acc:100.00%\n",
      "Epoch [243/250], Step [300/391], Loss: 0.00109, Train_Acc:100.00%\n",
      "Epoch [243/250], Step [360/391], Loss: 0.00210, Train_Acc:100.00%\n",
      "Accuary on test images:93.94%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [244/250], Step [60/391], Loss: 0.00175, Train_Acc:100.00%\n",
      "Epoch [244/250], Step [120/391], Loss: 0.00204, Train_Acc:100.00%\n",
      "Epoch [244/250], Step [180/391], Loss: 0.00271, Train_Acc:100.00%\n",
      "Epoch [244/250], Step [240/391], Loss: 0.00147, Train_Acc:100.00%\n",
      "Epoch [244/250], Step [300/391], Loss: 0.00194, Train_Acc:100.00%\n",
      "Epoch [244/250], Step [360/391], Loss: 0.00073, Train_Acc:100.00%\n",
      "Accuary on test images:93.91%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [245/250], Step [60/391], Loss: 0.00201, Train_Acc:100.00%\n",
      "Epoch [245/250], Step [120/391], Loss: 0.00088, Train_Acc:100.00%\n",
      "Epoch [245/250], Step [180/391], Loss: 0.00221, Train_Acc:100.00%\n",
      "Epoch [245/250], Step [240/391], Loss: 0.00110, Train_Acc:100.00%\n",
      "Epoch [245/250], Step [300/391], Loss: 0.00099, Train_Acc:100.00%\n",
      "Epoch [245/250], Step [360/391], Loss: 0.00187, Train_Acc:100.00%\n",
      "Accuary on test images:93.97%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [246/250], Step [60/391], Loss: 0.00111, Train_Acc:100.00%\n",
      "Epoch [246/250], Step [120/391], Loss: 0.00141, Train_Acc:100.00%\n",
      "Epoch [246/250], Step [180/391], Loss: 0.00263, Train_Acc:100.00%\n",
      "Epoch [246/250], Step [240/391], Loss: 0.00162, Train_Acc:100.00%\n",
      "Epoch [246/250], Step [300/391], Loss: 0.00096, Train_Acc:100.00%\n",
      "Epoch [246/250], Step [360/391], Loss: 0.00100, Train_Acc:100.00%\n",
      "Accuary on test images:94.02%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [247/250], Step [60/391], Loss: 0.00342, Train_Acc:100.00%\n",
      "Epoch [247/250], Step [120/391], Loss: 0.00224, Train_Acc:100.00%\n",
      "Epoch [247/250], Step [180/391], Loss: 0.00149, Train_Acc:100.00%\n",
      "Epoch [247/250], Step [240/391], Loss: 0.00172, Train_Acc:100.00%\n",
      "Epoch [247/250], Step [300/391], Loss: 0.00208, Train_Acc:100.00%\n",
      "Epoch [247/250], Step [360/391], Loss: 0.00139, Train_Acc:100.00%\n",
      "Accuary on test images:93.96%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [248/250], Step [60/391], Loss: 0.00189, Train_Acc:100.00%\n",
      "Epoch [248/250], Step [120/391], Loss: 0.00272, Train_Acc:100.00%\n",
      "Epoch [248/250], Step [180/391], Loss: 0.00103, Train_Acc:100.00%\n",
      "Epoch [248/250], Step [240/391], Loss: 0.00145, Train_Acc:100.00%\n",
      "Epoch [248/250], Step [300/391], Loss: 0.00112, Train_Acc:100.00%\n",
      "Epoch [248/250], Step [360/391], Loss: 0.00354, Train_Acc:100.00%\n",
      "Accuary on test images:94.00%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [249/250], Step [60/391], Loss: 0.00155, Train_Acc:100.00%\n",
      "Epoch [249/250], Step [120/391], Loss: 0.00230, Train_Acc:100.00%\n",
      "Epoch [249/250], Step [180/391], Loss: 0.00105, Train_Acc:100.00%\n",
      "Epoch [249/250], Step [240/391], Loss: 0.00108, Train_Acc:100.00%\n",
      "Epoch [249/250], Step [300/391], Loss: 0.00117, Train_Acc:100.00%\n",
      "Epoch [249/250], Step [360/391], Loss: 0.00144, Train_Acc:100.00%\n",
      "Accuary on test images:93.97%\n",
      "Best accuracy: 94.05%\n",
      "Epoch [250/250], Step [60/391], Loss: 0.00137, Train_Acc:100.00%\n",
      "Epoch [250/250], Step [120/391], Loss: 0.00096, Train_Acc:100.00%\n",
      "Epoch [250/250], Step [180/391], Loss: 0.00616, Train_Acc:100.00%\n",
      "Epoch [250/250], Step [240/391], Loss: 0.00147, Train_Acc:100.00%\n",
      "Epoch [250/250], Step [300/391], Loss: 0.00183, Train_Acc:100.00%\n",
      "Epoch [250/250], Step [360/391], Loss: 0.00128, Train_Acc:100.00%\n",
      "Accuary on test images:94.04%\n",
      "Best accuracy: 94.05%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "test_accuracy = torch.Tensor(test_accuracy)\n",
    "mean_difference_all = torch.Tensor(mean_difference_all).cpu()\n",
    "\n",
    "#for digit in range(10):\n",
    "\n",
    "plt.figure(figsize=(4,4)) \n",
    "ax1 = plt.subplot(111)\n",
    "\n",
    "plt.xticks(weight = 'bold')\n",
    "ax1.plot(torch.arange(len(test_accuracy))[:100], test_accuracy[:100] ,color = 'blue',linewidth = 3)\n",
    "plt.yticks(weight = 'bold')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(torch.arange(len(test_accuracy))[:100], mean_difference_all[:100] ,color = 'red',linewidth = 2,linestyle = '--')\n",
    "ax1.set_xlabel(\"Epochs\",fontsize = 12, fontweight = 'bold')\n",
    "ax2.set_ylabel('$\\mu_1-\\mu_2$',fontsize = 12, color = 'red', fontweight = 'bold')\n",
    "ax1.set_ylabel('Test accuracy',fontsize = 12, color = 'blue', fontweight = 'bold')\n",
    "plt.yticks(weight = 'bold')\n",
    "plt.title('ResNet 18 LR-NN: CIFAR10', fontsize = 12, fontweight = 'bold')\n",
    "ax1.grid()\n",
    "#savefig('first100_lrchange_md_resnet18lrnncifar10.pdf')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"332.17125pt\" height=\"283.57pt\" viewBox=\"0 0 332.17125 283.57\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-02-22T21:31:27.889412</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 283.57 \nL 332.17125 283.57 \nL 332.17125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 54.485625 244.078125 \nL 277.685625 244.078125 \nL 277.685625 22.318125 \nL 54.485625 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 64.63108 244.078125 \nL 64.63108 22.318125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m65708b95af\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m65708b95af\" x=\"64.63108\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(61.152173 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-30\" d=\"M 2944 2338 \nQ 2944 3213 2780 3570 \nQ 2616 3928 2228 3928 \nQ 1841 3928 1675 3570 \nQ 1509 3213 1509 2338 \nQ 1509 1453 1675 1090 \nQ 1841 728 2228 728 \nQ 2613 728 2778 1090 \nQ 2944 1453 2944 2338 \nz\nM 4147 2328 \nQ 4147 1169 3647 539 \nQ 3147 -91 2228 -91 \nQ 1306 -91 806 539 \nQ 306 1169 306 2328 \nQ 306 3491 806 4120 \nQ 1306 4750 2228 4750 \nQ 3147 4750 3647 4120 \nQ 4147 3491 4147 2328 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 105.622815 244.078125 \nL 105.622815 22.318125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m65708b95af\" x=\"105.622815\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(98.665003 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-32\" d=\"M 1844 884 \nL 3897 884 \nL 3897 0 \nL 506 0 \nL 506 884 \nL 2209 2388 \nQ 2438 2594 2547 2791 \nQ 2656 2988 2656 3200 \nQ 2656 3528 2436 3728 \nQ 2216 3928 1850 3928 \nQ 1569 3928 1234 3808 \nQ 900 3688 519 3450 \nL 519 4475 \nQ 925 4609 1322 4679 \nQ 1719 4750 2100 4750 \nQ 2938 4750 3402 4381 \nQ 3866 4013 3866 3353 \nQ 3866 2972 3669 2642 \nQ 3472 2313 2841 1759 \nL 1844 884 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-32\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 146.614551 244.078125 \nL 146.614551 22.318125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m65708b95af\" x=\"146.614551\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(139.656738 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-34\" d=\"M 2356 3675 \nL 1038 1722 \nL 2356 1722 \nL 2356 3675 \nz\nM 2156 4666 \nL 3494 4666 \nL 3494 1722 \nL 4159 1722 \nL 4159 850 \nL 3494 850 \nL 3494 0 \nL 2356 0 \nL 2356 850 \nL 288 850 \nL 288 1881 \nL 2156 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-34\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 187.606286 244.078125 \nL 187.606286 22.318125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m65708b95af\" x=\"187.606286\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(180.648474 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-36\" d=\"M 2316 2303 \nQ 2000 2303 1842 2098 \nQ 1684 1894 1684 1484 \nQ 1684 1075 1842 870 \nQ 2000 666 2316 666 \nQ 2634 666 2792 870 \nQ 2950 1075 2950 1484 \nQ 2950 1894 2792 2098 \nQ 2634 2303 2316 2303 \nz\nM 3803 4544 \nL 3803 3681 \nQ 3506 3822 3243 3889 \nQ 2981 3956 2731 3956 \nQ 2194 3956 1894 3657 \nQ 1594 3359 1544 2772 \nQ 1750 2925 1990 3001 \nQ 2231 3078 2516 3078 \nQ 3231 3078 3670 2659 \nQ 4109 2241 4109 1563 \nQ 4109 813 3618 361 \nQ 3128 -91 2303 -91 \nQ 1394 -91 895 523 \nQ 397 1138 397 2266 \nQ 397 3422 980 4083 \nQ 1563 4744 2578 4744 \nQ 2900 4744 3203 4694 \nQ 3506 4644 3803 4544 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-36\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 228.598022 244.078125 \nL 228.598022 22.318125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m65708b95af\" x=\"228.598022\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(221.640209 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-38\" d=\"M 2228 2088 \nQ 1891 2088 1709 1903 \nQ 1528 1719 1528 1375 \nQ 1528 1031 1709 848 \nQ 1891 666 2228 666 \nQ 2563 666 2741 848 \nQ 2919 1031 2919 1375 \nQ 2919 1722 2741 1905 \nQ 2563 2088 2228 2088 \nz\nM 1350 2484 \nQ 925 2613 709 2878 \nQ 494 3144 494 3541 \nQ 494 4131 934 4440 \nQ 1375 4750 2228 4750 \nQ 3075 4750 3515 4442 \nQ 3956 4134 3956 3541 \nQ 3956 3144 3739 2878 \nQ 3522 2613 3097 2484 \nQ 3572 2353 3814 2058 \nQ 4056 1763 4056 1313 \nQ 4056 619 3595 264 \nQ 3134 -91 2228 -91 \nQ 1319 -91 855 264 \nQ 391 619 391 1313 \nQ 391 1763 633 2058 \nQ 875 2353 1350 2484 \nz\nM 1631 3419 \nQ 1631 3141 1786 2991 \nQ 1941 2841 2228 2841 \nQ 2509 2841 2662 2991 \nQ 2816 3141 2816 3419 \nQ 2816 3697 2662 3845 \nQ 2509 3994 2228 3994 \nQ 1941 3994 1786 3844 \nQ 1631 3694 1631 3419 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-38\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 269.589757 244.078125 \nL 269.589757 22.318125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m65708b95af\" x=\"269.589757\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(259.153038 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-31\" d=\"M 750 831 \nL 1813 831 \nL 1813 3847 \nL 722 3622 \nL 722 4441 \nL 1806 4666 \nL 2950 4666 \nL 2950 831 \nL 4013 831 \nL 4013 0 \nL 750 0 \nL 750 831 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-31\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"139.160156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <g transform=\"translate(142.170937 273.874375) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-Bold-45\" d=\"M 588 4666 \nL 3834 4666 \nL 3834 3756 \nL 1791 3756 \nL 1791 2888 \nL 3713 2888 \nL 3713 1978 \nL 1791 1978 \nL 1791 909 \nL 3903 909 \nL 3903 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-70\" d=\"M 1656 506 \nL 1656 -1331 \nL 538 -1331 \nL 538 3500 \nL 1656 3500 \nL 1656 2988 \nQ 1888 3294 2169 3439 \nQ 2450 3584 2816 3584 \nQ 3463 3584 3878 3070 \nQ 4294 2556 4294 1747 \nQ 4294 938 3878 423 \nQ 3463 -91 2816 -91 \nQ 2450 -91 2169 54 \nQ 1888 200 1656 506 \nz\nM 2400 2772 \nQ 2041 2772 1848 2508 \nQ 1656 2244 1656 1747 \nQ 1656 1250 1848 986 \nQ 2041 722 2400 722 \nQ 2759 722 2948 984 \nQ 3138 1247 3138 1747 \nQ 3138 2247 2948 2509 \nQ 2759 2772 2400 2772 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-6f\" d=\"M 2203 2784 \nQ 1831 2784 1636 2517 \nQ 1441 2250 1441 1747 \nQ 1441 1244 1636 976 \nQ 1831 709 2203 709 \nQ 2569 709 2762 976 \nQ 2956 1244 2956 1747 \nQ 2956 2250 2762 2517 \nQ 2569 2784 2203 2784 \nz\nM 2203 3584 \nQ 3106 3584 3614 3096 \nQ 4122 2609 4122 1747 \nQ 4122 884 3614 396 \nQ 3106 -91 2203 -91 \nQ 1297 -91 786 396 \nQ 275 884 275 1747 \nQ 275 2609 786 3096 \nQ 1297 3584 2203 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-63\" d=\"M 3366 3391 \nL 3366 2478 \nQ 3138 2634 2908 2709 \nQ 2678 2784 2431 2784 \nQ 1963 2784 1702 2511 \nQ 1441 2238 1441 1747 \nQ 1441 1256 1702 982 \nQ 1963 709 2431 709 \nQ 2694 709 2930 787 \nQ 3166 866 3366 1019 \nL 3366 103 \nQ 3103 6 2833 -42 \nQ 2563 -91 2291 -91 \nQ 1344 -91 809 395 \nQ 275 881 275 1747 \nQ 275 2613 809 3098 \nQ 1344 3584 2291 3584 \nQ 2566 3584 2833 3536 \nQ 3100 3488 3366 3391 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-68\" d=\"M 4056 2131 \nL 4056 0 \nL 2931 0 \nL 2931 347 \nL 2931 1625 \nQ 2931 2084 2911 2256 \nQ 2891 2428 2841 2509 \nQ 2775 2619 2662 2680 \nQ 2550 2741 2406 2741 \nQ 2056 2741 1856 2470 \nQ 1656 2200 1656 1722 \nL 1656 0 \nL 538 0 \nL 538 4863 \nL 1656 4863 \nL 1656 2988 \nQ 1909 3294 2193 3439 \nQ 2478 3584 2822 3584 \nQ 3428 3584 3742 3212 \nQ 4056 2841 4056 2131 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-73\" d=\"M 3272 3391 \nL 3272 2541 \nQ 2913 2691 2578 2766 \nQ 2244 2841 1947 2841 \nQ 1628 2841 1473 2761 \nQ 1319 2681 1319 2516 \nQ 1319 2381 1436 2309 \nQ 1553 2238 1856 2203 \nL 2053 2175 \nQ 2913 2066 3209 1816 \nQ 3506 1566 3506 1031 \nQ 3506 472 3093 190 \nQ 2681 -91 1863 -91 \nQ 1516 -91 1145 -36 \nQ 775 19 384 128 \nL 384 978 \nQ 719 816 1070 734 \nQ 1422 653 1784 653 \nQ 2113 653 2278 743 \nQ 2444 834 2444 1013 \nQ 2444 1163 2330 1236 \nQ 2216 1309 1875 1350 \nL 1678 1375 \nQ 931 1469 631 1722 \nQ 331 1975 331 2491 \nQ 331 3047 712 3315 \nQ 1094 3584 1881 3584 \nQ 2191 3584 2531 3537 \nQ 2872 3491 3272 3391 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Bold-45\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-70\" x=\"68.310547\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-6f\" x=\"139.892578\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"208.59375\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-68\" x=\"267.871094\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-73\" x=\"339.0625\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path d=\"M 54.485625 220.998084 \nL 277.685625 220.998084 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path id=\"mf0514f7af6\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"220.998084\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.55 -->\n      <g transform=\"translate(22.81375 224.797302) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-2e\" d=\"M 653 1209 \nL 1778 1209 \nL 1778 0 \nL 653 0 \nL 653 1209 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-Bold-35\" d=\"M 678 4666 \nL 3669 4666 \nL 3669 3781 \nL 1638 3781 \nL 1638 3059 \nQ 1775 3097 1914 3117 \nQ 2053 3138 2203 3138 \nQ 3056 3138 3531 2711 \nQ 4006 2284 4006 1522 \nQ 4006 766 3489 337 \nQ 2972 -91 2053 -91 \nQ 1656 -91 1267 -14 \nQ 878 63 494 219 \nL 494 1166 \nQ 875 947 1217 837 \nQ 1559 728 1863 728 \nQ 2300 728 2551 942 \nQ 2803 1156 2803 1522 \nQ 2803 1891 2551 2103 \nQ 2300 2316 1863 2316 \nQ 1603 2316 1309 2248 \nQ 1016 2181 678 2041 \nL 678 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path d=\"M 54.485625 196.18883 \nL 277.685625 196.18883 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"196.18883\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.60 -->\n      <g transform=\"translate(22.81375 199.988049) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-36\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path d=\"M 54.485625 171.379576 \nL 277.685625 171.379576 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"171.379576\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.65 -->\n      <g transform=\"translate(22.81375 175.178795) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-36\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path d=\"M 54.485625 146.570322 \nL 277.685625 146.570322 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"146.570322\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.70 -->\n      <g transform=\"translate(22.81375 150.369541) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-37\" d=\"M 428 4666 \nL 3944 4666 \nL 3944 3988 \nL 2125 0 \nL 953 0 \nL 2675 3781 \nL 428 3781 \nL 428 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-37\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path d=\"M 54.485625 121.761069 \nL 277.685625 121.761069 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"121.761069\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.75 -->\n      <g transform=\"translate(22.81375 125.560288) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-37\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path d=\"M 54.485625 96.951815 \nL 277.685625 96.951815 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"96.951815\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.80 -->\n      <g transform=\"translate(22.81375 100.751034) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-38\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <path d=\"M 54.485625 72.142561 \nL 277.685625 72.142561 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"72.142561\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.85 -->\n      <g transform=\"translate(22.81375 75.94178) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-38\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_27\">\n      <path d=\"M 54.485625 47.333308 \nL 277.685625 47.333308 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"47.333308\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.90 -->\n      <g transform=\"translate(22.81375 51.132526) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-39\" d=\"M 641 103 \nL 641 966 \nQ 928 831 1190 764 \nQ 1453 697 1709 697 \nQ 2247 697 2547 995 \nQ 2847 1294 2900 1881 \nQ 2688 1725 2447 1647 \nQ 2206 1569 1925 1569 \nQ 1209 1569 770 1986 \nQ 331 2403 331 3084 \nQ 331 3838 820 4291 \nQ 1309 4744 2131 4744 \nQ 3044 4744 3544 4128 \nQ 4044 3513 4044 2388 \nQ 4044 1231 3459 570 \nQ 2875 -91 1856 -91 \nQ 1528 -91 1228 -42 \nQ 928 6 641 103 \nz\nM 2125 2350 \nQ 2441 2350 2600 2554 \nQ 2759 2759 2759 3169 \nQ 2759 3575 2600 3781 \nQ 2441 3988 2125 3988 \nQ 1809 3988 1650 3781 \nQ 1491 3575 1491 3169 \nQ 1491 2759 1650 2554 \nQ 1809 2350 2125 2350 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-39\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_29\">\n      <path d=\"M 54.485625 22.524054 \nL 277.685625 22.524054 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#mf0514f7af6\" x=\"54.485625\" y=\"22.524054\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.95 -->\n      <g transform=\"translate(22.81375 26.323273) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-39\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Test accuracy -->\n     <g style=\"fill: #0000ff\" transform=\"translate(16.224375 179.003438) rotate(-90) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-Bold-54\" d=\"M 31 4666 \nL 4331 4666 \nL 4331 3756 \nL 2784 3756 \nL 2784 0 \nL 1581 0 \nL 1581 3756 \nL 31 3756 \nL 31 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-65\" d=\"M 4031 1759 \nL 4031 1441 \nL 1416 1441 \nQ 1456 1047 1700 850 \nQ 1944 653 2381 653 \nQ 2734 653 3104 758 \nQ 3475 863 3866 1075 \nL 3866 213 \nQ 3469 63 3072 -14 \nQ 2675 -91 2278 -91 \nQ 1328 -91 801 392 \nQ 275 875 275 1747 \nQ 275 2603 792 3093 \nQ 1309 3584 2216 3584 \nQ 3041 3584 3536 3087 \nQ 4031 2591 4031 1759 \nz\nM 2881 2131 \nQ 2881 2450 2695 2645 \nQ 2509 2841 2209 2841 \nQ 1884 2841 1681 2658 \nQ 1478 2475 1428 2131 \nL 2881 2131 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-74\" d=\"M 1759 4494 \nL 1759 3500 \nL 2913 3500 \nL 2913 2700 \nL 1759 2700 \nL 1759 1216 \nQ 1759 972 1856 886 \nQ 1953 800 2241 800 \nL 2816 800 \nL 2816 0 \nL 1856 0 \nQ 1194 0 917 276 \nQ 641 553 641 1216 \nL 641 2700 \nL 84 2700 \nL 84 3500 \nL 641 3500 \nL 641 4494 \nL 1759 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-61\" d=\"M 2106 1575 \nQ 1756 1575 1579 1456 \nQ 1403 1338 1403 1106 \nQ 1403 894 1545 773 \nQ 1688 653 1941 653 \nQ 2256 653 2472 879 \nQ 2688 1106 2688 1447 \nL 2688 1575 \nL 2106 1575 \nz\nM 3816 1997 \nL 3816 0 \nL 2688 0 \nL 2688 519 \nQ 2463 200 2181 54 \nQ 1900 -91 1497 -91 \nQ 953 -91 614 226 \nQ 275 544 275 1050 \nQ 275 1666 698 1953 \nQ 1122 2241 2028 2241 \nL 2688 2241 \nL 2688 2328 \nQ 2688 2594 2478 2717 \nQ 2269 2841 1825 2841 \nQ 1466 2841 1156 2769 \nQ 847 2697 581 2553 \nL 581 3406 \nQ 941 3494 1303 3539 \nQ 1666 3584 2028 3584 \nQ 2975 3584 3395 3211 \nQ 3816 2838 3816 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-75\" d=\"M 500 1363 \nL 500 3500 \nL 1625 3500 \nL 1625 3150 \nQ 1625 2866 1622 2436 \nQ 1619 2006 1619 1863 \nQ 1619 1441 1641 1255 \nQ 1663 1069 1716 984 \nQ 1784 875 1895 815 \nQ 2006 756 2150 756 \nQ 2500 756 2700 1025 \nQ 2900 1294 2900 1772 \nL 2900 3500 \nL 4019 3500 \nL 4019 0 \nL 2900 0 \nL 2900 506 \nQ 2647 200 2364 54 \nQ 2081 -91 1741 -91 \nQ 1134 -91 817 281 \nQ 500 653 500 1363 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-72\" d=\"M 3138 2547 \nQ 2991 2616 2845 2648 \nQ 2700 2681 2553 2681 \nQ 2122 2681 1889 2404 \nQ 1656 2128 1656 1613 \nL 1656 0 \nL 538 0 \nL 538 3500 \nL 1656 3500 \nL 1656 2925 \nQ 1872 3269 2151 3426 \nQ 2431 3584 2822 3584 \nQ 2878 3584 2943 3579 \nQ 3009 3575 3134 3559 \nL 3138 2547 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-79\" d=\"M 78 3500 \nL 1197 3500 \nL 2138 1125 \nL 2938 3500 \nL 4056 3500 \nL 2584 -331 \nQ 2363 -916 2067 -1148 \nQ 1772 -1381 1288 -1381 \nL 641 -1381 \nL 641 -647 \nL 991 -647 \nQ 1275 -647 1404 -556 \nQ 1534 -466 1606 -231 \nL 1638 -134 \nL 78 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Bold-54\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"54.962891\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-73\" x=\"122.785156\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-74\" x=\"182.306641\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"230.109375\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"264.923828\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"332.404297\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"391.681641\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-75\" x=\"450.958984\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-72\" x=\"522.150391\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"571.466797\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"638.947266\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-79\" x=\"698.224609\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 64.63108 233.998125 \nL 66.680666 177.333791 \nL 68.730253 140.814573 \nL 70.77984 136.894711 \nL 72.829427 127.665675 \nL 74.879013 112.928988 \nL 76.9286 94.619737 \nL 78.978187 106.329725 \nL 81.027774 86.432685 \nL 83.077361 82.314346 \nL 85.126947 78.344887 \nL 87.176534 77.451753 \nL 89.226121 77.749454 \nL 91.275708 71.646371 \nL 93.325294 75.963187 \nL 95.374881 75.467009 \nL 97.424468 73.035683 \nL 99.474055 72.986086 \nL 101.523642 71.646371 \nL 103.573228 78.047156 \nL 105.622815 62.466962 \nL 107.672402 61.673052 \nL 109.721989 70.058581 \nL 111.771575 59.986039 \nL 113.821162 54.527984 \nL 115.870749 56.909683 \nL 117.920336 56.314251 \nL 119.969923 63.9059 \nL 122.019509 60.134889 \nL 124.069096 53.535627 \nL 126.118683 60.581441 \nL 128.16827 53.138672 \nL 130.217856 55.966923 \nL 132.267443 56.01655 \nL 134.31703 57.108161 \nL 136.366617 57.207385 \nL 138.416204 54.577611 \nL 140.46579 54.27991 \nL 142.515377 47.581394 \nL 144.564964 50.95545 \nL 146.614551 54.875312 \nL 148.664137 52.047061 \nL 150.713724 48.226453 \nL 152.763311 50.062317 \nL 154.812898 51.302778 \nL 156.862485 50.8066 \nL 158.912071 47.333319 \nL 160.961658 63.161618 \nL 163.011245 46.936365 \nL 165.060832 49.21881 \nL 167.110418 50.95545 \nL 169.160005 44.653919 \nL 171.209592 48.722632 \nL 173.259179 47.48217 \nL 175.308765 48.474527 \nL 177.358352 49.714989 \nL 179.407939 51.699733 \nL 181.457526 46.489783 \nL 183.507113 48.27605 \nL 185.556699 47.581394 \nL 187.606286 43.562308 \nL 189.655873 44.554665 \nL 191.70546 49.665392 \nL 193.755046 44.058487 \nL 195.804633 45.100471 \nL 197.85422 44.405815 \nL 199.903807 48.176826 \nL 201.953394 44.604292 \nL 204.00298 45.993604 \nL 206.052567 46.440186 \nL 208.102154 35.127152 \nL 210.151741 35.027928 \nL 212.201327 35.027928 \nL 214.250914 34.085168 \nL 216.300501 34.779824 \nL 218.350088 34.134795 \nL 220.399675 34.53172 \nL 222.449261 34.581347 \nL 224.498848 34.333272 \nL 226.548435 34.482123 \nL 228.598022 33.886691 \nL 230.647608 33.539363 \nL 232.697195 33.588989 \nL 234.746782 33.340885 \nL 236.796369 33.837064 \nL 238.845956 34.6806 \nL 240.895542 33.936317 \nL 242.945129 33.588989 \nL 244.994716 33.837064 \nL 247.044303 32.993557 \nL 249.093889 33.043184 \nL 251.143476 34.382869 \nL 253.193063 33.340885 \nL 255.24265 33.192035 \nL 257.292237 33.440139 \nL 259.341823 32.547005 \nL 261.39141 33.043184 \nL 263.440997 33.291258 \nL 265.490584 33.291258 \nL 267.54017 32.398125 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke: #0000ff; stroke-width: 3; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 54.485625 244.078125 \nL 54.485625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 277.685625 244.078125 \nL 277.685625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 54.485625 244.078125 \nL 277.685625 244.078125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 54.485625 22.318125 \nL 277.685625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"ytick_10\">\n     <g id=\"line2d_32\">\n      <defs>\n       <path id=\"mbe6da07863\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"236.029219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.40 -->\n      <g transform=\"translate(284.685625 239.828438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-34\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_33\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"209.804772\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0.45 -->\n      <g transform=\"translate(284.685625 213.603991) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-34\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_34\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"183.580325\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 0.50 -->\n      <g transform=\"translate(284.685625 187.379544) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_35\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"157.355878\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 0.55 -->\n      <g transform=\"translate(284.685625 161.155097) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_36\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"131.131432\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0.60 -->\n      <g transform=\"translate(284.685625 134.93065) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-36\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_37\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"104.906985\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0.65 -->\n      <g transform=\"translate(284.685625 108.706204) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-36\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_38\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"78.682538\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.70 -->\n      <g transform=\"translate(284.685625 82.481757) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-37\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_39\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"52.458091\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.75 -->\n      <g transform=\"translate(284.685625 56.25731) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-37\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_40\">\n      <g>\n       <use xlink:href=\"#mbe6da07863\" x=\"277.685625\" y=\"26.233645\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 0.80 -->\n      <g transform=\"translate(284.685625 30.032863) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-38\" x=\"107.568359\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"177.148438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_27\">\n     <!-- $\\mu_1-\\mu_2$ -->\n     <g style=\"fill: #ff0000\" transform=\"translate(322.45125 153.898125) rotate(-90) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-Oblique-3bc\" d=\"M -84 -1331 \nL 856 3500 \nL 1434 3500 \nL 1009 1322 \nQ 997 1256 987 1175 \nQ 978 1094 978 1013 \nQ 978 722 1161 565 \nQ 1344 409 1684 409 \nQ 2147 409 2431 671 \nQ 2716 934 2816 1459 \nL 3213 3500 \nL 3788 3500 \nL 3266 809 \nQ 3253 750 3248 706 \nQ 3244 663 3244 628 \nQ 3244 531 3283 486 \nQ 3322 441 3406 441 \nQ 3438 441 3492 456 \nQ 3547 472 3647 513 \nL 3559 50 \nQ 3422 -19 3297 -55 \nQ 3172 -91 3053 -91 \nQ 2847 -91 2730 40 \nQ 2613 172 2613 403 \nQ 2438 153 2195 31 \nQ 1953 -91 1625 -91 \nQ 1334 -91 1117 43 \nQ 900 178 831 397 \nL 494 -1331 \nL -84 -1331 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(0 0.3125)\"/>\n      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(63.623047 -16.09375) scale(0.7)\"/>\n      <use xlink:href=\"#DejaVuSans-2212\" transform=\"translate(130.375977 0.3125)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(233.647461 0.3125)\"/>\n      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(297.270508 -16.09375) scale(0.7)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 64.63108 233.998125 \nL 66.680666 179.692023 \nL 68.730253 159.673229 \nL 70.77984 152.039867 \nL 72.829427 133.928491 \nL 74.879013 128.551556 \nL 76.9286 122.376346 \nL 78.978187 127.915562 \nL 81.027774 106.795596 \nL 83.077361 94.222873 \nL 85.126947 85.195477 \nL 87.176534 87.389067 \nL 89.226121 89.792426 \nL 91.275708 85.374827 \nL 93.325294 88.780788 \nL 95.374881 81.348284 \nL 97.424468 78.002002 \nL 99.474055 79.709907 \nL 101.523642 79.062534 \nL 103.573228 81.289262 \nL 105.622815 64.577641 \nL 107.672402 72.309103 \nL 109.721989 62.795771 \nL 111.771575 61.005835 \nL 113.821162 59.248161 \nL 115.870749 59.253163 \nL 117.920336 58.993095 \nL 119.969923 59.831604 \nL 122.019509 54.316616 \nL 124.069096 51.920886 \nL 126.118683 51.170192 \nL 128.16827 52.556348 \nL 130.217856 50.611478 \nL 132.267443 44.695242 \nL 134.31703 49.575331 \nL 136.366617 46.451509 \nL 138.416204 53.159047 \nL 140.46579 46.497151 \nL 142.515377 46.971802 \nL 144.564964 44.573508 \nL 146.614551 48.587827 \nL 148.664137 54.567056 \nL 150.713724 48.378091 \nL 152.763311 48.453307 \nL 154.812898 42.465169 \nL 156.862485 49.880917 \nL 158.912071 44.848395 \nL 160.961658 60.417266 \nL 163.011245 44.668701 \nL 165.060832 48.048996 \nL 167.110418 43.737688 \nL 169.160005 35.998973 \nL 171.209592 43.099694 \nL 173.259179 38.25465 \nL 175.308765 37.609309 \nL 177.358352 49.377036 \nL 179.407939 52.592518 \nL 181.457526 41.76784 \nL 183.507113 41.664675 \nL 185.556699 43.159435 \nL 187.606286 41.291063 \nL 189.655873 49.298912 \nL 191.70546 48.714501 \nL 193.755046 43.189228 \nL 195.804633 43.49275 \nL 197.85422 40.907291 \nL 199.903807 45.366718 \nL 201.953394 45.986612 \nL 204.00298 43.536423 \nL 206.052567 44.982227 \nL 208.102154 35.618609 \nL 210.151741 35.7923 \nL 212.201327 34.729299 \nL 214.250914 33.997019 \nL 216.300501 33.446777 \nL 218.350088 33.36784 \nL 220.399675 33.028304 \nL 222.449261 34.260213 \nL 224.498848 33.01186 \nL 226.548435 33.560133 \nL 228.598022 32.792339 \nL 230.647608 33.689026 \nL 232.697195 34.827743 \nL 234.746782 32.398125 \nL 236.796369 33.207779 \nL 238.845956 34.292194 \nL 240.895542 34.504369 \nL 242.945129 33.903295 \nL 244.994716 34.209944 \nL 247.044303 34.019809 \nL 249.093889 33.699405 \nL 251.143476 32.915667 \nL 253.193063 34.774317 \nL 255.24265 33.726259 \nL 257.292237 34.468168 \nL 259.341823 33.087577 \nL 261.39141 34.363503 \nL 263.440997 33.579984 \nL 265.490584 32.737974 \nL 267.54017 33.953565 \n\" clip-path=\"url(#p523deac72e)\" style=\"fill: none; stroke-dasharray: 7.4,3.2; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 2\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 54.485625 244.078125 \nL 54.485625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 277.685625 244.078125 \nL 277.685625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 54.485625 244.078125 \nL 277.685625 244.078125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 54.485625 22.318125 \nL 277.685625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_28\">\n    <!-- ResNet 18 LR-NN: CIFAR10 -->\n    <g transform=\"translate(76.21875 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-Bold-52\" d=\"M 2297 2597 \nQ 2675 2597 2839 2737 \nQ 3003 2878 3003 3200 \nQ 3003 3519 2839 3656 \nQ 2675 3794 2297 3794 \nL 1791 3794 \nL 1791 2597 \nL 2297 2597 \nz\nM 1791 1766 \nL 1791 0 \nL 588 0 \nL 588 4666 \nL 2425 4666 \nQ 3347 4666 3776 4356 \nQ 4206 4047 4206 3378 \nQ 4206 2916 3982 2619 \nQ 3759 2322 3309 2181 \nQ 3556 2125 3751 1926 \nQ 3947 1728 4147 1325 \nL 4800 0 \nL 3519 0 \nL 2950 1159 \nQ 2778 1509 2601 1637 \nQ 2425 1766 2131 1766 \nL 1791 1766 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-4e\" d=\"M 588 4666 \nL 1931 4666 \nL 3628 1466 \nL 3628 4666 \nL 4769 4666 \nL 4769 0 \nL 3425 0 \nL 1728 3200 \nL 1728 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-4c\" d=\"M 588 4666 \nL 1791 4666 \nL 1791 909 \nL 3903 909 \nL 3903 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-2d\" d=\"M 347 2297 \nL 2309 2297 \nL 2309 1388 \nL 347 1388 \nL 347 2297 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-3a\" d=\"M 716 3500 \nL 1844 3500 \nL 1844 2291 \nL 716 2291 \nL 716 3500 \nz\nM 716 1209 \nL 1844 1209 \nL 1844 0 \nL 716 0 \nL 716 1209 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-43\" d=\"M 4288 256 \nQ 3956 84 3597 -3 \nQ 3238 -91 2847 -91 \nQ 1681 -91 1000 561 \nQ 319 1213 319 2328 \nQ 319 3447 1000 4098 \nQ 1681 4750 2847 4750 \nQ 3238 4750 3597 4662 \nQ 3956 4575 4288 4403 \nL 4288 3438 \nQ 3953 3666 3628 3772 \nQ 3303 3878 2944 3878 \nQ 2300 3878 1931 3465 \nQ 1563 3053 1563 2328 \nQ 1563 1606 1931 1193 \nQ 2300 781 2944 781 \nQ 3303 781 3628 887 \nQ 3953 994 4288 1222 \nL 4288 256 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-49\" d=\"M 588 4666 \nL 1791 4666 \nL 1791 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-46\" d=\"M 588 4666 \nL 3834 4666 \nL 3834 3756 \nL 1791 3756 \nL 1791 2888 \nL 3713 2888 \nL 3713 1978 \nL 1791 1978 \nL 1791 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-41\" d=\"M 3419 850 \nL 1538 850 \nL 1241 0 \nL 31 0 \nL 1759 4666 \nL 3194 4666 \nL 4922 0 \nL 3713 0 \nL 3419 850 \nz\nM 1838 1716 \nL 3116 1716 \nL 2478 3572 \nL 1838 1716 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-Bold-52\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"77.001953\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-73\" x=\"144.824219\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4e\" x=\"204.345703\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"288.037109\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-74\" x=\"355.859375\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"403.662109\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-31\" x=\"438.476562\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-38\" x=\"508.056641\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"577.636719\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4c\" x=\"612.451172\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-52\" x=\"676.171875\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-2d\" x=\"753.173828\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4e\" x=\"794.677734\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4e\" x=\"878.369141\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-3a\" x=\"962.060547\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"1002.050781\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-43\" x=\"1036.865234\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-49\" x=\"1110.253906\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-46\" x=\"1147.460938\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-41\" x=\"1204.271484\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-52\" x=\"1281.664062\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-31\" x=\"1358.666016\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"1428.246094\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p523deac72e\">\n   <rect x=\"54.485625\" y=\"22.318125\" width=\"223.2\" height=\"221.76\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    digit = 9\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input.cuda(), target.long().cuda()\n",
    "    eigenvalue = torch.zeros(512).cuda()\n",
    "    feature_map = model.module.feature_map(input)\n",
    "    feature_map_digit = feature_map[target==digit,:].reshape(sum(target==digit),512)\n",
    "    u,s,v = torch.svd(feature_map_digit-torch.mean(feature_map, dim=0))\n",
    "    eigenvalue = s \n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.bar(torch.arange(30),(eigenvalue[:30]/max(eigenvalue)).cpu(), color = 'orange')\n",
    "    plt.title('CLass {}'.format(digit), fontsize=12)\n",
    "    plt.xlabel('Index of eigens', fontsize=15)\n",
    "    plt.ylabel('Eigenvalue $\\lambda/\\lambda_{\\max}$',fontsize=15)\n",
    "    plt.ylim([0,1.1])\n",
    "    savefig('feature_map_correlation_{}.pdf'.format(digit))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import model.alexnet as AlexNet\n",
    "import model.resnet as ResNet\n",
    "import model.vgg as Vgg\n",
    "import model.lenet as LeNet\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def savefig(name):\n",
    "    plt.savefig(name,dpi=600, bbox_inches='tight')\n",
    "    return\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.benchmark = True #for accelerating the running\n",
    "    return\n",
    "\n",
    "#setup_seed(1)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Cifar10 Training')\n",
    "parser.add_argument('--gpu-id', default=[0], nargs='+', type=int, help='available GPU IDs')\n",
    "parser.add_argument('--epochs', default=250, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int, metavar='N', help='mini-batch size (default: 128),only used for train')\n",
    "parser.add_argument('-w', '--workers', default=0, type=int, metavar='N', help='num_workers, at most 16, must be 0 on windows')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.002, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-3, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int, metavar='N', help='print frequency (default: 10)')\n",
    "#parser.add_argument('--resume', default='checkpoint/Alexnet/checkpoint.pth.tar', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-t', '--train', dest='train', action='store_true', help='test model on test set')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "Path_Name = 'Alexnet'\n",
    "checkpoint_path = 'checkpoint/' + Path_Name\n",
    "summary_path = 'summary/' + Path_Name\n",
    "test_accuracy = []\n",
    "if args.train:\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(summary_path)\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, transform=train_transform, download=True)\n",
    "\n",
    "valid_dataset = torchvision.datasets.CIFAR10(root='data', train=False, transform=test_transform)\n",
    "\n",
    "# valid_dataset, test_dataset = torch.utils.data.random_split(test_dataset, (int(0.5*len(test_dataset)), int(0.5*len(test_dataset))))\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "\n",
    "# model = LinearNeuralNet(input_size, 5, num_classes).to(device)\n",
    "#model = AlexNet.AlexNet_0()\n",
    "model = ResNet.ResNet18()\n",
    "model = nn.DataParallel(model, device_ids=args.gpu_id).cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "model.load_state_dict(torch.load('resnet18cifar.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input.cuda(), target.cuda().long()\n",
    "        output = model(input)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        valid_total = output.shape[0]\n",
    "        valid_correct = (predicted == target).sum().item()\n",
    "        \n",
    "        prec = valid_correct / valid_total\n",
    "        test_accuracy.append(prec)\n",
    "        print('Accuary on test images:{:.2f}%'.format(prec*100))\n",
    "        feature_map = model.module.feature_map(input)\n",
    "        torch.save(feature_map,'cifar10_feature')\n",
    "        torch.save(target,'target')\n",
    "\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adversarial example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def linear(input):\n",
    "    weight = model.state_dict()['module.linear.weight']\n",
    "    output = torch.mm(input, weight.T)\n",
    "    return output\n",
    "\n",
    "def low_rank(input):\n",
    "    eigen = model.state_dict()['module.linear.eigen']\n",
    "    output = torch.zeros(input.shape[0],10).cuda()\n",
    "    for i in range(eigen.shape[2]):\n",
    "        output += (torch.mm(input, eigen[:,:,i].T))**2\n",
    "    return output\n",
    "\n",
    "def quadratic(input):\n",
    "    weight_a = model.state_dict()['module.linear.weight_a']\n",
    "    output = F.bilinear(input, input, weight_a)\n",
    "    return output\n",
    "\n",
    "def generate_adversial_exm_grad(varepsilon):\n",
    "    adversial_exm = torch.zeros(10000,512)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input.cuda(), target.cuda().long()\n",
    "\n",
    "        feature_map = model.module.feature_map(input).data\n",
    "\n",
    "        feature_map.requires_grad = True\n",
    "        #output = linear(feature_map)\n",
    "        output = quadratic(feature_map)\n",
    "        #output = low_rank(feature_map)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        adversial_exm[i:i+1,:] = feature_map + varepsilon*feature_map.grad/torch.norm(feature_map.grad)\n",
    "    return adversial_exm.data\n",
    "\n",
    "\n",
    "def generate_adversial_exm_sign(varepsilon):\n",
    "    adversial_exm = torch.zeros(10000,512)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input.cuda(), target.cuda().long()\n",
    "\n",
    "        feature_map = model.module.feature_map(input).data\n",
    "\n",
    "        feature_map.requires_grad = True\n",
    "        #output = linear(feature_map)\n",
    "        output = quadratic(feature_map)\n",
    "        #output = low_rank(feature_map)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "    \n",
    "        adversial_exm[i:i+1,:] = feature_map + varepsilon*torch.sign(feature_map.grad)\n",
    "        \n",
    "    return adversial_exm.data\n",
    "\n",
    "test_accuracy = torch.zeros(2,10)\n",
    "\n",
    "for j in range(10):\n",
    "    adversial_input = generate_adversial_exm_grad(j*0.5)\n",
    "    adversial_input_1 = generate_adversial_exm_sign(j*0.05)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input.cuda(), target.cuda().long()\n",
    "\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        #output = linear(adversial_input.cuda())\n",
    "        output = quadratic(adversial_input.cuda())\n",
    "        #output = low_rank(adversial_input.cuda())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        valid_total = target.size(0)\n",
    "        valid_correct = (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    prec = valid_correct / valid_total\n",
    "    print('Accuary on test images:{:.2f}%, epsilon:{}'.format(prec*100, j*0.5))\n",
    "\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        #output = linear(adversial_input_1.cuda())\n",
    "        output = quadratic(adversial_input_1.cuda())\n",
    "        #output = low_rank(adversial_input_1.cuda())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        valid_total = target.size(0)\n",
    "        valid_correct = (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "    prec_1 = valid_correct / valid_total\n",
    "    print('Accuary on test images:{:.2f}%, epsilon:{}'.format(prec_1*100, j*0.05))\n",
    "    test_accuracy[0,j] = prec # gradient ad\n",
    "    test_accuracy[1,j] = prec_1 #sign ad"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'module.linear.weight'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34802/989433777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mvalid_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversial_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34802/989433777.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'module.linear.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'module.linear.weight'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KS distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_accuracy = torch.Tensor(test_accuracy)\n",
    "KS_dis = torch.Tensor(KS_dis)\n",
    "\n",
    "for digit in range(10):\n",
    "    fig = plt.figure() \n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    ax1.plot(torch.arange(250), test_accuracy[torch.arange(2500)%10==digit] ,color = 'blue',linewidth = 5)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(torch.arange(250), KS_dis[torch.arange(2500)%10==digit] ,color = 'red', linewidth = 3, linestyle = '--')\n",
    "    ax1.set_xlabel(\"Epoch\",fontsize = 20)\n",
    "    ax2.set_ylabel('KS distance',fontsize = 20, color = 'red')\n",
    "    ax1.set_ylabel('Test Accuracy',fontsize = 20, color = 'blue')\n",
    "    plt.title('Class {}'.format(digit), fontsize = 20)\n",
    "    savefig('linear_KS_cifar10_two_norm_{}.svg'.format(digit))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.9 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "4dc8b9ca2dd743165c9a23c64ed5faa802a7566e9a05ea7fbc4e7eac82525774"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}