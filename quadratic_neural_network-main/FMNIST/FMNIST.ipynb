{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import model.simplenet as simplenet\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def change_weight(weight):\n",
    "    '''\n",
    "    Change the weight matrix in quadratic neural network.\n",
    "    '''\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict['classifier.0.weight_a'] = weight\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    return\n",
    "\n",
    "def matshow(a):\n",
    "    m = torch.max(abs(a))\n",
    "    plt.figure()    \n",
    "    plt.matshow(a.reshape(28,28),cmap=plt.cm.gray,vmin = -m,vmax = m)\n",
    "    plt.colorbar()\n",
    "    return\n",
    "\n",
    "def savefig(name):\n",
    "    plt.savefig(name,dpi=600, bbox_inches='tight')\n",
    "    return\n",
    "\n",
    "def change_eigen(eigen):\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict['classifier.0.eigen'] = eigen\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    return\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.benchmark = True #for accelerating the running\n",
    "    return\n",
    "\n",
    "\n",
    "def intermediate_output(x, k):\n",
    "    for i in range(k):\n",
    "        x = model[i](x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def intermediate_output(x, k, feature_layer=2):\n",
    "    with torch.no_grad():\n",
    "        for i in range(k):\n",
    "            if i < feature_layer:\n",
    "                x = model.features[i](x)\n",
    "            else:\n",
    "                x = x.view(x.size(0), -1)\n",
    "                x = model.classifier[i-feature_layer](x)\n",
    "    return x\n",
    "\n",
    "setup_seed(2)\n",
    "\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "valid_dataset = torchvision.datasets.FashionMNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#FMNIST dataset\n",
    "#train_dataset = torchvision.datasets.FashionMNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "#valid_dataset = torchvision.datasets.FashionMNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "all_train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=60000)\n",
    "#EI_distribution = torch.bernoulli(torch.ones(784)*0.75)\n",
    "#kappa_matrix = -torch.ones(784, 784)\n",
    "#kappa_matrix[EI_distribution==0,:] = 1\n",
    "#kappa_matrix[:,EI_distribution==0] = 1\n",
    "\n",
    "model = simplenet.SimpleNet_2(num_eigens=1)\n",
    "#model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
    "#model = nn.DataParallel(model, device_ids=[0])\n",
    "print(model)\n",
    "#initial_weight_a = model.state_dict()['classifier.0.weight_a']\n",
    "#change_weight(torch.zeros(10,784,784))\n",
    "#torch.save(initial_weight_a,'initial_weight_a')\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "all_loss = []\n",
    "all_train_loss = []\n",
    "best_prec = 0\n",
    "min_loss = 1\n",
    "image_list = []\n",
    "KS_dis = []\n",
    "number = 0\n",
    "difference = []\n",
    "mean = []\n",
    "#model_dict = model.state_dict()\n",
    "#model_dict['module.features.0.weight'] = torch.load('convolution_kernel')\n",
    "#model.load_state_dict(model_dict)\n",
    "#out = torch.zeros(240,10,10)\n",
    "#inner_product = torch.zeros(240,10,10)\n",
    "#s = torch.zeros(240,10,10)\n",
    "#phi = torch.zeros(240,10,784)\n",
    "for epoch in range(0, 2):\n",
    "    lr = 0.01\n",
    "    #else:\n",
    "     #   lr = 0.0001\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # train for one epoch\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        input, target = input, target.long()\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "       \n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        ave_loss = train_loss/(i+1)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total = target.size(0)\n",
    "        train_correct = (predicted == target).sum().item()\n",
    "\n",
    "        prec = train_correct / train_total\n",
    "\n",
    "        #eigen = model.state_dict()['classifier.0.eigen']\n",
    "        \n",
    "        #for j in range(10):\n",
    "         #   new_eigen[j,:,:] = eigen[j,:,:]/torch.norm(eigen[j,:,:],2)\n",
    "        #change_eigen(new_eigen)\n",
    "\n",
    "        #model_dict = model.state_dict()\n",
    "        #model_dict['module.features.0.weight'] = torch.load('convolution_kernel')\n",
    "        #model.load_state_dict(model_dict)\n",
    "\n",
    "        #weight_a = model.state_dict()['classifier.0.weight_a']\n",
    "        #for j in range(10):\n",
    "         #   new_weight[j,:,:] = weight_a[j,:,:]/torch.norm(weight_a[j,:,:],'fro')\n",
    "        #change_weight(new_weight)\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            train_accuracy.append(prec)\n",
    "            all_train_loss.append(loss)\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.5f}, Train_Acc:{:.2f}%'.format(epoch+1, 20, i, len(train_loader), ave_loss, prec*100))\n",
    "            \n",
    "            \n",
    "    # evaluate on test set\n",
    "    # switch to evaluate mode\n",
    "            #convolution_kernel = model.state_dict()['module.features.0.weight']\n",
    "            model.eval()\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                for j, (input, target) in enumerate(valid_loader):\n",
    "                    input, target = input, target.long()\n",
    "                    # compute output\n",
    "                    output = model(input)\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    #for digit in range(10):\n",
    "                     #   for index in range(output.shape[0]):\n",
    "                      #      if target[index] == digit:\n",
    "                       #         out[number,:,digit] += output[index,:]#/((torch.norm(eigen[digit,:,:],2)*torch.norm(output_first_layer_after[index,:],2))**2)\n",
    "                        #        inner_product[number,:,digit] += torch.mm(eigen[:,:,0],output_first_layer_after[index,:].reshape(784,1)).reshape(10)\n",
    "                         #       s[number,:,digit] += F.softmax(output[index,:],dim=0)\n",
    "                        #out[number,:,digit] = out[number,:,digit]/1000\n",
    "                        #inner_product[number,:,digit] = inner_product[number,:,digit]/1000\n",
    "                        #s[number,:,digit] = s[number,:,digit]/1000\n",
    "                        #phi[number,digit,:] = torch.sum(output_first_layer_after[target==digit,:], dim=0)/1000\n",
    "                    #number += 1\n",
    "\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    valid_total = output.shape[0]\n",
    "                    valid_correct = (predicted == target).sum().item()\n",
    "                    prec = valid_correct / valid_total\n",
    "                    print('Accuary on test images:{:.2f}%, loss:{:.2f}'.format(prec*100,loss))\n",
    "                    test_accuracy.append(prec)\n",
    "                    all_loss.append(loss)\n",
    "                    best_prec = max(prec, best_prec)\n",
    "\n",
    "                for j, (input, target) in enumerate(all_train_loader):\n",
    "                    input, target = input, target.long()\n",
    "                    output = model(input)\n",
    "\n",
    "                    mean_difference = 0\n",
    "                    for index in range(output.shape[0]):\n",
    "                        output[index,:] = output[index,:]/torch.norm(output[index,:],2)\n",
    "                        #all_cos_cor[index] = output[index, target[index]]\n",
    "                        #all_cos_mis[:,index] = output[index, torch.arange(10) != target[index]]\n",
    "                        for digit in range(10):\n",
    "                            if digit != target[index]:\n",
    "                                mean_difference += output[index, target[index]]-output[index,digit]\n",
    "\n",
    "                    mean.append(mean_difference/540000)\n",
    "\n",
    "                #all_cos_cor = torch.zeros(output.shape[0])\n",
    "                #all_cos_mis = torch.zeros(9,output.shape[0])  \n",
    "                        \n",
    "                #for index in range(output.shape[0]):\n",
    "                 #   output[index,:] = output[index,:]/torch.norm(output[index,:],2)\n",
    "                  #  all_cos_cor[index] = output[index, target[index]]\n",
    "                   # all_cos_mis[:,index] = output[index, torch.arange(10) != target[index]]\n",
    "\n",
    "                #bins=np.arange(min(torch.min(all_cos_cor), torch.min(all_cos_mis)),max(torch.max(all_cos_cor), torch.max(all_cos_mis)), 1/2000) \n",
    "                #frequency_each,_ = np.histogram(all_cos_mis.reshape(9*output.shape[0]).tolist(), bins = bins)\n",
    "                #frequency_each_c,_ = np.histogram(all_cos_cor.tolist(),bins = bins)\n",
    "                #cdf_mistaken = torch.cat((torch.Tensor([0]),torch.cumsum(torch.from_numpy(frequency_each)/(9*output.shape[0]), dim=0)),0)\n",
    "                #cdf_correct = torch.cat((torch.Tensor([0]),torch.cumsum(torch.from_numpy(frequency_each_c)/output.shape[0], dim=0)),0)\n",
    "                #KS_distance = torch.max(abs(cdf_correct-cdf_mistaken))\n",
    "                #KS_dis.append(KS_distance)\n",
    "                            \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "print('Best accuracy is: {:.2f}%'.format(best_prec*100))\n",
    "#weight_a = model.state_dict()['classifier.0.weight_a']\n",
    "#torch.save(weight_a,'afte r_train_weight')\n",
    "#torch.save(weight_a,'weight_a')\n",
    "# 改变模型参数\n",
    "#model_dict = model.state_dict()\n",
    "#model_dict['module.classifier.0.weight_a'] = torch.zeros(10,784,784)\n",
    "#model.load_state_dict(model_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleNet_2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Low_dimensional_quadratic(in_features=784, out_features=10, number_eigens=1, bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch [1/20], Step [0/600], Loss: 2.30253, Train_Acc:15.00%\n",
      "Accuary on test images:11.17%, loss:2.30\n",
      "Epoch [1/20], Step [5/600], Loss: 0.38375, Train_Acc:19.00%\n",
      "Accuary on test images:11.89%, loss:2.30\n",
      "Epoch [1/20], Step [10/600], Loss: 0.20933, Train_Acc:8.00%\n",
      "Accuary on test images:12.51%, loss:2.30\n",
      "Epoch [1/20], Step [15/600], Loss: 0.14391, Train_Acc:14.00%\n",
      "Accuary on test images:13.10%, loss:2.30\n",
      "Epoch [1/20], Step [20/600], Loss: 0.10965, Train_Acc:10.00%\n",
      "Accuary on test images:13.57%, loss:2.30\n",
      "Epoch [1/20], Step [25/600], Loss: 0.08856, Train_Acc:17.00%\n",
      "Accuary on test images:14.30%, loss:2.30\n",
      "Epoch [1/20], Step [30/600], Loss: 0.07427, Train_Acc:19.00%\n",
      "Accuary on test images:14.82%, loss:2.30\n",
      "Epoch [1/20], Step [35/600], Loss: 0.06396, Train_Acc:9.00%\n",
      "Accuary on test images:15.30%, loss:2.30\n",
      "Epoch [1/20], Step [40/600], Loss: 0.05616, Train_Acc:16.00%\n",
      "Accuary on test images:15.73%, loss:2.30\n",
      "Epoch [1/20], Step [45/600], Loss: 0.05006, Train_Acc:6.00%\n",
      "Accuary on test images:16.19%, loss:2.30\n",
      "Epoch [1/20], Step [50/600], Loss: 0.04515, Train_Acc:18.00%\n",
      "Accuary on test images:16.54%, loss:2.30\n",
      "Epoch [1/20], Step [55/600], Loss: 0.04112, Train_Acc:20.00%\n",
      "Accuary on test images:16.93%, loss:2.30\n",
      "Epoch [1/20], Step [60/600], Loss: 0.03775, Train_Acc:13.00%\n",
      "Accuary on test images:17.19%, loss:2.30\n",
      "Epoch [1/20], Step [65/600], Loss: 0.03489, Train_Acc:15.00%\n",
      "Accuary on test images:17.53%, loss:2.30\n",
      "Epoch [1/20], Step [70/600], Loss: 0.03243, Train_Acc:19.00%\n",
      "Accuary on test images:17.82%, loss:2.30\n",
      "Epoch [1/20], Step [75/600], Loss: 0.03030, Train_Acc:15.00%\n",
      "Accuary on test images:18.22%, loss:2.30\n",
      "Epoch [1/20], Step [80/600], Loss: 0.02843, Train_Acc:24.00%\n",
      "Accuary on test images:18.56%, loss:2.30\n",
      "Epoch [1/20], Step [85/600], Loss: 0.02677, Train_Acc:21.00%\n",
      "Accuary on test images:18.56%, loss:2.30\n",
      "Epoch [1/20], Step [90/600], Loss: 0.02530, Train_Acc:19.00%\n",
      "Accuary on test images:19.02%, loss:2.30\n",
      "Epoch [1/20], Step [95/600], Loss: 0.02398, Train_Acc:19.00%\n",
      "Accuary on test images:19.29%, loss:2.30\n",
      "Epoch [1/20], Step [100/600], Loss: 0.02280, Train_Acc:20.00%\n",
      "Accuary on test images:19.74%, loss:2.30\n",
      "Epoch [1/20], Step [105/600], Loss: 0.02172, Train_Acc:16.00%\n",
      "Accuary on test images:19.81%, loss:2.30\n",
      "Epoch [1/20], Step [110/600], Loss: 0.02074, Train_Acc:16.00%\n",
      "Accuary on test images:20.13%, loss:2.30\n",
      "Epoch [1/20], Step [115/600], Loss: 0.01985, Train_Acc:16.00%\n",
      "Accuary on test images:20.03%, loss:2.30\n",
      "Epoch [1/20], Step [120/600], Loss: 0.01903, Train_Acc:26.00%\n",
      "Accuary on test images:20.19%, loss:2.30\n",
      "Epoch [1/20], Step [125/600], Loss: 0.01827, Train_Acc:19.00%\n",
      "Accuary on test images:20.31%, loss:2.30\n",
      "Epoch [1/20], Step [130/600], Loss: 0.01757, Train_Acc:21.00%\n",
      "Accuary on test images:20.80%, loss:2.30\n",
      "Epoch [1/20], Step [135/600], Loss: 0.01693, Train_Acc:23.00%\n",
      "Accuary on test images:20.89%, loss:2.30\n",
      "Epoch [1/20], Step [140/600], Loss: 0.01633, Train_Acc:22.00%\n",
      "Accuary on test images:20.86%, loss:2.30\n",
      "Epoch [1/20], Step [145/600], Loss: 0.01577, Train_Acc:25.00%\n",
      "Accuary on test images:20.77%, loss:2.30\n",
      "Epoch [1/20], Step [150/600], Loss: 0.01525, Train_Acc:21.00%\n",
      "Accuary on test images:21.18%, loss:2.30\n",
      "Epoch [1/20], Step [155/600], Loss: 0.01476, Train_Acc:21.00%\n",
      "Accuary on test images:21.64%, loss:2.30\n",
      "Epoch [1/20], Step [160/600], Loss: 0.01430, Train_Acc:19.00%\n",
      "Accuary on test images:22.04%, loss:2.30\n",
      "Epoch [1/20], Step [165/600], Loss: 0.01387, Train_Acc:19.00%\n",
      "Accuary on test images:22.00%, loss:2.30\n",
      "Epoch [1/20], Step [170/600], Loss: 0.01346, Train_Acc:19.00%\n",
      "Accuary on test images:22.47%, loss:2.30\n",
      "Epoch [1/20], Step [175/600], Loss: 0.01308, Train_Acc:23.00%\n",
      "Accuary on test images:22.64%, loss:2.30\n",
      "Epoch [1/20], Step [180/600], Loss: 0.01272, Train_Acc:27.00%\n",
      "Accuary on test images:22.72%, loss:2.30\n",
      "Epoch [1/20], Step [185/600], Loss: 0.01238, Train_Acc:25.00%\n",
      "Accuary on test images:22.55%, loss:2.30\n",
      "Epoch [1/20], Step [190/600], Loss: 0.01205, Train_Acc:19.00%\n",
      "Accuary on test images:23.18%, loss:2.30\n",
      "Epoch [1/20], Step [195/600], Loss: 0.01174, Train_Acc:21.00%\n",
      "Accuary on test images:23.26%, loss:2.30\n",
      "Epoch [1/20], Step [200/600], Loss: 0.01145, Train_Acc:21.00%\n",
      "Accuary on test images:23.27%, loss:2.30\n",
      "Epoch [1/20], Step [205/600], Loss: 0.01118, Train_Acc:16.00%\n",
      "Accuary on test images:23.36%, loss:2.30\n",
      "Epoch [1/20], Step [210/600], Loss: 0.01091, Train_Acc:30.00%\n",
      "Accuary on test images:23.95%, loss:2.30\n",
      "Epoch [1/20], Step [215/600], Loss: 0.01066, Train_Acc:22.00%\n",
      "Accuary on test images:24.24%, loss:2.30\n",
      "Epoch [1/20], Step [220/600], Loss: 0.01041, Train_Acc:19.00%\n",
      "Accuary on test images:24.64%, loss:2.30\n",
      "Epoch [1/20], Step [225/600], Loss: 0.01018, Train_Acc:22.00%\n",
      "Accuary on test images:24.62%, loss:2.30\n",
      "Epoch [1/20], Step [230/600], Loss: 0.00996, Train_Acc:25.00%\n",
      "Accuary on test images:25.45%, loss:2.30\n",
      "Epoch [1/20], Step [235/600], Loss: 0.00974, Train_Acc:35.00%\n",
      "Accuary on test images:24.96%, loss:2.30\n",
      "Epoch [1/20], Step [240/600], Loss: 0.00955, Train_Acc:23.00%\n",
      "Accuary on test images:25.90%, loss:2.30\n",
      "Epoch [1/20], Step [245/600], Loss: 0.00935, Train_Acc:26.00%\n",
      "Accuary on test images:25.94%, loss:2.30\n",
      "Epoch [1/20], Step [250/600], Loss: 0.00916, Train_Acc:26.00%\n",
      "Accuary on test images:26.19%, loss:2.30\n",
      "Epoch [1/20], Step [255/600], Loss: 0.00898, Train_Acc:28.00%\n",
      "Accuary on test images:27.31%, loss:2.30\n",
      "Epoch [1/20], Step [260/600], Loss: 0.00880, Train_Acc:32.00%\n",
      "Accuary on test images:25.44%, loss:2.30\n",
      "Epoch [1/20], Step [265/600], Loss: 0.00862, Train_Acc:28.00%\n",
      "Accuary on test images:22.55%, loss:2.29\n",
      "Epoch [1/20], Step [270/600], Loss: 0.00846, Train_Acc:18.00%\n",
      "Accuary on test images:21.92%, loss:2.29\n",
      "Epoch [1/20], Step [275/600], Loss: 0.00826, Train_Acc:28.00%\n",
      "Accuary on test images:20.36%, loss:2.28\n",
      "Epoch [1/20], Step [280/600], Loss: 0.00804, Train_Acc:37.00%\n",
      "Accuary on test images:21.16%, loss:2.27\n",
      "Epoch [1/20], Step [285/600], Loss: 0.00787, Train_Acc:24.00%\n",
      "Accuary on test images:18.05%, loss:2.25\n",
      "Epoch [1/20], Step [290/600], Loss: 0.00762, Train_Acc:27.00%\n",
      "Accuary on test images:24.92%, loss:2.20\n",
      "Epoch [1/20], Step [295/600], Loss: 0.00725, Train_Acc:30.00%\n",
      "Accuary on test images:22.71%, loss:2.13\n",
      "Epoch [1/20], Step [300/600], Loss: 0.00702, Train_Acc:24.00%\n",
      "Accuary on test images:43.94%, loss:1.97\n",
      "Epoch [1/20], Step [305/600], Loss: 0.00662, Train_Acc:31.00%\n",
      "Accuary on test images:45.27%, loss:1.80\n",
      "Epoch [1/20], Step [310/600], Loss: 0.00592, Train_Acc:45.00%\n",
      "Accuary on test images:46.31%, loss:1.74\n",
      "Epoch [1/20], Step [315/600], Loss: 0.00499, Train_Acc:60.00%\n",
      "Accuary on test images:39.01%, loss:1.95\n",
      "Epoch [1/20], Step [320/600], Loss: 0.00605, Train_Acc:32.00%\n",
      "Accuary on test images:44.37%, loss:1.77\n",
      "Epoch [1/20], Step [325/600], Loss: 0.00469, Train_Acc:54.00%\n",
      "Accuary on test images:49.98%, loss:1.48\n",
      "Epoch [1/20], Step [330/600], Loss: 0.00514, Train_Acc:50.00%\n",
      "Accuary on test images:47.63%, loss:2.00\n",
      "Epoch [1/20], Step [335/600], Loss: 0.00416, Train_Acc:59.00%\n",
      "Accuary on test images:60.32%, loss:1.42\n",
      "Epoch [1/20], Step [340/600], Loss: 0.00439, Train_Acc:55.00%\n",
      "Accuary on test images:60.23%, loss:1.40\n",
      "Epoch [1/20], Step [345/600], Loss: 0.00485, Train_Acc:59.00%\n",
      "Accuary on test images:53.97%, loss:1.55\n",
      "Epoch [1/20], Step [350/600], Loss: 0.00348, Train_Acc:64.00%\n",
      "Accuary on test images:63.55%, loss:1.17\n",
      "Epoch [1/20], Step [355/600], Loss: 0.00348, Train_Acc:65.00%\n",
      "Accuary on test images:65.56%, loss:1.21\n",
      "Epoch [1/20], Step [360/600], Loss: 0.00378, Train_Acc:59.00%\n",
      "Accuary on test images:64.34%, loss:1.22\n",
      "Epoch [1/20], Step [365/600], Loss: 0.00308, Train_Acc:66.00%\n",
      "Accuary on test images:61.62%, loss:1.27\n",
      "Epoch [1/20], Step [370/600], Loss: 0.00371, Train_Acc:58.00%\n",
      "Accuary on test images:62.84%, loss:1.24\n",
      "Epoch [1/20], Step [375/600], Loss: 0.00330, Train_Acc:65.00%\n",
      "Accuary on test images:61.25%, loss:1.16\n",
      "Epoch [1/20], Step [380/600], Loss: 0.00428, Train_Acc:66.00%\n",
      "Accuary on test images:63.64%, loss:1.24\n",
      "Epoch [1/20], Step [385/600], Loss: 0.00240, Train_Acc:70.00%\n",
      "Accuary on test images:65.83%, loss:1.02\n",
      "Epoch [1/20], Step [390/600], Loss: 0.00309, Train_Acc:66.00%\n",
      "Accuary on test images:57.27%, loss:1.77\n",
      "Epoch [1/20], Step [395/600], Loss: 0.00212, Train_Acc:72.00%\n",
      "Accuary on test images:67.92%, loss:1.05\n",
      "Epoch [1/20], Step [400/600], Loss: 0.00231, Train_Acc:74.00%\n",
      "Accuary on test images:66.31%, loss:1.31\n",
      "Epoch [1/20], Step [405/600], Loss: 0.00292, Train_Acc:60.00%\n",
      "Accuary on test images:66.24%, loss:1.08\n",
      "Epoch [1/20], Step [410/600], Loss: 0.00271, Train_Acc:71.00%\n",
      "Accuary on test images:66.02%, loss:1.26\n",
      "Epoch [1/20], Step [415/600], Loss: 0.00269, Train_Acc:67.00%\n",
      "Accuary on test images:64.90%, loss:1.10\n",
      "Epoch [1/20], Step [420/600], Loss: 0.00276, Train_Acc:66.00%\n",
      "Accuary on test images:63.66%, loss:1.64\n",
      "Epoch [1/20], Step [425/600], Loss: 0.00249, Train_Acc:71.00%\n",
      "Accuary on test images:63.55%, loss:1.14\n",
      "Epoch [1/20], Step [430/600], Loss: 0.00174, Train_Acc:74.00%\n",
      "Accuary on test images:71.18%, loss:0.93\n",
      "Epoch [1/20], Step [435/600], Loss: 0.00292, Train_Acc:64.00%\n",
      "Accuary on test images:67.52%, loss:1.21\n",
      "Epoch [1/20], Step [440/600], Loss: 0.00206, Train_Acc:77.00%\n",
      "Accuary on test images:71.22%, loss:0.96\n",
      "Epoch [1/20], Step [445/600], Loss: 0.00204, Train_Acc:76.00%\n",
      "Accuary on test images:69.97%, loss:0.93\n",
      "Epoch [1/20], Step [450/600], Loss: 0.00145, Train_Acc:82.00%\n",
      "Accuary on test images:72.59%, loss:0.93\n",
      "Epoch [1/20], Step [455/600], Loss: 0.00283, Train_Acc:66.00%\n",
      "Accuary on test images:69.62%, loss:1.06\n",
      "Epoch [1/20], Step [460/600], Loss: 0.00183, Train_Acc:79.00%\n",
      "Accuary on test images:71.15%, loss:0.87\n",
      "Epoch [1/20], Step [465/600], Loss: 0.00153, Train_Acc:80.00%\n",
      "Accuary on test images:73.36%, loss:0.87\n",
      "Epoch [1/20], Step [470/600], Loss: 0.00249, Train_Acc:73.00%\n",
      "Accuary on test images:71.22%, loss:1.06\n",
      "Epoch [1/20], Step [475/600], Loss: 0.00221, Train_Acc:68.00%\n",
      "Accuary on test images:73.63%, loss:0.94\n",
      "Epoch [1/20], Step [480/600], Loss: 0.00176, Train_Acc:75.00%\n",
      "Accuary on test images:71.14%, loss:0.91\n",
      "Epoch [1/20], Step [485/600], Loss: 0.00227, Train_Acc:66.00%\n",
      "Accuary on test images:71.91%, loss:1.02\n",
      "Epoch [1/20], Step [490/600], Loss: 0.00156, Train_Acc:76.00%\n",
      "Accuary on test images:74.54%, loss:0.81\n",
      "Epoch [1/20], Step [495/600], Loss: 0.00205, Train_Acc:64.00%\n",
      "Accuary on test images:69.52%, loss:0.90\n",
      "Epoch [1/20], Step [500/600], Loss: 0.00140, Train_Acc:75.00%\n",
      "Accuary on test images:74.18%, loss:0.80\n",
      "Epoch [1/20], Step [505/600], Loss: 0.00203, Train_Acc:70.00%\n",
      "Accuary on test images:74.06%, loss:0.99\n",
      "Epoch [1/20], Step [510/600], Loss: 0.00269, Train_Acc:67.00%\n",
      "Accuary on test images:71.55%, loss:1.05\n",
      "Epoch [1/20], Step [515/600], Loss: 0.00191, Train_Acc:67.00%\n",
      "Accuary on test images:72.77%, loss:0.87\n",
      "Epoch [1/20], Step [520/600], Loss: 0.00150, Train_Acc:67.00%\n",
      "Accuary on test images:72.01%, loss:0.89\n",
      "Epoch [1/20], Step [525/600], Loss: 0.00134, Train_Acc:74.00%\n",
      "Accuary on test images:73.56%, loss:0.80\n",
      "Epoch [1/20], Step [530/600], Loss: 0.00212, Train_Acc:69.00%\n",
      "Accuary on test images:73.58%, loss:1.02\n",
      "Epoch [1/20], Step [535/600], Loss: 0.00198, Train_Acc:73.00%\n",
      "Accuary on test images:71.71%, loss:0.91\n",
      "Epoch [1/20], Step [540/600], Loss: 0.00149, Train_Acc:77.00%\n",
      "Accuary on test images:74.86%, loss:0.82\n",
      "Epoch [1/20], Step [545/600], Loss: 0.00132, Train_Acc:82.00%\n",
      "Accuary on test images:73.55%, loss:0.86\n",
      "Epoch [1/20], Step [550/600], Loss: 0.00158, Train_Acc:73.00%\n",
      "Accuary on test images:73.12%, loss:0.83\n",
      "Epoch [1/20], Step [555/600], Loss: 0.00163, Train_Acc:68.00%\n",
      "Accuary on test images:72.58%, loss:0.94\n",
      "Epoch [1/20], Step [560/600], Loss: 0.00137, Train_Acc:78.00%\n",
      "Accuary on test images:73.74%, loss:0.81\n",
      "Epoch [1/20], Step [565/600], Loss: 0.00139, Train_Acc:73.00%\n",
      "Accuary on test images:73.71%, loss:0.94\n",
      "Epoch [1/20], Step [570/600], Loss: 0.00121, Train_Acc:81.00%\n",
      "Accuary on test images:77.20%, loss:0.75\n",
      "Epoch [1/20], Step [575/600], Loss: 0.00228, Train_Acc:70.00%\n",
      "Accuary on test images:75.78%, loss:0.90\n",
      "Epoch [1/20], Step [580/600], Loss: 0.00121, Train_Acc:76.00%\n",
      "Accuary on test images:75.11%, loss:0.77\n",
      "Epoch [1/20], Step [585/600], Loss: 0.00162, Train_Acc:73.00%\n",
      "Accuary on test images:76.47%, loss:0.84\n",
      "Epoch [1/20], Step [590/600], Loss: 0.00134, Train_Acc:71.00%\n",
      "Accuary on test images:70.70%, loss:1.00\n",
      "Epoch [1/20], Step [595/600], Loss: 0.00119, Train_Acc:76.00%\n",
      "Accuary on test images:76.59%, loss:0.73\n",
      "Epoch [2/20], Step [0/600], Loss: 0.71732, Train_Acc:73.00%\n",
      "Accuary on test images:75.51%, loss:0.75\n",
      "Epoch [2/20], Step [5/600], Loss: 0.17683, Train_Acc:74.00%\n",
      "Accuary on test images:75.68%, loss:0.94\n",
      "Epoch [2/20], Step [10/600], Loss: 0.08442, Train_Acc:72.00%\n",
      "Accuary on test images:76.95%, loss:0.75\n",
      "Epoch [2/20], Step [15/600], Loss: 0.05129, Train_Acc:70.00%\n",
      "Accuary on test images:73.01%, loss:0.80\n",
      "Epoch [2/20], Step [20/600], Loss: 0.02487, Train_Acc:83.00%\n",
      "Accuary on test images:77.23%, loss:0.71\n",
      "Epoch [2/20], Step [25/600], Loss: 0.03610, Train_Acc:68.00%\n",
      "Accuary on test images:71.40%, loss:1.15\n",
      "Epoch [2/20], Step [30/600], Loss: 0.02281, Train_Acc:75.00%\n",
      "Accuary on test images:74.55%, loss:0.78\n",
      "Epoch [2/20], Step [35/600], Loss: 0.03110, Train_Acc:68.00%\n",
      "Accuary on test images:72.92%, loss:1.00\n",
      "Epoch [2/20], Step [40/600], Loss: 0.01423, Train_Acc:83.00%\n",
      "Accuary on test images:76.27%, loss:0.84\n",
      "Epoch [2/20], Step [45/600], Loss: 0.01634, Train_Acc:77.00%\n",
      "Accuary on test images:76.94%, loss:0.71\n",
      "Epoch [2/20], Step [50/600], Loss: 0.01817, Train_Acc:72.00%\n",
      "Accuary on test images:71.82%, loss:0.92\n",
      "Epoch [2/20], Step [55/600], Loss: 0.01213, Train_Acc:82.00%\n",
      "Accuary on test images:77.19%, loss:0.75\n",
      "Epoch [2/20], Step [60/600], Loss: 0.01770, Train_Acc:70.00%\n",
      "Accuary on test images:72.30%, loss:0.89\n",
      "Epoch [2/20], Step [65/600], Loss: 0.01296, Train_Acc:70.00%\n",
      "Accuary on test images:72.13%, loss:0.87\n",
      "Epoch [2/20], Step [70/600], Loss: 0.00929, Train_Acc:74.00%\n",
      "Accuary on test images:77.35%, loss:0.69\n",
      "Epoch [2/20], Step [75/600], Loss: 0.01101, Train_Acc:80.00%\n",
      "Accuary on test images:76.78%, loss:0.89\n",
      "Epoch [2/20], Step [80/600], Loss: 0.01220, Train_Acc:73.00%\n",
      "Accuary on test images:76.37%, loss:0.86\n",
      "Epoch [2/20], Step [85/600], Loss: 0.00892, Train_Acc:77.00%\n",
      "Accuary on test images:76.48%, loss:0.76\n",
      "Epoch [2/20], Step [90/600], Loss: 0.00698, Train_Acc:77.00%\n",
      "Accuary on test images:76.52%, loss:0.71\n",
      "Epoch [2/20], Step [95/600], Loss: 0.00965, Train_Acc:70.00%\n",
      "Accuary on test images:75.37%, loss:0.79\n",
      "Epoch [2/20], Step [100/600], Loss: 0.00674, Train_Acc:79.00%\n",
      "Accuary on test images:76.95%, loss:0.71\n",
      "Epoch [2/20], Step [105/600], Loss: 0.00814, Train_Acc:74.00%\n",
      "Accuary on test images:75.11%, loss:0.74\n",
      "Epoch [2/20], Step [110/600], Loss: 0.00651, Train_Acc:78.00%\n",
      "Accuary on test images:76.14%, loss:0.70\n",
      "Epoch [2/20], Step [115/600], Loss: 0.00706, Train_Acc:69.00%\n",
      "Accuary on test images:74.74%, loss:0.78\n",
      "Epoch [2/20], Step [120/600], Loss: 0.00764, Train_Acc:76.00%\n",
      "Accuary on test images:76.77%, loss:0.73\n",
      "Epoch [2/20], Step [125/600], Loss: 0.00719, Train_Acc:72.00%\n",
      "Accuary on test images:76.58%, loss:0.78\n",
      "Epoch [2/20], Step [130/600], Loss: 0.00506, Train_Acc:75.00%\n",
      "Accuary on test images:75.43%, loss:0.71\n",
      "Epoch [2/20], Step [135/600], Loss: 0.00846, Train_Acc:69.00%\n",
      "Accuary on test images:73.83%, loss:1.00\n",
      "Epoch [2/20], Step [140/600], Loss: 0.00566, Train_Acc:80.00%\n",
      "Accuary on test images:77.63%, loss:0.76\n",
      "Epoch [2/20], Step [145/600], Loss: 0.00317, Train_Acc:79.00%\n",
      "Accuary on test images:78.93%, loss:0.66\n",
      "Epoch [2/20], Step [150/600], Loss: 0.00566, Train_Acc:67.00%\n",
      "Accuary on test images:75.23%, loss:0.81\n",
      "Epoch [2/20], Step [155/600], Loss: 0.00538, Train_Acc:70.00%\n",
      "Accuary on test images:74.56%, loss:0.80\n",
      "Epoch [2/20], Step [160/600], Loss: 0.00432, Train_Acc:75.00%\n",
      "Accuary on test images:79.76%, loss:0.62\n",
      "Epoch [2/20], Step [165/600], Loss: 0.00406, Train_Acc:83.00%\n",
      "Accuary on test images:77.58%, loss:0.72\n",
      "Epoch [2/20], Step [170/600], Loss: 0.00414, Train_Acc:76.00%\n",
      "Accuary on test images:77.91%, loss:0.67\n",
      "Epoch [2/20], Step [175/600], Loss: 0.00334, Train_Acc:77.00%\n",
      "Accuary on test images:76.74%, loss:0.73\n",
      "Epoch [2/20], Step [180/600], Loss: 0.00496, Train_Acc:73.00%\n",
      "Accuary on test images:77.49%, loss:0.75\n",
      "Epoch [2/20], Step [185/600], Loss: 0.00405, Train_Acc:73.00%\n",
      "Accuary on test images:74.89%, loss:0.78\n",
      "Epoch [2/20], Step [190/600], Loss: 0.00330, Train_Acc:79.00%\n",
      "Accuary on test images:77.48%, loss:0.66\n",
      "Epoch [2/20], Step [195/600], Loss: 0.00343, Train_Acc:83.00%\n",
      "Accuary on test images:80.07%, loss:0.61\n",
      "Epoch [2/20], Step [200/600], Loss: 0.00350, Train_Acc:76.00%\n",
      "Accuary on test images:76.87%, loss:0.69\n",
      "Epoch [2/20], Step [205/600], Loss: 0.00288, Train_Acc:83.00%\n",
      "Accuary on test images:78.87%, loss:0.63\n",
      "Epoch [2/20], Step [210/600], Loss: 0.00352, Train_Acc:71.00%\n",
      "Accuary on test images:78.15%, loss:0.67\n",
      "Epoch [2/20], Step [215/600], Loss: 0.00204, Train_Acc:82.00%\n",
      "Accuary on test images:78.34%, loss:0.66\n",
      "Epoch [2/20], Step [220/600], Loss: 0.00296, Train_Acc:80.00%\n",
      "Accuary on test images:79.31%, loss:0.64\n",
      "Epoch [2/20], Step [225/600], Loss: 0.00343, Train_Acc:73.00%\n",
      "Accuary on test images:77.94%, loss:0.68\n",
      "Epoch [2/20], Step [230/600], Loss: 0.00217, Train_Acc:85.00%\n",
      "Accuary on test images:77.65%, loss:0.70\n",
      "Epoch [2/20], Step [235/600], Loss: 0.00282, Train_Acc:76.00%\n",
      "Accuary on test images:78.16%, loss:0.66\n",
      "Epoch [2/20], Step [240/600], Loss: 0.00272, Train_Acc:74.00%\n",
      "Accuary on test images:78.92%, loss:0.62\n",
      "Epoch [2/20], Step [245/600], Loss: 0.00279, Train_Acc:78.00%\n",
      "Accuary on test images:78.62%, loss:0.63\n",
      "Epoch [2/20], Step [250/600], Loss: 0.00327, Train_Acc:78.00%\n",
      "Accuary on test images:78.20%, loss:0.66\n",
      "Epoch [2/20], Step [255/600], Loss: 0.00225, Train_Acc:83.00%\n",
      "Accuary on test images:80.31%, loss:0.61\n",
      "Epoch [2/20], Step [260/600], Loss: 0.00275, Train_Acc:74.00%\n",
      "Accuary on test images:80.14%, loss:0.60\n",
      "Epoch [2/20], Step [265/600], Loss: 0.00221, Train_Acc:80.00%\n",
      "Accuary on test images:78.40%, loss:0.63\n",
      "Epoch [2/20], Step [270/600], Loss: 0.00237, Train_Acc:78.00%\n",
      "Accuary on test images:80.16%, loss:0.60\n",
      "Epoch [2/20], Step [275/600], Loss: 0.00216, Train_Acc:79.00%\n",
      "Accuary on test images:80.86%, loss:0.58\n",
      "Epoch [2/20], Step [280/600], Loss: 0.00338, Train_Acc:75.00%\n",
      "Accuary on test images:76.84%, loss:0.71\n",
      "Epoch [2/20], Step [285/600], Loss: 0.00215, Train_Acc:79.00%\n",
      "Accuary on test images:78.33%, loss:0.66\n",
      "Epoch [2/20], Step [290/600], Loss: 0.00227, Train_Acc:79.00%\n",
      "Accuary on test images:79.86%, loss:0.61\n",
      "Epoch [2/20], Step [295/600], Loss: 0.00127, Train_Acc:88.00%\n",
      "Accuary on test images:80.02%, loss:0.61\n",
      "Epoch [2/20], Step [300/600], Loss: 0.00185, Train_Acc:84.00%\n",
      "Accuary on test images:77.18%, loss:0.66\n",
      "Epoch [2/20], Step [305/600], Loss: 0.00231, Train_Acc:76.00%\n",
      "Accuary on test images:78.34%, loss:0.66\n",
      "Epoch [2/20], Step [310/600], Loss: 0.00188, Train_Acc:82.00%\n",
      "Accuary on test images:79.34%, loss:0.64\n",
      "Epoch [2/20], Step [315/600], Loss: 0.00230, Train_Acc:80.00%\n",
      "Accuary on test images:81.82%, loss:0.58\n",
      "Epoch [2/20], Step [320/600], Loss: 0.00188, Train_Acc:80.00%\n",
      "Accuary on test images:80.89%, loss:0.58\n",
      "Epoch [2/20], Step [325/600], Loss: 0.00198, Train_Acc:76.00%\n",
      "Accuary on test images:78.41%, loss:0.63\n",
      "Epoch [2/20], Step [330/600], Loss: 0.00157, Train_Acc:83.00%\n",
      "Accuary on test images:80.00%, loss:0.61\n",
      "Epoch [2/20], Step [335/600], Loss: 0.00170, Train_Acc:80.00%\n",
      "Accuary on test images:78.54%, loss:0.65\n",
      "Epoch [2/20], Step [340/600], Loss: 0.00173, Train_Acc:81.00%\n",
      "Accuary on test images:79.67%, loss:0.60\n",
      "Epoch [2/20], Step [345/600], Loss: 0.00141, Train_Acc:83.00%\n",
      "Accuary on test images:80.42%, loss:0.58\n",
      "Epoch [2/20], Step [350/600], Loss: 0.00122, Train_Acc:87.00%\n",
      "Accuary on test images:80.96%, loss:0.58\n",
      "Epoch [2/20], Step [355/600], Loss: 0.00148, Train_Acc:82.00%\n",
      "Accuary on test images:79.95%, loss:0.58\n",
      "Epoch [2/20], Step [360/600], Loss: 0.00165, Train_Acc:82.00%\n",
      "Accuary on test images:79.94%, loss:0.58\n",
      "Epoch [2/20], Step [365/600], Loss: 0.00123, Train_Acc:86.00%\n",
      "Accuary on test images:80.28%, loss:0.60\n",
      "Epoch [2/20], Step [370/600], Loss: 0.00196, Train_Acc:79.00%\n",
      "Accuary on test images:75.50%, loss:0.67\n",
      "Epoch [2/20], Step [375/600], Loss: 0.00143, Train_Acc:83.00%\n",
      "Accuary on test images:80.35%, loss:0.61\n",
      "Epoch [2/20], Step [380/600], Loss: 0.00158, Train_Acc:77.00%\n",
      "Accuary on test images:80.94%, loss:0.57\n",
      "Epoch [2/20], Step [385/600], Loss: 0.00131, Train_Acc:84.00%\n",
      "Accuary on test images:79.80%, loss:0.62\n",
      "Epoch [2/20], Step [390/600], Loss: 0.00159, Train_Acc:79.00%\n",
      "Accuary on test images:81.63%, loss:0.57\n",
      "Epoch [2/20], Step [395/600], Loss: 0.00162, Train_Acc:79.00%\n",
      "Accuary on test images:80.99%, loss:0.61\n",
      "Epoch [2/20], Step [400/600], Loss: 0.00124, Train_Acc:83.00%\n",
      "Accuary on test images:81.38%, loss:0.58\n",
      "Epoch [2/20], Step [405/600], Loss: 0.00107, Train_Acc:84.00%\n",
      "Accuary on test images:80.12%, loss:0.60\n",
      "Epoch [2/20], Step [410/600], Loss: 0.00125, Train_Acc:82.00%\n",
      "Accuary on test images:80.90%, loss:0.57\n",
      "Epoch [2/20], Step [415/600], Loss: 0.00153, Train_Acc:81.00%\n",
      "Accuary on test images:80.36%, loss:0.57\n",
      "Epoch [2/20], Step [420/600], Loss: 0.00098, Train_Acc:85.00%\n",
      "Accuary on test images:81.60%, loss:0.55\n",
      "Epoch [2/20], Step [425/600], Loss: 0.00156, Train_Acc:78.00%\n",
      "Accuary on test images:79.78%, loss:0.62\n",
      "Epoch [2/20], Step [430/600], Loss: 0.00099, Train_Acc:81.00%\n",
      "Accuary on test images:80.87%, loss:0.57\n",
      "Epoch [2/20], Step [435/600], Loss: 0.00142, Train_Acc:77.00%\n",
      "Accuary on test images:79.76%, loss:0.58\n",
      "Epoch [2/20], Step [440/600], Loss: 0.00106, Train_Acc:85.00%\n",
      "Accuary on test images:78.18%, loss:0.66\n",
      "Epoch [2/20], Step [445/600], Loss: 0.00104, Train_Acc:83.00%\n",
      "Accuary on test images:80.34%, loss:0.60\n",
      "Epoch [2/20], Step [450/600], Loss: 0.00106, Train_Acc:85.00%\n",
      "Accuary on test images:80.87%, loss:0.59\n",
      "Epoch [2/20], Step [455/600], Loss: 0.00116, Train_Acc:77.00%\n",
      "Accuary on test images:80.56%, loss:0.59\n",
      "Epoch [2/20], Step [460/600], Loss: 0.00114, Train_Acc:86.00%\n",
      "Accuary on test images:80.87%, loss:0.56\n",
      "Epoch [2/20], Step [465/600], Loss: 0.00078, Train_Acc:87.00%\n",
      "Accuary on test images:81.19%, loss:0.58\n",
      "Epoch [2/20], Step [470/600], Loss: 0.00128, Train_Acc:80.00%\n",
      "Accuary on test images:80.24%, loss:0.60\n",
      "Epoch [2/20], Step [475/600], Loss: 0.00139, Train_Acc:75.00%\n",
      "Accuary on test images:81.64%, loss:0.57\n",
      "Epoch [2/20], Step [480/600], Loss: 0.00129, Train_Acc:79.00%\n",
      "Accuary on test images:77.94%, loss:0.68\n",
      "Epoch [2/20], Step [485/600], Loss: 0.00096, Train_Acc:85.00%\n",
      "Accuary on test images:78.95%, loss:0.71\n",
      "Epoch [2/20], Step [490/600], Loss: 0.00165, Train_Acc:79.00%\n",
      "Accuary on test images:76.93%, loss:0.64\n",
      "Epoch [2/20], Step [495/600], Loss: 0.00123, Train_Acc:81.00%\n",
      "Accuary on test images:81.73%, loss:0.56\n",
      "Epoch [2/20], Step [500/600], Loss: 0.00094, Train_Acc:83.00%\n",
      "Accuary on test images:81.30%, loss:0.57\n",
      "Epoch [2/20], Step [505/600], Loss: 0.00100, Train_Acc:84.00%\n",
      "Accuary on test images:81.97%, loss:0.54\n",
      "Epoch [2/20], Step [510/600], Loss: 0.00142, Train_Acc:74.00%\n",
      "Accuary on test images:76.93%, loss:0.76\n",
      "Epoch [2/20], Step [515/600], Loss: 0.00074, Train_Acc:92.00%\n",
      "Accuary on test images:80.84%, loss:0.57\n",
      "Epoch [2/20], Step [520/600], Loss: 0.00125, Train_Acc:79.00%\n",
      "Accuary on test images:77.92%, loss:0.74\n",
      "Epoch [2/20], Step [525/600], Loss: 0.00074, Train_Acc:87.00%\n",
      "Accuary on test images:80.91%, loss:0.57\n",
      "Epoch [2/20], Step [530/600], Loss: 0.00079, Train_Acc:87.00%\n",
      "Accuary on test images:81.91%, loss:0.56\n",
      "Epoch [2/20], Step [535/600], Loss: 0.00112, Train_Acc:76.00%\n",
      "Accuary on test images:81.50%, loss:0.57\n",
      "Epoch [2/20], Step [540/600], Loss: 0.00071, Train_Acc:88.00%\n",
      "Accuary on test images:81.86%, loss:0.55\n",
      "Epoch [2/20], Step [545/600], Loss: 0.00103, Train_Acc:79.00%\n",
      "Accuary on test images:81.60%, loss:0.55\n",
      "Epoch [2/20], Step [550/600], Loss: 0.00100, Train_Acc:84.00%\n",
      "Accuary on test images:79.91%, loss:0.59\n",
      "Epoch [2/20], Step [555/600], Loss: 0.00102, Train_Acc:80.00%\n",
      "Accuary on test images:82.34%, loss:0.53\n",
      "Epoch [2/20], Step [560/600], Loss: 0.00128, Train_Acc:79.00%\n",
      "Accuary on test images:75.87%, loss:0.71\n",
      "Epoch [2/20], Step [565/600], Loss: 0.00084, Train_Acc:86.00%\n",
      "Accuary on test images:81.38%, loss:0.57\n",
      "Epoch [2/20], Step [570/600], Loss: 0.00112, Train_Acc:79.00%\n",
      "Accuary on test images:76.69%, loss:0.63\n",
      "Epoch [2/20], Step [575/600], Loss: 0.00087, Train_Acc:84.00%\n",
      "Accuary on test images:82.05%, loss:0.53\n",
      "Epoch [2/20], Step [580/600], Loss: 0.00116, Train_Acc:81.00%\n",
      "Accuary on test images:81.09%, loss:0.56\n",
      "Epoch [2/20], Step [585/600], Loss: 0.00116, Train_Acc:76.00%\n",
      "Accuary on test images:74.17%, loss:0.70\n",
      "Epoch [2/20], Step [590/600], Loss: 0.00107, Train_Acc:83.00%\n",
      "Accuary on test images:81.89%, loss:0.53\n",
      "Epoch [2/20], Step [595/600], Loss: 0.00105, Train_Acc:77.00%\n",
      "Accuary on test images:80.97%, loss:0.57\n",
      "Best accuracy is: 82.34%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "test_accuracy = torch.Tensor(test_accuracy)\n",
    "mean = torch.Tensor(mean).cpu()\n",
    "\n",
    "#for digit in range(10):\n",
    "\n",
    "plt.figure(figsize=(4,4)) \n",
    "ax1 = plt.subplot(111)\n",
    "\n",
    "plt.xticks(weight = 'bold')\n",
    "ax1.plot(5*torch.arange(len(test_accuracy))[:160], test_accuracy[:160] ,color = 'blue',linewidth = 3)\n",
    "plt.yticks(weight = 'bold')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(5*torch.arange(len(test_accuracy))[:160], mean[:160],color = 'red',linewidth = 2,linestyle = '--')\n",
    "ax1.set_xlabel(\"Training steps\",fontsize = 12, fontweight = 'bold')\n",
    "ax2.set_ylabel('$\\mu_1-\\mu_2$',fontsize = 12, color = 'red', fontweight = 'bold')\n",
    "ax1.set_ylabel('Test accuracy',fontsize = 12, color = 'blue', fontweight = 'bold')\n",
    "plt.yticks(weight = 'bold')\n",
    "plt.title('SCLR-NN: FMNIST', fontsize = 12, fontweight = 'bold')\n",
    "ax1.grid()\n",
    "savefig('md_sclrnnfmnist.pdf')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"318.255625pt\" height=\"283.66375pt\" viewBox=\"0 0 318.255625 283.66375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-02-25T20:52:23.466506</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 283.66375 \nL 318.255625 283.66375 \nL 318.255625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 47.527813 244.078125 \nL 270.727812 244.078125 \nL 270.727812 22.318125 \nL 47.527813 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 57.673267 244.078125 \nL 57.673267 22.318125 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m238c9d444e\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m238c9d444e\" x=\"57.673267\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(54.194361 258.676563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-30\" d=\"M 2944 2338 \nQ 2944 3213 2780 3570 \nQ 2616 3928 2228 3928 \nQ 1841 3928 1675 3570 \nQ 1509 3213 1509 2338 \nQ 1509 1453 1675 1090 \nQ 1841 728 2228 728 \nQ 2613 728 2778 1090 \nQ 2944 1453 2944 2338 \nz\nM 4147 2328 \nQ 4147 1169 3647 539 \nQ 3147 -91 2228 -91 \nQ 1306 -91 806 539 \nQ 306 1169 306 2328 \nQ 306 3491 806 4120 \nQ 1306 4750 2228 4750 \nQ 3147 4750 3647 4120 \nQ 4147 3491 4147 2328 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 108.719579 244.078125 \nL 108.719579 22.318125 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m238c9d444e\" x=\"108.719579\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(98.28286 258.676563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-32\" d=\"M 1844 884 \nL 3897 884 \nL 3897 0 \nL 506 0 \nL 506 884 \nL 2209 2388 \nQ 2438 2594 2547 2791 \nQ 2656 2988 2656 3200 \nQ 2656 3528 2436 3728 \nQ 2216 3928 1850 3928 \nQ 1569 3928 1234 3808 \nQ 900 3688 519 3450 \nL 519 4475 \nQ 925 4609 1322 4679 \nQ 1719 4750 2100 4750 \nQ 2938 4750 3402 4381 \nQ 3866 4013 3866 3353 \nQ 3866 2972 3669 2642 \nQ 3472 2313 2841 1759 \nL 1844 884 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-32\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"139.160156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 159.765891 244.078125 \nL 159.765891 22.318125 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m238c9d444e\" x=\"159.765891\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(149.329173 258.676563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-34\" d=\"M 2356 3675 \nL 1038 1722 \nL 2356 1722 \nL 2356 3675 \nz\nM 2156 4666 \nL 3494 4666 \nL 3494 1722 \nL 4159 1722 \nL 4159 850 \nL 3494 850 \nL 3494 0 \nL 2356 0 \nL 2356 850 \nL 288 850 \nL 288 1881 \nL 2156 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-34\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"139.160156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 210.812204 244.078125 \nL 210.812204 22.318125 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m238c9d444e\" x=\"210.812204\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(200.375485 258.676563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-36\" d=\"M 2316 2303 \nQ 2000 2303 1842 2098 \nQ 1684 1894 1684 1484 \nQ 1684 1075 1842 870 \nQ 2000 666 2316 666 \nQ 2634 666 2792 870 \nQ 2950 1075 2950 1484 \nQ 2950 1894 2792 2098 \nQ 2634 2303 2316 2303 \nz\nM 3803 4544 \nL 3803 3681 \nQ 3506 3822 3243 3889 \nQ 2981 3956 2731 3956 \nQ 2194 3956 1894 3657 \nQ 1594 3359 1544 2772 \nQ 1750 2925 1990 3001 \nQ 2231 3078 2516 3078 \nQ 3231 3078 3670 2659 \nQ 4109 2241 4109 1563 \nQ 4109 813 3618 361 \nQ 3128 -91 2303 -91 \nQ 1394 -91 895 523 \nQ 397 1138 397 2266 \nQ 397 3422 980 4083 \nQ 1563 4744 2578 4744 \nQ 2900 4744 3203 4694 \nQ 3506 4644 3803 4544 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-36\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"139.160156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 261.858516 244.078125 \nL 261.858516 22.318125 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m238c9d444e\" x=\"261.858516\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(251.421797 258.676563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-38\" d=\"M 2228 2088 \nQ 1891 2088 1709 1903 \nQ 1528 1719 1528 1375 \nQ 1528 1031 1709 848 \nQ 1891 666 2228 666 \nQ 2563 666 2741 848 \nQ 2919 1031 2919 1375 \nQ 2919 1722 2741 1905 \nQ 2563 2088 2228 2088 \nz\nM 1350 2484 \nQ 925 2613 709 2878 \nQ 494 3144 494 3541 \nQ 494 4131 934 4440 \nQ 1375 4750 2228 4750 \nQ 3075 4750 3515 4442 \nQ 3956 4134 3956 3541 \nQ 3956 3144 3739 2878 \nQ 3522 2613 3097 2484 \nQ 3572 2353 3814 2058 \nQ 4056 1763 4056 1313 \nQ 4056 619 3595 264 \nQ 3134 -91 2228 -91 \nQ 1319 -91 855 264 \nQ 391 619 391 1313 \nQ 391 1763 633 2058 \nQ 875 2353 1350 2484 \nz\nM 1631 3419 \nQ 1631 3141 1786 2991 \nQ 1941 2841 2228 2841 \nQ 2509 2841 2662 2991 \nQ 2816 3141 2816 3419 \nQ 2816 3697 2662 3845 \nQ 2509 3994 2228 3994 \nQ 1941 3994 1786 3844 \nQ 1631 3694 1631 3419 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-38\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"139.160156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Training steps -->\n     <g transform=\"translate(111.273125 273.874375) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-Bold-54\" d=\"M 31 4666 \nL 4331 4666 \nL 4331 3756 \nL 2784 3756 \nL 2784 0 \nL 1581 0 \nL 1581 3756 \nL 31 3756 \nL 31 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-72\" d=\"M 3138 2547 \nQ 2991 2616 2845 2648 \nQ 2700 2681 2553 2681 \nQ 2122 2681 1889 2404 \nQ 1656 2128 1656 1613 \nL 1656 0 \nL 538 0 \nL 538 3500 \nL 1656 3500 \nL 1656 2925 \nQ 1872 3269 2151 3426 \nQ 2431 3584 2822 3584 \nQ 2878 3584 2943 3579 \nQ 3009 3575 3134 3559 \nL 3138 2547 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-61\" d=\"M 2106 1575 \nQ 1756 1575 1579 1456 \nQ 1403 1338 1403 1106 \nQ 1403 894 1545 773 \nQ 1688 653 1941 653 \nQ 2256 653 2472 879 \nQ 2688 1106 2688 1447 \nL 2688 1575 \nL 2106 1575 \nz\nM 3816 1997 \nL 3816 0 \nL 2688 0 \nL 2688 519 \nQ 2463 200 2181 54 \nQ 1900 -91 1497 -91 \nQ 953 -91 614 226 \nQ 275 544 275 1050 \nQ 275 1666 698 1953 \nQ 1122 2241 2028 2241 \nL 2688 2241 \nL 2688 2328 \nQ 2688 2594 2478 2717 \nQ 2269 2841 1825 2841 \nQ 1466 2841 1156 2769 \nQ 847 2697 581 2553 \nL 581 3406 \nQ 941 3494 1303 3539 \nQ 1666 3584 2028 3584 \nQ 2975 3584 3395 3211 \nQ 3816 2838 3816 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-69\" d=\"M 538 3500 \nL 1656 3500 \nL 1656 0 \nL 538 0 \nL 538 3500 \nz\nM 538 4863 \nL 1656 4863 \nL 1656 3950 \nL 538 3950 \nL 538 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-6e\" d=\"M 4056 2131 \nL 4056 0 \nL 2931 0 \nL 2931 347 \nL 2931 1631 \nQ 2931 2084 2911 2256 \nQ 2891 2428 2841 2509 \nQ 2775 2619 2662 2680 \nQ 2550 2741 2406 2741 \nQ 2056 2741 1856 2470 \nQ 1656 2200 1656 1722 \nL 1656 0 \nL 538 0 \nL 538 3500 \nL 1656 3500 \nL 1656 2988 \nQ 1909 3294 2193 3439 \nQ 2478 3584 2822 3584 \nQ 3428 3584 3742 3212 \nQ 4056 2841 4056 2131 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-67\" d=\"M 2919 594 \nQ 2688 288 2409 144 \nQ 2131 0 1766 0 \nQ 1125 0 706 504 \nQ 288 1009 288 1791 \nQ 288 2575 706 3076 \nQ 1125 3578 1766 3578 \nQ 2131 3578 2409 3434 \nQ 2688 3291 2919 2981 \nL 2919 3500 \nL 4044 3500 \nL 4044 353 \nQ 4044 -491 3511 -936 \nQ 2978 -1381 1966 -1381 \nQ 1638 -1381 1331 -1331 \nQ 1025 -1281 716 -1178 \nL 716 -306 \nQ 1009 -475 1290 -558 \nQ 1572 -641 1856 -641 \nQ 2406 -641 2662 -400 \nQ 2919 -159 2919 353 \nL 2919 594 \nz\nM 2181 2772 \nQ 1834 2772 1640 2515 \nQ 1447 2259 1447 1791 \nQ 1447 1309 1634 1061 \nQ 1822 813 2181 813 \nQ 2531 813 2725 1069 \nQ 2919 1325 2919 1791 \nQ 2919 2259 2725 2515 \nQ 2531 2772 2181 2772 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-73\" d=\"M 3272 3391 \nL 3272 2541 \nQ 2913 2691 2578 2766 \nQ 2244 2841 1947 2841 \nQ 1628 2841 1473 2761 \nQ 1319 2681 1319 2516 \nQ 1319 2381 1436 2309 \nQ 1553 2238 1856 2203 \nL 2053 2175 \nQ 2913 2066 3209 1816 \nQ 3506 1566 3506 1031 \nQ 3506 472 3093 190 \nQ 2681 -91 1863 -91 \nQ 1516 -91 1145 -36 \nQ 775 19 384 128 \nL 384 978 \nQ 719 816 1070 734 \nQ 1422 653 1784 653 \nQ 2113 653 2278 743 \nQ 2444 834 2444 1013 \nQ 2444 1163 2330 1236 \nQ 2216 1309 1875 1350 \nL 1678 1375 \nQ 931 1469 631 1722 \nQ 331 1975 331 2491 \nQ 331 3047 712 3315 \nQ 1094 3584 1881 3584 \nQ 2191 3584 2531 3537 \nQ 2872 3491 3272 3391 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-74\" d=\"M 1759 4494 \nL 1759 3500 \nL 2913 3500 \nL 2913 2700 \nL 1759 2700 \nL 1759 1216 \nQ 1759 972 1856 886 \nQ 1953 800 2241 800 \nL 2816 800 \nL 2816 0 \nL 1856 0 \nQ 1194 0 917 276 \nQ 641 553 641 1216 \nL 641 2700 \nL 84 2700 \nL 84 3500 \nL 641 3500 \nL 641 4494 \nL 1759 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-65\" d=\"M 4031 1759 \nL 4031 1441 \nL 1416 1441 \nQ 1456 1047 1700 850 \nQ 1944 653 2381 653 \nQ 2734 653 3104 758 \nQ 3475 863 3866 1075 \nL 3866 213 \nQ 3469 63 3072 -14 \nQ 2675 -91 2278 -91 \nQ 1328 -91 801 392 \nQ 275 875 275 1747 \nQ 275 2603 792 3093 \nQ 1309 3584 2216 3584 \nQ 3041 3584 3536 3087 \nQ 4031 2591 4031 1759 \nz\nM 2881 2131 \nQ 2881 2450 2695 2645 \nQ 2509 2841 2209 2841 \nQ 1884 2841 1681 2658 \nQ 1478 2475 1428 2131 \nL 2881 2131 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-70\" d=\"M 1656 506 \nL 1656 -1331 \nL 538 -1331 \nL 538 3500 \nL 1656 3500 \nL 1656 2988 \nQ 1888 3294 2169 3439 \nQ 2450 3584 2816 3584 \nQ 3463 3584 3878 3070 \nQ 4294 2556 4294 1747 \nQ 4294 938 3878 423 \nQ 3463 -91 2816 -91 \nQ 2450 -91 2169 54 \nQ 1888 200 1656 506 \nz\nM 2400 2772 \nQ 2041 2772 1848 2508 \nQ 1656 2244 1656 1747 \nQ 1656 1250 1848 986 \nQ 2041 722 2400 722 \nQ 2759 722 2948 984 \nQ 3138 1247 3138 1747 \nQ 3138 2247 2948 2509 \nQ 2759 2772 2400 2772 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Bold-54\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-72\" x=\"57.212891\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"106.529297\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-69\" x=\"174.009766\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-6e\" x=\"208.287109\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-69\" x=\"279.478516\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-6e\" x=\"313.755859\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-67\" x=\"384.947266\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"456.529297\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-73\" x=\"491.34375\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-74\" x=\"550.865234\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"598.667969\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-70\" x=\"666.490234\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-73\" x=\"738.072266\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 47.527813 237.421521 \nL 270.727812 237.421521 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"m267ffce0b7\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"237.421521\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.1 -->\n      <g transform=\"translate(22.81375 241.220739) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-2e\" d=\"M 653 1209 \nL 1778 1209 \nL 1778 0 \nL 653 0 \nL 653 1209 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-Bold-31\" d=\"M 750 831 \nL 1813 831 \nL 1813 3847 \nL 722 3622 \nL 722 4441 \nL 1806 4666 \nL 2950 4666 \nL 2950 831 \nL 4013 831 \nL 4013 0 \nL 750 0 \nL 750 831 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-31\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 47.527813 208.161724 \nL 270.727812 208.161724 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"208.161724\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(22.81375 211.960943) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-32\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 47.527813 178.901928 \nL 270.727812 178.901928 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"178.901928\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.3 -->\n      <g transform=\"translate(22.81375 182.701147) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-33\" d=\"M 2981 2516 \nQ 3453 2394 3698 2092 \nQ 3944 1791 3944 1325 \nQ 3944 631 3412 270 \nQ 2881 -91 1863 -91 \nQ 1503 -91 1142 -33 \nQ 781 25 428 141 \nL 428 1069 \nQ 766 900 1098 814 \nQ 1431 728 1753 728 \nQ 2231 728 2486 893 \nQ 2741 1059 2741 1369 \nQ 2741 1688 2480 1852 \nQ 2219 2016 1709 2016 \nL 1228 2016 \nL 1228 2791 \nL 1734 2791 \nQ 2188 2791 2409 2933 \nQ 2631 3075 2631 3366 \nQ 2631 3634 2415 3781 \nQ 2200 3928 1806 3928 \nQ 1516 3928 1219 3862 \nQ 922 3797 628 3669 \nL 628 4550 \nQ 984 4650 1334 4700 \nQ 1684 4750 2022 4750 \nQ 2931 4750 3382 4451 \nQ 3834 4153 3834 3553 \nQ 3834 3144 3618 2883 \nQ 3403 2622 2981 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-33\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 47.527813 149.642132 \nL 270.727812 149.642132 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"149.642132\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(22.81375 153.44135) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-34\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 47.527813 120.382335 \nL 270.727812 120.382335 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"120.382335\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.5 -->\n      <g transform=\"translate(22.81375 124.181554) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-35\" d=\"M 678 4666 \nL 3669 4666 \nL 3669 3781 \nL 1638 3781 \nL 1638 3059 \nQ 1775 3097 1914 3117 \nQ 2053 3138 2203 3138 \nQ 3056 3138 3531 2711 \nQ 4006 2284 4006 1522 \nQ 4006 766 3489 337 \nQ 2972 -91 2053 -91 \nQ 1656 -91 1267 -14 \nQ 878 63 494 219 \nL 494 1166 \nQ 875 947 1217 837 \nQ 1559 728 1863 728 \nQ 2300 728 2551 942 \nQ 2803 1156 2803 1522 \nQ 2803 1891 2551 2103 \nQ 2300 2316 1863 2316 \nQ 1603 2316 1309 2248 \nQ 1016 2181 678 2041 \nL 678 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path d=\"M 47.527813 91.122539 \nL 270.727812 91.122539 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"91.122539\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.6 -->\n      <g transform=\"translate(22.81375 94.921758) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-36\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_23\">\n      <path d=\"M 47.527813 61.862743 \nL 270.727812 61.862743 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"61.862743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.7 -->\n      <g transform=\"translate(22.81375 65.661961) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-Bold-37\" d=\"M 428 4666 \nL 3944 4666 \nL 3944 3988 \nL 2125 0 \nL 953 0 \nL 2675 3781 \nL 428 3781 \nL 428 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-37\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_25\">\n      <path d=\"M 47.527813 32.602946 \nL 270.727812 32.602946 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m267ffce0b7\" x=\"47.527813\" y=\"32.602946\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.8 -->\n      <g transform=\"translate(22.81375 36.402165) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-38\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Test accuracy -->\n     <g style=\"fill: #0000ff\" transform=\"translate(16.224375 179.003438) rotate(-90) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-Bold-63\" d=\"M 3366 3391 \nL 3366 2478 \nQ 3138 2634 2908 2709 \nQ 2678 2784 2431 2784 \nQ 1963 2784 1702 2511 \nQ 1441 2238 1441 1747 \nQ 1441 1256 1702 982 \nQ 1963 709 2431 709 \nQ 2694 709 2930 787 \nQ 3166 866 3366 1019 \nL 3366 103 \nQ 3103 6 2833 -42 \nQ 2563 -91 2291 -91 \nQ 1344 -91 809 395 \nQ 275 881 275 1747 \nQ 275 2613 809 3098 \nQ 1344 3584 2291 3584 \nQ 2566 3584 2833 3536 \nQ 3100 3488 3366 3391 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-75\" d=\"M 500 1363 \nL 500 3500 \nL 1625 3500 \nL 1625 3150 \nQ 1625 2866 1622 2436 \nQ 1619 2006 1619 1863 \nQ 1619 1441 1641 1255 \nQ 1663 1069 1716 984 \nQ 1784 875 1895 815 \nQ 2006 756 2150 756 \nQ 2500 756 2700 1025 \nQ 2900 1294 2900 1772 \nL 2900 3500 \nL 4019 3500 \nL 4019 0 \nL 2900 0 \nL 2900 506 \nQ 2647 200 2364 54 \nQ 2081 -91 1741 -91 \nQ 1134 -91 817 281 \nQ 500 653 500 1363 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-Bold-79\" d=\"M 78 3500 \nL 1197 3500 \nL 2138 1125 \nL 2938 3500 \nL 4056 3500 \nL 2584 -331 \nQ 2363 -916 2067 -1148 \nQ 1772 -1381 1288 -1381 \nL 641 -1381 \nL 641 -647 \nL 991 -647 \nQ 1275 -647 1404 -556 \nQ 1534 -466 1606 -231 \nL 1638 -134 \nL 78 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Bold-54\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"54.962891\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-73\" x=\"122.785156\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-74\" x=\"182.306641\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"230.109375\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"264.923828\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"332.404297\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"391.681641\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-75\" x=\"450.958984\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-72\" x=\"522.150391\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"571.466797\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-63\" x=\"638.947266\"/>\n      <use xlink:href=\"#DejaVuSans-Bold-79\" x=\"698.224609\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 57.673267 233.998125 \nL 58.949425 231.891419 \nL 61.50174 228.350985 \nL 62.777898 226.975773 \nL 64.054056 224.839806 \nL 66.606372 221.913829 \nL 70.434845 218.285614 \nL 71.711003 217.14448 \nL 72.987161 216.383726 \nL 74.263319 215.388893 \nL 75.539476 214.540358 \nL 76.815634 213.369968 \nL 78.091792 212.375136 \nL 79.36795 212.375136 \nL 80.644108 211.029184 \nL 81.920265 210.239169 \nL 83.196423 208.922478 \nL 84.472581 208.71766 \nL 85.748739 207.781348 \nL 87.024897 208.073947 \nL 89.577212 207.254672 \nL 90.85337 205.820939 \nL 92.129528 205.557601 \nL 93.405686 205.645382 \nL 94.681843 205.90872 \nL 98.510317 202.192724 \nL 99.786475 202.309765 \nL 101.062632 200.934554 \nL 102.33879 200.437137 \nL 103.614948 200.203059 \nL 104.891106 200.700476 \nL 106.167264 198.857108 \nL 107.443421 198.62303 \nL 108.719579 198.593769 \nL 109.995737 198.330431 \nL 111.271895 196.604104 \nL 112.548053 195.755569 \nL 113.82421 194.585179 \nL 115.100368 194.6437 \nL 116.376526 192.215135 \nL 117.652684 193.648867 \nL 118.928842 190.898444 \nL 120.204999 190.781402 \nL 121.481157 190.049908 \nL 122.757315 186.772817 \nL 124.033473 192.244391 \nL 125.309631 200.700476 \nL 126.585788 202.543843 \nL 127.861946 207.10837 \nL 129.138104 204.767586 \nL 130.414262 213.867384 \nL 131.69042 193.765904 \nL 132.966578 200.23232 \nL 134.242735 138.113776 \nL 135.518893 134.222222 \nL 136.795051 131.179204 \nL 138.071209 152.538851 \nL 140.623524 120.440856 \nL 141.899682 127.316907 \nL 143.17584 90.18622 \nL 144.451998 90.449567 \nL 145.728156 108.766204 \nL 147.004313 80.735307 \nL 148.280471 74.854089 \nL 149.556629 78.423783 \nL 150.832787 86.382461 \nL 152.108945 82.812749 \nL 153.385102 87.465061 \nL 155.937418 74.064083 \nL 157.213576 99.110456 \nL 158.489734 67.948782 \nL 159.765891 72.659606 \nL 161.042049 72.864424 \nL 162.318207 73.508142 \nL 163.594365 76.785242 \nL 164.870523 80.413448 \nL 166.14668 80.735307 \nL 167.422838 58.410093 \nL 168.698996 69.119176 \nL 169.975154 58.293051 \nL 171.251312 61.950523 \nL 172.527469 54.284457 \nL 173.803627 62.974611 \nL 175.079785 58.497869 \nL 176.355943 52.031445 \nL 177.632101 58.293051 \nL 178.908258 51.241439 \nL 180.184416 58.527134 \nL 181.460574 56.274122 \nL 182.736732 48.578792 \nL 184.01289 63.267205 \nL 185.289047 49.632145 \nL 186.565205 49.983268 \nL 187.841363 57.327475 \nL 189.117521 53.75778 \nL 190.393679 55.981528 \nL 191.669837 51.446257 \nL 192.945994 51.387727 \nL 194.222152 56.85931 \nL 195.49831 47.64248 \nL 196.774468 51.475521 \nL 198.050626 52.733692 \nL 199.326783 54.313722 \nL 200.602941 50.91958 \nL 201.879099 51.007357 \nL 203.155257 40.795685 \nL 204.431415 44.950585 \nL 205.707572 46.910986 \nL 206.98373 42.931656 \nL 208.259888 59.814552 \nL 209.536046 42.580532 \nL 210.812204 45.740591 \nL 212.088361 45.243179 \nL 213.364519 41.527179 \nL 214.640677 53.055551 \nL 215.916835 40.707908 \nL 217.192993 57.766375 \nL 218.46915 48.549527 \nL 219.745308 53.31888 \nL 221.021466 43.516844 \nL 222.297624 41.556444 \nL 223.573782 56.537451 \nL 224.849939 40.824949 \nL 226.126097 55.132992 \nL 227.402255 55.630404 \nL 228.678413 40.356785 \nL 229.954571 42.024608 \nL 231.230728 43.22425 \nL 232.506886 42.902391 \nL 233.783044 42.78535 \nL 235.059202 46.150227 \nL 236.33536 41.527179 \nL 237.611517 46.910986 \nL 238.887675 43.897232 \nL 240.163833 47.993603 \nL 241.439991 42.053856 \nL 242.716149 42.609797 \nL 243.992306 45.974674 \nL 245.268464 50.656233 \nL 246.544622 39.537514 \nL 247.82078 35.733737 \nL 249.096938 46.559862 \nL 250.373096 48.52028 \nL 251.649253 33.30519 \nL 252.925411 39.68382 \nL 254.201569 38.718243 \nL 255.477727 42.141632 \nL 256.753885 39.947149 \nL 258.030042 47.554703 \nL 260.582358 32.398125 \nL 260.582358 32.398125 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke: #0000ff; stroke-width: 3; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 47.527813 244.078125 \nL 47.527813 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 270.727812 244.078125 \nL 270.727812 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 47.527813 244.078125 \nL 270.727812 244.078125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 47.527813 22.318125 \nL 270.727812 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"ytick_9\">\n     <g id=\"line2d_28\">\n      <defs>\n       <path id=\"m14088c9f87\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m14088c9f87\" x=\"270.727812\" y=\"238.744562\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.0 -->\n      <g transform=\"translate(277.727812 242.54378) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-30\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_29\">\n      <g>\n       <use xlink:href=\"#m14088c9f87\" x=\"270.727812\" y=\"202.538139\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.1 -->\n      <g transform=\"translate(277.727812 206.337358) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-31\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m14088c9f87\" x=\"270.727812\" y=\"166.331716\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.2 -->\n      <g transform=\"translate(277.727812 170.130935) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-32\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_31\">\n      <g>\n       <use xlink:href=\"#m14088c9f87\" x=\"270.727812\" y=\"130.125294\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0.3 -->\n      <g transform=\"translate(277.727812 133.924512) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-33\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_32\">\n      <g>\n       <use xlink:href=\"#m14088c9f87\" x=\"270.727812\" y=\"93.918871\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 0.4 -->\n      <g transform=\"translate(277.727812 97.71809) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-34\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_33\">\n      <g>\n       <use xlink:href=\"#m14088c9f87\" x=\"270.727812\" y=\"57.712448\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 0.5 -->\n      <g transform=\"translate(277.727812 61.511667) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-Bold-30\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-2e\" x=\"69.580078\"/>\n       <use xlink:href=\"#DejaVuSans-Bold-35\" x=\"107.568359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- $\\mu_1-\\mu_2$ -->\n     <g style=\"fill: #ff0000\" transform=\"translate(308.535625 153.898125) rotate(-90) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-Oblique-3bc\" d=\"M -84 -1331 \nL 856 3500 \nL 1434 3500 \nL 1009 1322 \nQ 997 1256 987 1175 \nQ 978 1094 978 1013 \nQ 978 722 1161 565 \nQ 1344 409 1684 409 \nQ 2147 409 2431 671 \nQ 2716 934 2816 1459 \nL 3213 3500 \nL 3788 3500 \nL 3266 809 \nQ 3253 750 3248 706 \nQ 3244 663 3244 628 \nQ 3244 531 3283 486 \nQ 3322 441 3406 441 \nQ 3438 441 3492 456 \nQ 3547 472 3647 513 \nL 3559 50 \nQ 3422 -19 3297 -55 \nQ 3172 -91 3053 -91 \nQ 2847 -91 2730 40 \nQ 2613 172 2613 403 \nQ 2438 153 2195 31 \nQ 1953 -91 1625 -91 \nQ 1334 -91 1117 43 \nQ 900 178 831 397 \nL 494 -1331 \nL -84 -1331 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(0 0.3125)\"/>\n      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(63.623047 -16.09375) scale(0.7)\"/>\n      <use xlink:href=\"#DejaVuSans-2212\" transform=\"translate(130.375977 0.3125)\"/>\n      <use xlink:href=\"#DejaVuSans-Oblique-3bc\" transform=\"translate(233.647461 0.3125)\"/>\n      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(297.270508 -16.09375) scale(0.7)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 57.673267 233.998125 \nL 60.225583 231.008224 \nL 61.50174 229.579541 \nL 62.777898 228.410819 \nL 65.330214 225.408947 \nL 69.158687 221.577515 \nL 75.539476 215.563891 \nL 76.815634 214.191614 \nL 78.091792 213.163716 \nL 79.36795 212.368724 \nL 80.644108 211.193298 \nL 81.920265 210.218311 \nL 83.196423 208.922203 \nL 85.748739 207.334033 \nL 88.301054 206.363144 \nL 89.577212 205.893599 \nL 90.85337 204.970112 \nL 92.129528 204.488643 \nL 93.405686 204.265922 \nL 94.681843 204.163274 \nL 95.958001 203.459403 \nL 97.234159 202.529441 \nL 98.510317 201.844 \nL 99.786475 201.771225 \nL 101.062632 201.085778 \nL 102.33879 201.000076 \nL 103.614948 201.213309 \nL 104.891106 201.577689 \nL 106.167264 201.10219 \nL 107.443421 201.171629 \nL 108.719579 201.375383 \nL 109.995737 201.131057 \nL 111.271895 201.666229 \nL 112.548053 201.42411 \nL 113.82421 201.052015 \nL 115.100368 201.526402 \nL 116.376526 201.020008 \nL 117.652684 201.464528 \nL 118.928842 201.677767 \nL 120.204999 200.970483 \nL 121.481157 201.130828 \nL 122.757315 201.955163 \nL 124.033473 204.074445 \nL 125.309631 205.818169 \nL 126.585788 206.90097 \nL 127.861946 209.344737 \nL 130.414262 209.904064 \nL 131.69042 207.354084 \nL 132.966578 198.337397 \nL 134.242735 172.58164 \nL 135.518893 150.298327 \nL 136.795051 141.820707 \nL 138.071209 139.893517 \nL 139.347367 133.753481 \nL 140.623524 115.810708 \nL 141.899682 122.157983 \nL 143.17584 93.30247 \nL 144.451998 80.13623 \nL 145.728156 97.488318 \nL 147.004313 67.308128 \nL 148.280471 61.021624 \nL 149.556629 56.032002 \nL 150.832787 57.611149 \nL 152.108945 65.202152 \nL 153.385102 54.067985 \nL 154.66126 63.025231 \nL 155.937418 62.80856 \nL 157.213576 71.642527 \nL 158.489734 59.818283 \nL 159.765891 69.001717 \nL 161.042049 55.84548 \nL 162.318207 61.878367 \nL 163.594365 53.215741 \nL 164.870523 67.599143 \nL 166.14668 58.424364 \nL 167.422838 44.995433 \nL 168.698996 53.813418 \nL 169.975154 39.517493 \nL 171.251312 45.174013 \nL 172.527469 39.727538 \nL 173.803627 43.774353 \nL 175.079785 41.318511 \nL 176.355943 39.4029 \nL 177.632101 40.181488 \nL 178.908258 34.52225 \nL 180.184416 39.45301 \nL 181.460574 40.360652 \nL 182.736732 39.209709 \nL 184.01289 42.031862 \nL 185.289047 42.824607 \nL 186.565205 38.431962 \nL 187.841363 37.931764 \nL 189.117521 36.233298 \nL 190.393679 41.824298 \nL 191.669837 41.758585 \nL 192.945994 38.407856 \nL 194.222152 35.548585 \nL 195.49831 38.712101 \nL 196.774468 37.480015 \nL 198.050626 37.268654 \nL 199.326783 44.339185 \nL 200.602941 41.731825 \nL 201.879099 44.407984 \nL 203.155257 39.979644 \nL 204.431415 36.18254 \nL 205.707572 41.133521 \nL 206.98373 32.398125 \nL 208.259888 47.732341 \nL 209.536046 41.020223 \nL 210.812204 46.908973 \nL 212.088361 32.800238 \nL 213.364519 34.412727 \nL 214.640677 40.226635 \nL 215.916835 39.787468 \nL 217.192993 56.536429 \nL 218.46915 44.879113 \nL 219.745308 46.514477 \nL 221.021466 40.247461 \nL 222.297624 41.511939 \nL 223.573782 47.05106 \nL 224.849939 37.218435 \nL 227.402255 49.012401 \nL 228.678413 42.682779 \nL 229.954571 39.965034 \nL 231.230728 36.676695 \nL 232.506886 34.096484 \nL 233.783044 38.117941 \nL 235.059202 39.479554 \nL 236.33536 36.623542 \nL 237.611517 45.133463 \nL 238.887675 42.239274 \nL 240.163833 45.051456 \nL 242.716149 37.276531 \nL 243.992306 46.879644 \nL 245.268464 47.209592 \nL 246.544622 36.064731 \nL 247.82078 41.355285 \nL 249.096938 44.155662 \nL 251.649253 44.417674 \nL 252.925411 42.345192 \nL 254.201569 47.424859 \nL 255.477727 46.965622 \nL 256.753885 45.769576 \nL 258.030042 53.405824 \nL 259.3062 51.204764 \nL 260.582358 48.336515 \nL 260.582358 48.336515 \n\" clip-path=\"url(#p2fc603f37d)\" style=\"fill: none; stroke-dasharray: 7.4,3.2; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 2\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 47.527813 244.078125 \nL 47.527813 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 270.727812 244.078125 \nL 270.727812 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 47.527813 244.078125 \nL 270.727812 244.078125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 47.527813 22.318125 \nL 270.727812 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_23\">\n    <!-- SCLR-NN: FMNIST -->\n    <g transform=\"translate(99.202812 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-Bold-53\" d=\"M 3834 4519 \nL 3834 3531 \nQ 3450 3703 3084 3790 \nQ 2719 3878 2394 3878 \nQ 1963 3878 1756 3759 \nQ 1550 3641 1550 3391 \nQ 1550 3203 1689 3098 \nQ 1828 2994 2194 2919 \nL 2706 2816 \nQ 3484 2659 3812 2340 \nQ 4141 2022 4141 1434 \nQ 4141 663 3683 286 \nQ 3225 -91 2284 -91 \nQ 1841 -91 1394 -6 \nQ 947 78 500 244 \nL 500 1259 \nQ 947 1022 1364 901 \nQ 1781 781 2169 781 \nQ 2563 781 2772 912 \nQ 2981 1044 2981 1288 \nQ 2981 1506 2839 1625 \nQ 2697 1744 2272 1838 \nL 1806 1941 \nQ 1106 2091 782 2419 \nQ 459 2747 459 3303 \nQ 459 4000 909 4375 \nQ 1359 4750 2203 4750 \nQ 2588 4750 2994 4692 \nQ 3400 4634 3834 4519 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-43\" d=\"M 4288 256 \nQ 3956 84 3597 -3 \nQ 3238 -91 2847 -91 \nQ 1681 -91 1000 561 \nQ 319 1213 319 2328 \nQ 319 3447 1000 4098 \nQ 1681 4750 2847 4750 \nQ 3238 4750 3597 4662 \nQ 3956 4575 4288 4403 \nL 4288 3438 \nQ 3953 3666 3628 3772 \nQ 3303 3878 2944 3878 \nQ 2300 3878 1931 3465 \nQ 1563 3053 1563 2328 \nQ 1563 1606 1931 1193 \nQ 2300 781 2944 781 \nQ 3303 781 3628 887 \nQ 3953 994 4288 1222 \nL 4288 256 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-4c\" d=\"M 588 4666 \nL 1791 4666 \nL 1791 909 \nL 3903 909 \nL 3903 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-52\" d=\"M 2297 2597 \nQ 2675 2597 2839 2737 \nQ 3003 2878 3003 3200 \nQ 3003 3519 2839 3656 \nQ 2675 3794 2297 3794 \nL 1791 3794 \nL 1791 2597 \nL 2297 2597 \nz\nM 1791 1766 \nL 1791 0 \nL 588 0 \nL 588 4666 \nL 2425 4666 \nQ 3347 4666 3776 4356 \nQ 4206 4047 4206 3378 \nQ 4206 2916 3982 2619 \nQ 3759 2322 3309 2181 \nQ 3556 2125 3751 1926 \nQ 3947 1728 4147 1325 \nL 4800 0 \nL 3519 0 \nL 2950 1159 \nQ 2778 1509 2601 1637 \nQ 2425 1766 2131 1766 \nL 1791 1766 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-2d\" d=\"M 347 2297 \nL 2309 2297 \nL 2309 1388 \nL 347 1388 \nL 347 2297 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-4e\" d=\"M 588 4666 \nL 1931 4666 \nL 3628 1466 \nL 3628 4666 \nL 4769 4666 \nL 4769 0 \nL 3425 0 \nL 1728 3200 \nL 1728 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-3a\" d=\"M 716 3500 \nL 1844 3500 \nL 1844 2291 \nL 716 2291 \nL 716 3500 \nz\nM 716 1209 \nL 1844 1209 \nL 1844 0 \nL 716 0 \nL 716 1209 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-46\" d=\"M 588 4666 \nL 3834 4666 \nL 3834 3756 \nL 1791 3756 \nL 1791 2888 \nL 3713 2888 \nL 3713 1978 \nL 1791 1978 \nL 1791 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-4d\" d=\"M 588 4666 \nL 2119 4666 \nL 3181 2169 \nL 4250 4666 \nL 5778 4666 \nL 5778 0 \nL 4641 0 \nL 4641 3413 \nL 3566 897 \nL 2803 897 \nL 1728 3413 \nL 1728 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-Bold-49\" d=\"M 588 4666 \nL 1791 4666 \nL 1791 0 \nL 588 0 \nL 588 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-Bold-53\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-43\" x=\"72.021484\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4c\" x=\"145.410156\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-52\" x=\"209.130859\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-2d\" x=\"286.132812\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4e\" x=\"327.636719\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4e\" x=\"411.328125\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-3a\" x=\"495.019531\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-20\" x=\"535.009766\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-46\" x=\"569.824219\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4d\" x=\"638.134766\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-4e\" x=\"737.646484\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-49\" x=\"821.337891\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-53\" x=\"858.544922\"/>\n     <use xlink:href=\"#DejaVuSans-Bold-54\" x=\"930.566406\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2fc603f37d\">\n   <rect x=\"47.527813\" y=\"22.318125\" width=\"223.2\" height=\"221.76\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adversarial example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def generate_adversial_exm_grad(varepsilon):\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input, target.long()\n",
    "        adversial_exm = input\n",
    "        \n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input, target.long()\n",
    "\n",
    "        input.requires_grad = True\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "    \n",
    "        adversial_exm[i:i+1,:,:,:] = input + varepsilon*input.grad/torch.norm(input.grad)\n",
    "        \n",
    "        \n",
    "    adversial_exm = adversial_exm.detach()\n",
    "    return adversial_exm.data\n",
    "\n",
    "\n",
    "def generate_adversial_exm_sign(varepsilon):\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input, target.long()\n",
    "        adversial_exm = input\n",
    "        \n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "    for i, (input, target) in enumerate(valid_loader):\n",
    "        input, target = input, target.long()\n",
    "\n",
    "        input.requires_grad = True\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "    \n",
    "        adversial_exm[i:i+1,:,:,:] = input + varepsilon*torch.sign(input.grad)\n",
    "        \n",
    "        \n",
    "    adversial_exm = adversial_exm.detach()\n",
    "    return adversial_exm.data\n",
    "\n",
    "for j, (input, target) in enumerate(valid_loader):\n",
    "    input, target = input, target.long()\n",
    "\n",
    "test_accuracy = torch.zeros(10)\n",
    "adversial_input_all = torch.zeros(10,10000,784)\n",
    "for j in range(10):\n",
    "    varepsilon = 0.01*j\n",
    "    #adversial_input = generate_adversial_exm_grad(varepsilon)\n",
    "    adversial_input_1 = generate_adversial_exm_sign(varepsilon)\n",
    "    adversial_input_all[j,:,:] = adversial_input_1.reshape(10000,784)\n",
    "\n",
    "    model.eval()\n",
    "#valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "#for i, input in enumerate(valid_loader):\n",
    " #   input = input\n",
    "\n",
    "#valid_correct = 0\n",
    "#valid_total = 0\n",
    "#with torch.no_grad():\n",
    " #   output = model(adversial_input)\n",
    "\n",
    "   # _, predicted = torch.max(output.data, 1)\n",
    "    #valid_total = target.size(0)\n",
    "    #valid_correct = (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "#prec = valid_correct / valid_total\n",
    "#print('Accuary on test images:{:.2f}%, epsilon:{}'.format(prec*100, j*0.1))\n",
    "\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad(): \n",
    "        output = model(adversial_input_1)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        valid_total = target.size(0)\n",
    "        valid_correct = (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "    prec_1 = valid_correct / valid_total\n",
    "    print('Accuary on test images:{:.2f}%, epsilon:{}'.format(prec_1*100, varepsilon))\n",
    "\n",
    "#input = input.reshape(10000,512)\n",
    "#difference = torch.cat([difference,torch.zeros(10000,1)],dim=1)\n",
    "    test_accuracy[j] = prec_1 #gradient ad\n",
    "#test_accuracy[1,j] = prec_1 #sign ad"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuary on test images:83.64%, epsilon:0.0\n",
      "Accuary on test images:4.19%, epsilon:0.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for j, (input, target) in enumerate(valid_loader):\n",
    "    input, target = input, target.long()\n",
    "\n",
    "test_accuracy = torch.zeros(10)\n",
    "for j in range(10):\n",
    "#adversial_input = generate_adversial_exm_grad(0.5)\n",
    "    adversial_input_1 = torch.load('ad_all_fmnist_gradient')[j,:,:].reshape(10000,1,28,28)\n",
    "\n",
    "    model.eval()\n",
    "#valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=10000)\n",
    "#for i, input in enumerate(valid_loader):\n",
    " #   input = input\n",
    "\n",
    "#valid_correct = 0\n",
    "#valid_total = 0\n",
    "#with torch.no_grad():\n",
    " #   output = model(adversial_input)\n",
    "\n",
    "   # _, predicted = torch.max(output.data, 1)\n",
    "    #valid_total = target.size(0)\n",
    "    #valid_correct = (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "#prec = valid_correct / valid_total \n",
    "#print('Accuary on test images:{:.2f}%, epsilon:{}'.format(prec*100, j*0.1))\n",
    "\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        output = model(adversial_input_1)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        valid_total = target.size(0)\n",
    "        valid_correct = (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "    prec_1 = valid_correct / valid_total\n",
    "    print('Accuary on test images:{:.2f}%, epsilon:{}'.format(prec_1*100, j*0.01))\n",
    "    test_accuracy[j] = prec_1\n",
    "\n",
    "torch.save(test_accuracy, 'ad_gradient_quadratic_fmnist')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuary on test images:88.68%, epsilon:0.0\n",
      "Accuary on test images:84.23%, epsilon:0.01\n",
      "Accuary on test images:78.61%, epsilon:0.02\n",
      "Accuary on test images:71.69%, epsilon:0.03\n",
      "Accuary on test images:63.44%, epsilon:0.04\n",
      "Accuary on test images:55.53%, epsilon:0.05\n",
      "Accuary on test images:47.72%, epsilon:0.06\n",
      "Accuary on test images:40.06%, epsilon:0.07\n",
      "Accuary on test images:33.34%, epsilon:0.08\n",
      "Accuary on test images:27.42%, epsilon:0.09\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "digit = 9\n",
    "for j, (input, target) in enumerate(valid_loader):\n",
    "    input, target = input, target.long()\n",
    "feature_map = intermediate_output(input, 2)\n",
    "feature_map_digit = feature_map[target==digit,:].reshape(sum(target==digit),784)\n",
    "with torch.no_grad():\n",
    "    u,s,v = torch.svd(feature_map_digit-torch.mean(feature_map.reshape(10000,784), dim=0))\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.bar(torch.arange(30),s[:30]/max(s), color = 'orange')\n",
    "    plt.title('Class {}'.format(digit), fontsize=12)\n",
    "    plt.xlabel('Index of eigens', fontsize=15)\n",
    "    plt.ylabel('Eigenvalue $\\lambda/\\lambda_{\\max}$',fontsize=15)\n",
    "    plt.ylim([0,1.1])\n",
    "    savefig('fmnist_feature_similarity_{}.pdf'.format(digit))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out = inner_product\n",
    "\n",
    "for digit in range(10):\n",
    "    plt.figure(figsize=(17.5,15.7))\n",
    "    plt.title('Class {}'.format(digit),fontsize=20)\n",
    "    for svg in range(9):\n",
    "    #fig,ax = plt.subplots(1, 1)\n",
    "    #plt.scatter(out[60:,0,digit],out[60:,1,digit],c=np.arange(180),cmap=plt.cm.Purples,s=100)\n",
    "    #cb_1 = plt.colorbar()\n",
    "    #cb_1.set_label('last 180 steps')\n",
    "        plt.subplot(331+svg)\n",
    "        plt.scatter(out[:,digit,digit],out[:,digit,torch.arange(10)[torch.arange(10)!=digit][svg]],c=np.arange(240),cmap=plt.cm.YlOrBr,s=100)\n",
    "        #cb = plt.colorbar()\n",
    "        #cb.set_label('training steps',fontsize=15)\n",
    "        plt.scatter(out[30,digit,digit],out[30,digit,torch.arange(10)[torch.arange(10)!=digit][svg]],c='red',s=100,label='loss converge point')\n",
    "        #axins = ax.inset_axes((0, 8, 5, 8))\n",
    "        #axins.plot(torch.arange(240),x_norm,color='black',linestyle='--',linewidth=3)\n",
    "        #axins.grid()\n",
    "        #axins.set_xlabel('Training steps')\n",
    "        #axins.set_ylabel('$\\|\\hat{{x}}\\|$')\n",
    "        #plt.scatter(out_1[:,0],out_1[:,1],c=np.arange(120),cmap=plt.cm.GnBu,s=100)\n",
    "        #plt.plot(x,y,linestyle='--',linewidth=3,label='Unit Circle')\n",
    "        plt.grid()\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('Target output ($a_{{{},{}}}$)'.format(digit,digit))\n",
    "        plt.ylabel('Other output ($a_{{{},{}}}$)'.format(digit,torch.arange(10)[torch.arange(10)!=digit][svg]))\n",
    "    #savefig('batch_output_relation_{}.svg'.format(digit))\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_accuracy = torch.Tensor(test_accuracy)\n",
    "KS_dis = torch.Tensor(KS_dis)\n",
    "\n",
    "for digit in range(10):\n",
    "    fig = plt.figure() \n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    ax1.plot(torch.arange(300), torch.Tensor(test_accuracy[torch.arange(3000)%10==digit]) ,color = 'blue',linewidth = 5)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(torch.arange(300), torch.Tensor(KS_dis[torch.arange(3000)%10==digit]) ,color = 'red',linewidth = 3,linestyle = '--')\n",
    "    ax1.set_xlabel(\"Training step\",fontsize = 20)\n",
    "    ax2.set_ylabel('KS distance',fontsize = 20, color = 'red')\n",
    "    ax1.set_ylabel('Test Accuracy',fontsize = 20, color = 'blue')\n",
    "    plt.title('Class {}'.format(digit), fontsize = 20)\n",
    "    savefig('linear_KS_fmnist_two_norm_{}.svg'.format(digit))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Quadratic recognize the feature map of different class, spike trigger average\n",
    "for i, (input, target) in enumerate(valid_loader):\n",
    "    input, target = input.cuda(), target.long().cuda()\n",
    "    \n",
    "convolution_kernel = model.state_dict()['module.features.0.weight']\n",
    "output_first_layer = F.conv2d(input, convolution_kernel.cuda(), padding=2).reshape(10000,784)\n",
    "spike_trigger_average = torch.zeros(784).cuda()\n",
    "picture_label = 9\n",
    "k = 0\n",
    "#obtain input\n",
    "input = input.reshape(10000,784)\n",
    "#output = model(input)\n",
    "\n",
    "for i in range(10000):\n",
    "    if target[i] == picture_label:\n",
    "        spike_trigger_average += output_first_layer[i,:]\n",
    "        k = k+1\n",
    "spike_trigger_average = spike_trigger_average/k \n",
    "matshow(spike_trigger_average.cpu())"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.9 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "4dc8b9ca2dd743165c9a23c64ed5faa802a7566e9a05ea7fbc4e7eac82525774"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}